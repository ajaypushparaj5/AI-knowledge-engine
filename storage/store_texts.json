[{"text": "History-Independent Load Balancing Michael A. Bender\u2217 William Kuszmaul\u2020 Elaine Shi\u2021 Rose Silver\u00a7 Abstract We give a (strongly) history-independent two-choice balls-and-bins algorithm on\ud835\udc5b bins that supports both insertions and deletions on a set of up to\ud835\udc5a balls, while guaranteeing a maximum load of\ud835\udc5a/\ud835\udc5b+\ud835\udc42( 1) with high probability, and achieving an expected recourse of\ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) per operation. To the best of our knowledge, this is the first history-independent solution to achieve nontrivial guarantees of any sort for\ud835\udc5a/\ud835\udc5b\u2265\ud835\udf14( 1) and is the first fully dynamic solution (history independent or not) to achieve\ud835\udc42( 1) overload with\ud835\udc5c(\ud835\udc5a/\ud835\udc5b)expected recourse. 1 Introduction Since its introduction more than two decades ago, history independence [28, 22] has become one of the most widely studied security properties in data structures", "source": "loadbalancing.pdf"}, {"text": "overload with\ud835\udc5c(\ud835\udc5a/\ud835\udc5b)expected recourse. 1 Introduction Since its introduction more than two decades ago, history independence [28, 22] has become one of the most widely studied security properties in data structures [22, 28, 19, 18, 1, 12, 11, 26, 16, 17, 7]. Formally, a data structure is said to behistory independent[ 28] (orstrongly history independent[ 28]) if, for any sequence \ud835\udc611, \ud835\udc612, . . . of times, leaking the data structure\u2019s states\ud835\udc46\ud835\udc611, \ud835\udc46\ud835\udc612, . . . at those times does not leak any information beyond which elements were contained in the data structure at those points in time. With a vanilla data structure, if the adversary hacks into the server and obtains a snapshot of the data structure\u2019s state, it can", "source": "loadbalancing.pdf"}, {"text": "data structure at those points in time. With a vanilla data structure, if the adversary hacks into the server and obtains a snapshot of the data structure\u2019s state, it can learn not only which elements are currently in the data structure, but also historical information such as the order of past insertions and deletions, or even sensitive information about which elements were present in the past but have since been deleted. History independent data structures defend against such attacks, revealing the information-theoretically minimum amount of information even if the adversary hacks the data structure at multiple points in time. A natural way to construct a history independent data structure is to define it in such a way that its state", "source": "loadbalancing.pdf"}, {"text": "hacks the data structure at multiple points in time. A natural way to construct a history independent data structure is to define it in such a way that its state at any given moment iscompletely determinedby its current set of elements (and its random tape). Hartline et al. [19, 18] proved that, in most data-structural settings1 (including those in this paper), this is actually theonlyway to achieve history independence. Thus one can think of history independence as really being a type of unique representability. In this paper, we revisit a basic problem where efficient history independence has so far proven difficult to achieve: the problem of maintaining a dynamic assignment of balls to bins, where each ball has two random", "source": "loadbalancing.pdf"}, {"text": "basic problem where efficient history independence has so far proven difficult to achieve: the problem of maintaining a dynamic assignment of balls to bins, where each ball has two random choices for where it could go, and where the goal is to prevent any one bin from being too overloaded. Although this problem has been heavily studied in the data structure [5, 14, 15, 27, 29] and scheduling [4, 32, 13, 24, 23, 9, 5] literatures, the best known solutions are all (significantly) historydependent. In this paper, we show that history independent solutions can also do surprisingly well. In fact, our final solution even improveson the best prior history-dependent results, achieving a doubly-exponential reduction to recourse (the number of balls", "source": "loadbalancing.pdf"}, {"text": "history independent solutions can also do surprisingly well. In fact, our final solution even improveson the best prior history-dependent results, achieving a doubly-exponential reduction to recourse (the number of balls moved per operation) while maintaining constant overload (the maximum amount by which any bin is overloaded). \u2217Department of Computer Science, Stony Brook University. \u2020Computer Science Department, Carnegie Mellon University. \u2021Computer Science Department & Electrical and Computer Engineering, Carnegie Mellon University. \u00a7Computer Science Department, Carnegie Mellon University. 1Hartline et al. \u2019s result [18, 19] holds whenever the logical states of the data structure form a strongly connected graph (i.e., all logical states are mutually reachable from each other). 1 arXiv:2602.11953v1 [cs.DS] 12 Feb 2026 Two-choice load balancing.In thetwo-choice load-balancing problem, a", "source": "loadbalancing.pdf"}, {"text": "data structure form a strongly connected graph (i.e., all logical states are mutually reachable from each other). 1 arXiv:2602.11953v1 [cs.DS] 12 Feb 2026 Two-choice load balancing.In thetwo-choice load-balancing problem, a set S of up to\ud835\udc5a balls must be assigned to\ud835\udc5b bins. Each ball \ud835\udc65\u2208 S has two uniformly random bins\u210e1(\ud835\udc65), \u210e2(\ud835\udc65) \u2208 [\ud835\udc5b] where it is capable of going. In its most general form, the goal of two-choice load balancing is to maintain a dynamic allocation of balls to bins, so that, over time, as balls are inserted and deleted, we achieve two simultaneous guarantees: 1.Low Overload:The amount by which the fullest bin\u2019s load exceeds\ud835\udc5a/\ud835\udc5b(i.e., theoverload) is small; 2. Low Recourse:On any given insertion and deletion, the number of balls", "source": "loadbalancing.pdf"}, {"text": "we achieve two simultaneous guarantees: 1.Low Overload:The amount by which the fullest bin\u2019s load exceeds\ud835\udc5a/\ud835\udc5b(i.e., theoverload) is small; 2. Low Recourse:On any given insertion and deletion, the number of balls that get moved around (this is known as therecourse) is small. Recourse will often be measured as a function of\ud835\udf07 :=\ud835\udc5a/\ud835\udc5b. The two-choice load-balancing problem [4, 32, 13, 24, 23, 9, 5] can be viewed as capturing a natural scheduling problem: balls represent jobs, each of which are capable of running on two random servers.2 The goal is to schedule jobs to servers so that no server is too overloaded and so that, as jobs arrive and depart, the jobs that are already present experience as little migration as possible.", "source": "loadbalancing.pdf"}, {"text": "jobs to servers so that no server is too overloaded and so that, as jobs arrive and depart, the jobs that are already present experience as little migration as possible. As we will discuss, in some restricted versions of the problem (e.g., where balls are inserted but not deleted), it is even possible to achieve nontrivial guarantees with no migration at all. But, in the full version of the problem, where balls are both inserted and deleted, there is strong evidence [5] that any algorithm with low overload must incur recourse. The simplest solution to the two-choice load-balancing problem is to assign each ball\ud835\udc65\u2208 S to itsfirst choice \u210e1(\ud835\udc65) . This solution, known as thesingle-choice algorithm, has two appealing properties:", "source": "loadbalancing.pdf"}, {"text": "recourse. The simplest solution to the two-choice load-balancing problem is to assign each ball\ud835\udc65\u2208 S to itsfirst choice \u210e1(\ud835\udc65) . This solution, known as thesingle-choice algorithm, has two appealing properties: it is naturally history independent, and it has no recourse. However, the algorithm has a relatively large overload, e.g., when\ud835\udc5a\u2265\ud835\udc5blog\ud835\udc5b, the overload is\u0398( \u221a\ufe01 \ud835\udc5alog\ud835\udc5b)with high probability. Past work: The (history-dependent) power of two choices.By using both choices, instead of just one, one can get much better bounds on overload. If we consider a setting in which balls are inserted (but not deleted), then better overload can be achieved with thegreedy algorithm, which implements insertions by placing the new ball\ud835\udc65 in the less loaded of the two bins\u210e1(\ud835\udc65) and\u210e2(\ud835\udc65) .", "source": "loadbalancing.pdf"}, {"text": "(but not deleted), then better overload can be achieved with thegreedy algorithm, which implements insertions by placing the new ball\ud835\udc65 in the less loaded of the two bins\u210e1(\ud835\udc65) and\u210e2(\ud835\udc65) . This simple algorithm, which was first analyzed for\ud835\udc5a=\ud835\udc5b by Azar, Broder, Karlin and Up- fal [4] in 1994, ends up being surprisingly tricky to analyze for larger\ud835\udc5a, and it was only after a long line of work [4, 32, 13, 24, 23, 9] that Berenbrink, Czumaj, Steger, and V\u00f6cking [9] were able to achieve a tight bound, show- ing that the algorithm achieves overloadlog log\ud835\udc5b+\ud835\udc42( 1) with high probability in\ud835\udc5b (and independently of\ud835\udc5a). Whereas the single-choice algorithm was history independent, the greedy algorithm is emphatically not\u2014changes to the order in", "source": "loadbalancing.pdf"}, {"text": "the algorithm achieves overloadlog log\ud835\udc5b+\ud835\udc42( 1) with high probability in\ud835\udc5b (and independently of\ud835\udc5a). Whereas the single-choice algorithm was history independent, the greedy algorithm is emphatically not\u2014changes to the order in which elements are inserted can result in significantly different outcomes for the final state of the system. The algorithm also suffers from the fact that its overload guarantee, which holds for insertion-only workloads, actually fails for workloads with both insertions and deletions [5]. In fact, Bansal and Kuszmaul [5] give evidence (in the form of a lower bound for a large class of algorithms) that the only way to achieve good load balancing on fully dynamic workloads is with algorithms that incur non-zero recourse. Note that, if one is willing", "source": "loadbalancing.pdf"}, {"text": "large class of algorithms) that the only way to achieve good load balancing on fully dynamic workloads is with algorithms that incur non-zero recourse. Note that, if one is willing to incur recourse, then one can usetombstones3 in order to transform the greedy algorithm into a dynamic algorithm\u2014this maintains an overload bound of\ud835\udc42(log log\ud835\udc5b) while achieving amortized expected recourse\ud835\udc42(\ud835\udf07)=\ud835\udc42(\ud835\udc5a/\ud835\udc5b)per operation. In the data-structures literature, there has also been a great deal of work on non-greedy solutions that achieve even better bounds on overload [29, 14, 15]. Notably, Dietzfelbinger and Weidling [14] give an (again, highly history dependent) solution that achieves overload\u2264 1with high probability, while achieving expected 2In practice, there are many reasons, depending on the setting, that a system", "source": "loadbalancing.pdf"}, {"text": "[14] give an (again, highly history dependent) solution that achieves overload\u2264 1with high probability, while achieving expected 2In practice, there are many reasons, depending on the setting, that a system may choose to restrict each job to only two possible servers. In general, it allows the system to minimize the degree to which it duplicates certain types of resources. As a simple example, suppose that a job requires access to a stream of incoming data that is specific to that job. If the data is sent by external sources who do not know which server the job will be on, they can send the data to just two servers, instead of all\ud835\udc5bservers. 3This means that the algorithm simplymarkselements as deleted,", "source": "loadbalancing.pdf"}, {"text": "do not know which server the job will be on, they can send the data to just two servers, instead of all\ud835\udc5bservers. 3This means that the algorithm simplymarkselements as deleted, and then rebuilds the entire system (incurring significant recourse) every, say,\ud835\udc5boperations. 2 recourse \ud835\udc42(\ud835\udf07) .4 Whether or not a solution with overload\ud835\udc42( 1) and expected recourse\ud835\udc5c(\ud835\udf07) exists has, as far as we know, remained open. Past work: History independent solutions.For history-independent solutions, the only parameter regime that is well understood is the one where\ud835\udc5a<\ud835\udc5b/ 2 \u2212\u03a9(\ud835\udc5b) (i.e., \ud835\udf07= 1/2 \u2212\u03a9( 1)) [27]. Here, Naor, Segev, and Wieder [27] show that it is possible to achieve both overload and expected recourse\ud835\udc42( 1). This is achieved by applying a separate orientation algorithm", "source": "loadbalancing.pdf"}, {"text": "1)) [27]. Here, Naor, Segev, and Wieder [27] show that it is possible to achieve both overload and expected recourse\ud835\udc42( 1). This is achieved by applying a separate orientation algorithm to each connected component of the graph (\ud835\udc49=[\ud835\udc5b], \ud835\udc38= {(\u210e 1(\ud835\udc65), \u210e2(\ud835\udc65)) |\ud835\udc65\u2208 S}) , and by exploiting the fact that most of the components in the graph contain only a constant number of edges. This \u201csmall-component\u201d graph structure is special to the\ud835\udc5a<\ud835\udc5b/ 2 \u2212\u03a9(\ud835\udc5b) regime, and the question of whether efficient history-independent solutions also exist for larger\ud835\udc5a has remained open. 1.1 This Paper: The History-Independent Power of Two Choices In this paper, we construct a history-independent two-choice algorithm that simultaneously achieves overload \ud835\udc42( 1) with high probability in\ud835\udc5b, and", "source": "loadbalancing.pdf"}, {"text": "open. 1.1 This Paper: The History-Independent Power of Two Choices In this paper, we construct a history-independent two-choice algorithm that simultaneously achieves overload \ud835\udc42( 1) with high probability in\ud835\udc5b, and expected recourse\ud835\udc42(log log\ud835\udf07) where \ud835\udf07=\ud835\udc5a/\ud835\udc5b . Even for non-history- independent solutions, our algorithm is the first to achieve overload\ud835\udc42(1)with recourse\ud835\udc5c(\ud835\udf07). We remark that, due to the aforementioned unique representability property shown by Hartline et al. [19, 18], we can fully define any history-independent data structure by simply describing the data struc- ture\u2019s state given a setSof balls. This is the approach that we will take in discussing the algorithms below. Warmup: History-independent greedy.We begin in Section 3 by exploring what is arguably the simplest history-independent algorithm that one could consider", "source": "loadbalancing.pdf"}, {"text": "approach that we will take in discussing the algorithms below. Warmup: History-independent greedy.We begin in Section 3 by exploring what is arguably the simplest history-independent algorithm that one could consider (after single-choice), namely, thehistory- independent greedy algorithm. The algorithm calculates the allocation for a given setS of balls by assigning a canonical ordering \ud835\udc651 <\ud835\udc65 2 <\u00b7 \u00b7 \u00b7 to the balls in S, and then computing the allocation that the greedy algorithm would have produced if the balls were inserted in that order(\ud835\udc651, \ud835\udc652, . . .). For many data structural problems [ 27, 10, 20, 3, 30], this basic approach (i.e., applying a canonical ordering, and then inserting the items greedily in that order) ends up being an", "source": "loadbalancing.pdf"}, {"text": "data structural problems [ 27, 10, 20, 3, 30], this basic approach (i.e., applying a canonical ordering, and then inserting the items greedily in that order) ends up being an efficient or even optimal history- independent solution for that problem. This makes the algorithm a natural first candidate for our exploration. We show that the history-independent greedy algorithmdoesachieve a nontrivial bound: it incurs expected recourse \ud835\udc42(\ud835\udf07) while (trivially) offering an overload bound oflog log\ud835\udc5b+\ud835\udc42( 1). We also prove for a large range of parameters this recourse analysis is tight. A better algorithm: Slice and Spread.The main result of the paper is an alternative history-independent algorithm that achieves a doubly exponentially smaller expected recourse of\ud835\udc42(log log\ud835\udf07) while offering a high-probability overload", "source": "loadbalancing.pdf"}, {"text": "better algorithm: Slice and Spread.The main result of the paper is an alternative history-independent algorithm that achieves a doubly exponentially smaller expected recourse of\ud835\udc42(log log\ud835\udf07) while offering a high-probability overload of\ud835\udc42(1). We begin in Section 4.4 by presenting a basic version of the algorithm, which we call theSlice and Spread Algorithm, that achieves the desired recourse bound of\ud835\udc42(log log\ud835\udf07) but that comes with only a relatively weak overload bound. All that the basic version of the algorithm guarantees for overload is that theaverage number of balls above height\ud835\udc5a/\ud835\udc5b in each bin is\ud835\udc42( 1) \u2014 but the algorithm does not (a priori) say anything about the maximum. We then show in Sections 5.3 an (almost) black-box approach for history-independently transforming an", "source": "loadbalancing.pdf"}, {"text": "bin is\ud835\udc42( 1) \u2014 but the algorithm does not (a priori) say anything about the maximum. We then show in Sections 5.3 an (almost) black-box approach for history-independently transforming an algorithm with low expected overloadper bininto an algorithm with low maximum overloadacross all the bins, while only increasing expected recourse by a constant factor. This leads to the final result of the paper: 4Although the expected recourse in [14] is\ud835\udc42(\ud835\udf07) , the expectedtimeends up being much larger (super-polynomial in \ud835\udf07=\ud835\udc5a/\ud835\udc5b ). We remark that this will not be the case for any of the algorithms studied in this paper\u2014all of the algorithms that we propose can be straightforwardly implemented to incur time proportional to their recourse, and with\ud835\udc42(\ud835\udc5a)space used to", "source": "loadbalancing.pdf"}, {"text": "for any of the algorithms studied in this paper\u2014all of the algorithms that we propose can be straightforwardly implemented to incur time proportional to their recourse, and with\ud835\udc42(\ud835\udc5a)space used to store metadata. 3 Theorem 1.1.There exists a history-independent two-choice allocation algorithm A that achieves the following guarantees: The expected recourse of A is \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Furthermore, for each set S of balls, the overload induced byAis\ud835\udc42(1)with high probability in\ud835\udc5b. The surprising power of history independence.Our final result can be viewed as part of a larger trend in recent years, in which history-independent data structures have been able to achievebetter overall boundsthan the prior (non-history-independent!) states of the art [20, 8, 6]. These results suggest that history independence should be", "source": "loadbalancing.pdf"}, {"text": "in which history-independent data structures have been able to achievebetter overall boundsthan the prior (non-history-independent!) states of the art [20, 8, 6]. These results suggest that history independence should be viewed not just as a natural privacy guarantee, but also as a powerful algorithmic paradigm for building natural and efficient algorithms. 2 Preliminaries Dynamic balls and bins.We begin by reviewing the dynamic balls-and-bins setting. In a balls-and-bins system, there are a fixed number\ud835\udc5b of bins. Balls areinsertedinto anddeletedfrom the system dynamically over time. There are up to\ud835\udc5a balls in the system at any time. When a ball is inserted into the system, it is allocated to a bin. When a ball is deleted from the system, it is removed from", "source": "loadbalancing.pdf"}, {"text": "system at any time. When a ball is inserted into the system, it is allocated to a bin. When a ball is deleted from the system, it is removed from its bin. Additionally, as balls get inserted or deleted, balls that are currently in bins can get shifted around to other bins. The number of balls that get shifted during a given insertion or deletion is referred to as therecourseof the allocation algorithm during that operation. Balls come from the universeU, and we denoteS\ud835\udc61 \u2286 U as the set of balls currently in the system at time step \ud835\udc61. When there is no ambiguity, we abbreviate toS. An adversary can update the set of balls present in the system by", "source": "loadbalancing.pdf"}, {"text": "balls currently in the system at time step \ud835\udc61. When there is no ambiguity, we abbreviate toS. An adversary can update the set of balls present in the system by inserting or deleting one ball per time step. Thestate of the systemis (1) a set S \u2286 U of balls, (2) a set of\ud835\udc5b bins labeled1 , . . . , \ud835\udc5b, and (3) an allocation of each ball \ud835\udc65\u2208 S to exactly one of the \ud835\udc5b bins. Given a state, theload \u2113\ud835\udc56 of a bin \ud835\udc56 is the number of balls allocated to bin\ud835\udc56; themaximum loadis the maximum over all bins of the loads of the bins; theoverload is the amount by which the maximum load exceeds\ud835\udc5a/\ud835\udc5b. We will", "source": "loadbalancing.pdf"}, {"text": "of balls allocated to bin\ud835\udc56; themaximum loadis the maximum over all bins of the loads of the bins; theoverload is the amount by which the maximum load exceeds\ud835\udc5a/\ud835\udc5b. We will also refer to thecumulative overload, which is the total amount by which the bins\u2019 loads exceed\ud835\udc5a/\ud835\udc5b(i.e.,\u00cd\ud835\udc5b \ud835\udc56=1 max(\u2113\ud835\udc56 \u2212\ud835\udc5a/\ud835\udc5b,0)). The two-choice paradigm.In the two-choice paradigm, the system gets two fully random hash functions \u210e1, \u210e2: U \u2192 [\ud835\udc5b] . The system maintains the invariant that, for all balls\ud835\udc65 present in the system,\ud835\udc65 is allocated either to bin\u210e1(\ud835\udc65) or \u210e2(\ud835\udc65) , which are the twoallowable locations for \ud835\udc99. In particular, in each time step\ud835\udc61, theallocatorassigns (or reassigns) each ball \ud835\udc65\u2208 S \ud835\udc61 to one of its two allowable locations. The allocator", "source": "loadbalancing.pdf"}, {"text": "which are the twoallowable locations for \ud835\udc99. In particular, in each time step\ud835\udc61, theallocatorassigns (or reassigns) each ball \ud835\udc65\u2208 S \ud835\udc61 to one of its two allowable locations. The allocator may be randomized, in which case, we can model the system as having an additional infinite string\ud835\udc45 of random bits (distinct from the random hash functions\u210e1 and\u210e 2). The goal is to design an allocation strategy in the two-choice setting that achieves a good high-probability bound on the overload for each state, while simultaneously minimizing the expected number of ball move- ments per insert/delete. History-independent two-choice allocation.A two-choice allocation algorithm is history indepen- dent if, for each S, \u210e1, \u210e2, and \ud835\udc45, the allocator allocates all \ud835\udc65\u2208 S to bins", "source": "loadbalancing.pdf"}, {"text": "move- ments per insert/delete. History-independent two-choice allocation.A two-choice allocation algorithm is history indepen- dent if, for each S, \u210e1, \u210e2, and \ud835\udc45, the allocator allocates all \ud835\udc65\u2208 S to bins according to some function \ud835\udf19 S,\u210e1,\u210e2,\ud835\udc45 : S \u2192 [\ud835\udc5b] . When the context is clear, we will drop the subscripts\u210e1, \u210e2, and \ud835\udc45 and simply write\ud835\udf19 S. Since our allocation is history independent, it is not a function of time but rather only a function ofS (and additional randomness). When analyzing the overload of the algorithm, we\u2019ll want to show that, for any setS, the overload is small with high probability in\ud835\udc5b, where the randomness is over\u210e1,\u210e 2, and\ud835\udc45. 4 Recourse for history independent solutions.Let two sets S,S \u2032", "source": "loadbalancing.pdf"}, {"text": "show that, for any setS, the overload is small with high probability in\ud835\udc5b, where the randomness is over\u210e1,\u210e 2, and\ud835\udc45. 4 Recourse for history independent solutions.Let two sets S,S \u2032 \u2286 U beneighboring setsif their contents differ by exactly one ball, i.e.|S\u25b3S \u2032|= 1.5 For any pair (S,S \u2032) of neighboring sets, and for any \u210e1, \u210e2, \ud835\udc45, we are interested in therecourseof (S,S \u2032, \u210e1, \u210e2, \ud835\udc45), which is the number of balls whose allocations change betweenSandS \u2032. More formally, it is defined as Recourse (S,S \u2032, \u210e1, \u210e2, \ud835\udc45) :=1+ \u2211\ufe01 \ud835\udc65\u2208 S\u2229S \u2032 I \u0000\ud835\udf19 S,\u210e1,\u210e2,\ud835\udc45 (\ud835\udc65)\u2260\ud835\udf19 S\u2032,\u210e1,\u210e2,\ud835\udc45 (\ud835\udc65)\u0001 .6 (1) The expected recourseRecourse(A)of an allocation algorithmAis defined as: Recourse(A) :=max S,S \u2032 | S\u25b3 S\u2032 |", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc45) :=1+ \u2211\ufe01 \ud835\udc65\u2208 S\u2229S \u2032 I \u0000\ud835\udf19 S,\u210e1,\u210e2,\ud835\udc45 (\ud835\udc65)\u2260\ud835\udf19 S\u2032,\u210e1,\u210e2,\ud835\udc45 (\ud835\udc65)\u0001 .6 (1) The expected recourseRecourse(A)of an allocation algorithmAis defined as: Recourse(A) :=max S,S \u2032 | S\u25b3 S\u2032 | \u22641 E \u210e1,\u210e2,\ud835\udc45 [Recourse(S,S \u2032, \u210e1, \u210e2, \ud835\udc45)].(2) A WLOG assumption on the number of balls.We conclude the section by noting one WLOG assumption that will be helpful in some sections of the paper. Note that, by taking each set of size\ud835\udc58<\ud835\udc5a balls and adding (\ud835\udc5a\u2212\ud835\udc58) dummy balls \ud835\udc511, . . . , \ud835\udc51\ud835\udc58 (these are fake balls to pad the size of the set), we can treat any set of size less than\ud835\udc5a as a set of size\ud835\udc5a. Furthermore, by alternating between\ud835\udc5a and \ud835\udc5a\u2212 1balls, we can get between", "source": "loadbalancing.pdf"}, {"text": "the size of the set), we can treat any set of size less than\ud835\udc5a as a set of size\ud835\udc5a. Furthermore, by alternating between\ud835\udc5a and \ud835\udc5a\u2212 1balls, we can get between any two sets of size\ud835\udc5a. Therefore, when designing history-independent allocation algorithms, we may assume without loss of generality that all of our sets have sizes\ud835\udc5a\u22121or\ud835\udc5a, and, in particular, that neighboring setsSandS \u2032 have sizes\ud835\udc5a\u22121and\ud835\udc5a. 3 The History-Independent Greedy Algorithm We now formally describe the history-independent greedy allocation strategy, which we callHI Greedy. For each S, \u210e1, \u210e2, and \ud835\udc45, the HI Greedy allocator\ud835\udf19 S,\u210e1,\u210e2,\ud835\udc45 can be computed via the following procedure: We imagine that all of the bins are initialized as empty, and that the procedure inserts all of the", "source": "loadbalancing.pdf"}, {"text": "HI Greedy allocator\ud835\udf19 S,\u210e1,\u210e2,\ud835\udc45 can be computed via the following procedure: We imagine that all of the bins are initialized as empty, and that the procedure inserts all of the balls\ud835\udc65\u2208 S one-by- one into the system according to some fixed canonical ordering of the balls. When each\ud835\udc65\u2208 S is inserted, \ud835\udc65 is then allocated to the bin in{\u210e1(\ud835\udc65), \u210e2(\ud835\udc65)} that has the smaller load at that moment (ties are broken by using\u210e1). The following proposition describes both the recourse and overload of HI Greedy. Proposition 3.1.The expected recourse of HI Greedy is \ud835\udc42(\ud835\udc5a/\ud835\udc5b) . Furthermore, for each set S, with high probability in\ud835\udc5b, the overload islog log\ud835\udc5b+\ud835\udc42(1). Proof. The overload beinglog log\ud835\udc5b+\ud835\udc42( 1) is directly implied by Berenbrink, Czumaj,", "source": "loadbalancing.pdf"}, {"text": "HI Greedy is \ud835\udc42(\ud835\udc5a/\ud835\udc5b) . Furthermore, for each set S, with high probability in\ud835\udc5b, the overload islog log\ud835\udc5b+\ud835\udc42(1). Proof. The overload beinglog log\ud835\udc5b+\ud835\udc42( 1) is directly implied by Berenbrink, Czumaj, Steger, and V\u00f6cking [9]. Below we focus on proving the recourse. Fix two arbitrary neighboring setsS and S\u2032 =S \u222a {\ud835\udc65 \u2217} that differ in only one ball denoted\ud835\udc65 \u2217. Consider two parallel worlds\u2013called world0and world1\u2013in which we insert the balls in S and S\u2032, respectively, one-by-one according to the canonical ordering. We imagine that we perform one insertion in each of the two worlds in each time step, and we will analyze the difference in the bin loads in the two worlds in each time step. Let \ud835\udc61 \u2217", "source": "loadbalancing.pdf"}, {"text": "each of the two worlds in each time step, and we will analyze the difference in the bin loads in the two worlds in each time step. Let \ud835\udc61 \u2217 be the time step when\ud835\udc65 \u2217 is inserted into world1. (We may assume that nothing happens in world0at this time step.) For each time step before\ud835\udc61 \u2217, the two worlds have identical configurations. However, at time step \ud835\udc61 \u2217, exactly one bin denoted\ud835\udc56\u2217 differs in these two worlds. Specifically, bin\ud835\udc56\u2217 picks up one more ball in world1. The key to bounding recourse is to observe the following fact: Fact 3.2.At the end of every time step \ud835\udc61\u2265\ud835\udc61 \u2217, exactly one bin differs in load in the two worlds, and in", "source": "loadbalancing.pdf"}, {"text": "bounding recourse is to observe the following fact: Fact 3.2.At the end of every time step \ud835\udc61\u2265\ud835\udc61 \u2217, exactly one bin differs in load in the two worlds, and in this special bin, world1has one more ball than world0. 5The symmetric difference of sets\ud835\udc34and\ud835\udc35is defined as\ud835\udc34\u25b3\ud835\udc35 :=(\ud835\udc34\\\ud835\udc35) \u222a (\ud835\udc35\\\ud835\udc34). 6Iis the indicator function that maps true to1and false to0. 5 Proof. We can prove this fact inductively for each\ud835\udc61\u2265\ud835\udc61 \u2217, where we have just seen that it is true when\ud835\udc61=\ud835\udc61 \u2217. Suppose the claim is true for some time step\ud835\udc61=\ud835\udc58 . We show that the claim is then true for time step \ud835\udc61=\ud835\udc58+ 1(assuming the time step exists). When we insert the ball\ud835\udc65 in both worlds in time step\ud835\udc58+ 1,", "source": "loadbalancing.pdf"}, {"text": ". We show that the claim is then true for time step \ud835\udc61=\ud835\udc58+ 1(assuming the time step exists). When we insert the ball\ud835\udc65 in both worlds in time step\ud835\udc58+ 1, either (1)\ud835\udc65 \u2019s allocation is the same in both worlds, or(2)\ud835\udc65 \u2019s allocation is different in both worlds. In case1, the special bin from time step\ud835\udc58 remains a special bin in time step\ud835\udc58+ 1, and furthermore remains the only special bin, thus proving the claim for time step\ud835\udc58+1in this case. In case2, suppose that \ud835\udc65 was allocated to some bin\ud835\udc560 in world0and is allocated to some different bin \ud835\udc561 in world1. Note that this is only possible if the special bin is\ud835\udc560. We prove the claim by imagining that in", "source": "loadbalancing.pdf"}, {"text": "in world0and is allocated to some different bin \ud835\udc561 in world1. Note that this is only possible if the special bin is\ud835\udc560. We prove the claim by imagining that in time step \ud835\udc58+ 1we first insert \ud835\udc65 into world0and then insert \ud835\udc65 into world1. When we first insert \ud835\udc65 into world 0, the load of bin\ud835\udc560 in world0catches up to its load in world1; in other words, none of the bins at this point are special anymore. When\ud835\udc65 is then inserted into world1, bin\ud835\udc561 in world1gets an extra ball and becomes the special bin between the two worlds, thus proving the claim.\u25a1 Completing the proof of Proposition 3.1.When a ball is inserted, it is only possible for it to enter", "source": "loadbalancing.pdf"}, {"text": "becomes the special bin between the two worlds, thus proving the claim.\u25a1 Completing the proof of Proposition 3.1.When a ball is inserted, it is only possible for it to enter different bins in the two worlds if its two choices include the differing bin in that time step. The probability that it happens to choose the differing bin is\ud835\udc42( 1/\ud835\udc5b). Therefore, among all\u2264\ud835\udc5a balls inserted, in expectation \ud835\udc42(\ud835\udc5a/\ud835\udc5b) of them will enter different bins in the two worlds. Since these are the only balls that contribute to the recourse, the expected recourse is upper bounded by\ud835\udc42(\ud835\udc5a/\ud835\udc5b).\u25a1 We also prove a lower bound on the expected recourse of HI Greedy, showing that so long as\ud835\udc5a=\ud835\udc5b 2\u2212\u03a9(1) , the expected recourse isat", "source": "loadbalancing.pdf"}, {"text": "expected recourse is upper bounded by\ud835\udc42(\ud835\udc5a/\ud835\udc5b).\u25a1 We also prove a lower bound on the expected recourse of HI Greedy, showing that so long as\ud835\udc5a=\ud835\udc5b 2\u2212\u03a9(1) , the expected recourse isat least\u03a9(\ud835\udc5a/\ud835\udc5b). For brevity, the proof is deferred to Appendix A. Proposition 3.3.If\ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , then the expected recourse of HI Greedy is\u03a9(\ud835\udc5a/\ud835\udc5b). 4 Improving the Overload: Slicing and Spreading In this section, we introduce the Slice and Spread Algorithm (Algorithm 1), whose guarantees are given in Theorem 4.1. Theorem 4.1.The expected recourse of Slice and Spread is \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Furthermore, for each state, the cumulative overload of that state under Slice and Spread is\ud835\udc42(\ud835\udc5b)with high probability in\ud835\udc5b. Throughout the section, we will make the WLOG assumption from Section 2", "source": "loadbalancing.pdf"}, {"text": "Furthermore, for each state, the cumulative overload of that state under Slice and Spread is\ud835\udc42(\ud835\udc5b)with high probability in\ud835\udc5b. Throughout the section, we will make the WLOG assumption from Section 2 that the number of balls is always either\ud835\udc5aor\ud835\udc5a\u22121. 4.1 Slice and Spread Overview The slice-and-spread gadget.At the heart of Slice and Spread is a key gadget that we will refer to as the slice-and-spread gadget. The gadget first takes as input an initial allocation of balls to bins, where each ball \ud835\udc65 is placed into\u210e1(\ud835\udc65) . The gadget works by first picking someslicing threshold \u2113. Then, for all bins that have more than\u2113 balls, the gadget tries to temporarily remove balls from each of these bins so that each bin\u2019s", "source": "loadbalancing.pdf"}, {"text": "first picking someslicing threshold \u2113. Then, for all bins that have more than\u2113 balls, the gadget tries to temporarily remove balls from each of these bins so that each bin\u2019s load is reduced to\u2113. (For reasons we will come back to later, this will sometimes fail.) One can imagine the gadget as cleanly slicing off the top of each bin so each bin has height\u2113. Finally, the gadget reallocates the removed balls, putting each removed ball\ud835\udc65 into \u210e2(\ud835\udc65) . This is the \u201cspreading\u201d part, as one can imagine the gadget spreading out the removed balls among the bins. 6 Reducing the overload.The slice-and-spread gadget can be used to reduce the overload. To see how, imagine the scenario in which\ud835\udc5a>>\ud835\udc5b balls", "source": "loadbalancing.pdf"}, {"text": "spreading out the removed balls among the bins. 6 Reducing the overload.The slice-and-spread gadget can be used to reduce the overload. To see how, imagine the scenario in which\ud835\udc5a>>\ud835\udc5b balls are allocated into\ud835\udc5b bins according to their first-choice hash. By a Chernoff bound, with high probability in\ud835\udc5b, this allocation has overload\ud835\udc65=\ud835\udc42 \u0010\u221a\ufe01 \ud835\udc5a/\ud835\udc5b\u00b7 \u221a\ufe01 log\ud835\udc5b+log\ud835\udc5b \u0011 and furthermore, every bin has load between\ud835\udc5a/\ud835\udc5b\u00b1\ud835\udc65 with high probability in\ud835\udc5b. Suppose the gadget picks the slicing threshold to be approximately the height\ud835\udc5a/\ud835\udc5b\u2212\ud835\udc65 (the final algorithm will need to pick the threshold more carefully). Then, the gadget will remove roughly \ud835\udc42(\ud835\udc65\ud835\udc5b)=\ud835\udc42( \u221a\ud835\udc5a\ud835\udc5b) balls from the system and put them into their second hash. Since the second hashes are independent and uniformly random, then this", "source": "loadbalancing.pdf"}, {"text": "Then, the gadget will remove roughly \ud835\udc42(\ud835\udc65\ud835\udc5b)=\ud835\udc42( \u221a\ud835\udc5a\ud835\udc5b) balls from the system and put them into their second hash. Since the second hashes are independent and uniformly random, then this is equivalent to throwing\u221a\ud835\udc5a\ud835\udc5bballs into\ud835\udc5bbins. The new overload is reduced to \u02dc\ud835\udc42 \u0000\u221a\ud835\udc65\u0001. Repeatedly applying the slice-and-spread gadget.The algorithm repeatedly applies the slice-and- spread gadget in order to continually reduce the overload. The algorithm proceeds in rounds, where in each round, the gadget carefully selects a slicing threshold so that intuitively most of the jaggedness from the bins gets sliced and then spread. However, in order for the slice and spread to reduce the overload, the gadget must be careful to only slice away balls that have not been previously", "source": "loadbalancing.pdf"}, {"text": "and then spread. However, in order for the slice and spread to reduce the overload, the gadget must be careful to only slice away balls that have not been previously sliced before. This is so that every time balls are spread, they are making use of the fresh randomness coming from their\u210e2. To do this (and, specifically, to do this with good recourse), the algorithm gives balls inS a round assignment, where a ball can only be evicted in the round if it is also assigned to that round. The details of the round assignment are given in Section 4.2. With the round assignments, and with carefully-chosen slicing thresholds, the algorithm is able to reduce the cumulative overload to\ud835\udc42(\ud835\udc5b). Gadget", "source": "loadbalancing.pdf"}, {"text": "details of the round assignment are given in Section 4.2. With the round assignments, and with carefully-chosen slicing thresholds, the algorithm is able to reduce the cumulative overload to\ud835\udc42(\ud835\udc5b). Gadget failures.Before we present the algorithm in its entirety, it is worth identifying what will end up being one of the primary challenges in the analysis of its overload. In some cases, a bin may contain many balls, but very few (or no) balls assigned to the current round. In this case the slice-and-spread gadget will not be able to bring the load down to the threshold for the current round. These types ofslicing failurescan be avoided in rounds of the algorithm where the number of sliced balls is very large,", "source": "loadbalancing.pdf"}, {"text": "the load down to the threshold for the current round. These types ofslicing failurescan be avoided in rounds of the algorithm where the number of sliced balls is very large, but become unavoidable in rounds where the number of such balls is\ud835\udc5c(\ud835\udc5blog\ud835\udc5b). Somewhat subtly,spreading failuresalso turn out to be a problem. This happens when the slicing gadget fails to place enough balls in a bin for it toreachthe threshold in the next round of the algorithm. Spreading failures in one round increase the number of balls that get sliced in the next, and can prevent the number of balls in play from decreasing at the necessary rate between consecutive rounds. A key insight in the algorithm is that, if we", "source": "loadbalancing.pdf"}, {"text": "the next, and can prevent the number of balls in play from decreasing at the necessary rate between consecutive rounds. A key insight in the algorithm is that, if we pick our thresholds correctly, then even if we allow slicing and spreading failures to occur, we can still bound the final number of balls that reside above height\ud835\udc5a/\ud835\udc5b. At the same time, we will be able to keep the number of rounds that our algorithm has to be\ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) , which will in turn allow us to bound the expected recourse. 4.2 Slice and Spread Full Description The Slice and Spread Algorithm begins by initializing parameters. These parameters are assumed in the Slice and Spread procedure described in Algorithm 1.", "source": "loadbalancing.pdf"}, {"text": "recourse. 4.2 Slice and Spread Full Description The Slice and Spread Algorithm begins by initializing parameters. These parameters are assumed in the Slice and Spread procedure described in Algorithm 1. Variable initialization.Let \ud835\udf07 :=\ud835\udc5a/\ud835\udc5b and let\ud835\udc47 :=log 4/3 log(\ud835\udf07) . We refer to each\ud835\udc61\u2208 [\ud835\udc47] as around. For each round\ud835\udc61\u2208 [\ud835\udc47], the algorithm initializes the following: 7 \u2022\ud835\udf07 \ud835\udc61 :=(\ud835\udf07 \ud835\udc61\u22121 )3/4, where\ud835\udf07 0 :=\ud835\udf07. \u2022\ud835\udc5a \ud835\udc61 :=\ud835\udf07 \ud835\udc61 \u00b7\ud835\udc5b, where\ud835\udc5a 0 :=\ud835\udc5a. \u2022\u2113 (\ud835\udc61) :=\ud835\udf07\u2212\ud835\udf07 \ud835\udc61, where\u2113 (0) :=0. To a first approximation, we can think of\ud835\udf07\ud835\udc61 as being the expected number of balls each bin receives during the spreading stage of each round\ud835\udc61 if we start with |S|=\ud835\udc5a and if we never incur any gadget failures; we can", "source": "loadbalancing.pdf"}, {"text": "the expected number of balls each bin receives during the spreading stage of each round\ud835\udc61 if we start with |S|=\ud835\udc5a and if we never incur any gadget failures; we can also think of\ud835\udc5a\ud835\udc61 as the number of balls that are thrown in each round\ud835\udc61 in this scenario. The values\u2113 (\ud835\udc61) will be the heights at which the algorithm tries to slice each bin during round\ud835\udc61, and we will refer to them asslicing thresholds. We remark that, to simplify our exposition throughout the section, we will treat each\ud835\udf07\ud835\udc61 as an integer\u2014this allows us to avoid having to propagate a large number of floors and ceilings throughout the section. Round assignments.Using \ud835\udc45, the algorithm also initializes each ball \ud835\udc65\u2208 S to be", "source": "loadbalancing.pdf"}, {"text": "allows us to avoid having to propagate a large number of floors and ceilings throughout the section. Round assignments.Using \ud835\udc45, the algorithm also initializes each ball \ud835\udc65\u2208 S to be around-assigned ballwith probability0 .01. (We remark that the reason for assigning balls to rounds with probability0.01 becomes apparent only in Section 5.3.) The algorithm makes the additional initializations related specifically to round-assigned balls: \u2022\ud835\udc50 : U \u2192 [\ud835\udc47] maps each round-assigned ball \ud835\udc65\u2208 S to a round \ud835\udc50(\ud835\udc65) such that each ball independently satisfies Pr[\ud835\udc50(\ud835\udc65)=\ud835\udc61]=\u0398 \u0010\ud835\udc5a\ud835\udc61\u22121 \ud835\udc5a \u0011 for each\ud835\udc61\u2208 [\ud835\udc47]. \u2022C (\ud835\udc61) \ud835\udc56 :={\ud835\udc65\u2208 S |\u210e 1(\ud835\udc65)=\ud835\udc56and\ud835\udc50(\ud835\udc65)=\ud835\udc61}for each(\ud835\udc61, \ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b]. We now give the full details of Slice and Spread below in Algorithm 1. Algorithm 1Slice", "source": "loadbalancing.pdf"}, {"text": "[\ud835\udc47]. \u2022C (\ud835\udc61) \ud835\udc56 :={\ud835\udc65\u2208 S |\u210e 1(\ud835\udc65)=\ud835\udc56and\ud835\udc50(\ud835\udc65)=\ud835\udc61}for each(\ud835\udc61, \ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b]. We now give the full details of Slice and Spread below in Algorithm 1. Algorithm 1Slice and Spread. 1:procedureSliceAndSpread(S, \u210e 1, \u210e2, \ud835\udc45) 2:Place each\ud835\udc65\u2208 Sinto bin\u210e 1(\ud835\udc65). 3:foreach round\ud835\udc61=1to\ud835\udc47do 4:Slice:For each\ud835\udc56\u2208 [\ud835\udc5b], let\u2113 (\ud835\udc61) \ud835\udc56 be the current number of balls in bin\ud835\udc56. For each\ud835\udc56\u2208 [\ud835\udc5b], take the 5:min(|C (\ud835\udc61) \ud835\udc56 |, \u2113 (\ud835\udc61) \ud835\udc56 \u2212\u2113 (\ud835\udc61) )-first balls fromC (\ud835\udc61) \ud835\udc56 according to a canonical total ordering, remove 6:(evict) them from bin\ud835\udc56, and add them to therethrow setP (\ud835\udc61) . 7:Spread:For each ball\ud835\udc65\u2208 P (\ud835\udc61) , place\ud835\udc65into bin\u210e 2(\ud835\udc65). 8:end for 9:end procedure 4.3 The Cumulative Overload of Slice and Spread In this section,", "source": "loadbalancing.pdf"}, {"text": "them to therethrow setP (\ud835\udc61) . 7:Spread:For each ball\ud835\udc65\u2208 P (\ud835\udc61) , place\ud835\udc65into bin\u210e 2(\ud835\udc65). 8:end for 9:end procedure 4.3 The Cumulative Overload of Slice and Spread In this section, we bound the cumulative overload of Slice and Spread: Proposition 4.2.For each S \u2286 U , if |S| \u2264\ud835\udc5a , then after applying Slice and Spread to S, the cumulative overload is\ud835\udc42(\ud835\udc5b)with high probability in\ud835\udc5b. 8 To show that the cumulative overload is small, we want to be arguing that Slice and Spread is actually \u201csmoothing out\u201d the bin loads in every round. Ideally, in each round, it is taking balls from bins with large loads and distributing them to bins with small loads. There are a couple of situations", "source": "loadbalancing.pdf"}, {"text": "in every round. Ideally, in each round, it is taking balls from bins with large loads and distributing them to bins with small loads. There are a couple of situations which make it difficult for Slice and Spread to smooth out the loads. For example, during the slicing stage in a round\ud835\udc61, there may be bins that cannot be sliced down to\u2113 (\ud835\udc61) . This is because of at least one of two reasons: either the bin doesn\u2019t have enough round-\ud835\udc61 assigned balls in its possession to support the removal of balls, or the bin\u2019s load may be exceedingly high. Additionally, there may be bins whose loads are already below the slicing threshold\u2113 (\ud835\udc61) . This implies that an earlier", "source": "loadbalancing.pdf"}, {"text": "of balls, or the bin\u2019s load may be exceedingly high. Additionally, there may be bins whose loads are already below the slicing threshold\u2113 (\ud835\udc61) . This implies that an earlier round of Slice and Spread failed to smooth out balls into this bin. It\u2019s important to note that overfilled and underfilled bins are connected to each other: having a lot of overfilled bins means that there may be more underfilled bins, and vice versa. More generally, there can be cause-and-effect loops between rounds. If a bin is overloaded (resp. underloaded) in one round, it is more likely to continue being overloaded (resp. underloaded) in the next. And, more subtly, if there are many underfilled bins in one round, then this", "source": "loadbalancing.pdf"}, {"text": "in one round, it is more likely to continue being overloaded (resp. underloaded) in the next. And, more subtly, if there are many underfilled bins in one round, then this causes more balls to be above height\u2113 (\ud835\udc61) in that round (namely, the balls that should have been below the threshold in the underfilled bins), which causes more balls to get sliced and spread in that round, which causes more overloaded bins in the next round. Bounding the effect of these types of feedback loops, and more generally bounding the effect of overfilled and underfilled bins on the algorithm, is the main technical challenge in this section. To prove Proposition 4.2, we will make use of some additional notation and", "source": "loadbalancing.pdf"}, {"text": "effect of overfilled and underfilled bins on the algorithm, is the main technical challenge in this section. To prove Proposition 4.2, we will make use of some additional notation and definitions. To simplify our indexing of variables, throughout the section, we will consider there to be around 0in which there is a spreading stage (given by Line 7 in Algorithm 1) but no slicing stage. With this convention in mind, for\ud835\udc61\u2208 { 0, . . . , \ud835\udc47} and \ud835\udc56\u2208 [\ud835\udc5b] , define \u02dc\u2113 (\ud835\udc61) \ud835\udc56 to be the number of balls in bin \ud835\udc56 immediately prior to the spreading stage in round\ud835\udc61. (For \ud835\udc61= 0, this gives \u02dc\u2113 (\ud835\udc61) \ud835\udc56 = 0.) A bin isunderfilled bad in round \ud835\udc95", "source": "loadbalancing.pdf"}, {"text": "of balls in bin \ud835\udc56 immediately prior to the spreading stage in round\ud835\udc61. (For \ud835\udc61= 0, this gives \u02dc\u2113 (\ud835\udc61) \ud835\udc56 = 0.) A bin isunderfilled bad in round \ud835\udc95 if \u02dc\u2113 (\ud835\udc61) \ud835\udc56 <\u2113 (\ud835\udc61) and isoverfilled bad in round \ud835\udc95 if \u02dc\u2113 (\ud835\udc61) \ud835\udc56 >\u2113 (\ud835\udc61) . Define theround- \ud835\udc95 errorof bin \ud835\udc56 to be | \u02dc\u2113 (\ud835\udc61) \ud835\udc56 \u2212\u2113 (\ud835\udc61) |. Define \ud835\udf00 \u2212 \ud835\udc61 (resp. \ud835\udf00+ \ud835\udc61 ) to be the sum of the errors of the underfilled bad (resp. overfilled bad) bins in round\ud835\udc61, and define\ud835\udf00 \ud835\udc61 :=\ud835\udf00 \u2212 \ud835\udc61 +\ud835\udf00 + \ud835\udc61 . We begin our analysis with a technical lemma about throwing balls into bins. We remark that the proof of this lemma", "source": "loadbalancing.pdf"}, {"text": "define\ud835\udf00 \ud835\udc61 :=\ud835\udf00 \u2212 \ud835\udc61 +\ud835\udf00 + \ud835\udc61 . We begin our analysis with a technical lemma about throwing balls into bins. We remark that the proof of this lemma makes use of McDiarmid\u2019s Inequality [21] (which we restate for reference in Appendix B). Lemma 4.3.Let \ud835\udf07\u2265 1and suppose we throw \ud835\udc5a=\ud835\udf07\ud835\udc5b\u00b1\ud835\udc42(\ud835\udc5b) balls into \ud835\udc5b bins uniformly at random. Let \ud835\udc4b\ud835\udc56 denote the number of balls in each bin \ud835\udc56, and call bin \ud835\udc56 imbalancedif |\ud835\udc4b\ud835\udc56 \u2212\ud835\udf07| \u2265\ud835\udf07 3/4. Define \ud835\udc34\ud835\udc56 to be0if bin \ud835\udc56 is balanced, and to be |\ud835\udf07\u2212\ud835\udc4b \ud835\udc56 | if bin \ud835\udc56 is imbalanced. Then, with high probability in \ud835\udc5a, the number of imbalanced bins is at most\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) and \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 \u2264\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) ,", "source": "loadbalancing.pdf"}, {"text": "|\ud835\udf07\u2212\ud835\udc4b \ud835\udc56 | if bin \ud835\udc56 is imbalanced. Then, with high probability in \ud835\udc5a, the number of imbalanced bins is at most\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) and \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 \u2264\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) , where the\ud835\udf14(1)term is parameterized by\ud835\udf07(not\ud835\udc5b). Proof. Since the number of imbalanced bins is at most\u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56, it suffices to show that\u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 \u2264\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) . Furthermore, since the result is trivially true for small\ud835\udf07=\ud835\udc42( 1), we may assume that \ud835\udf07 is at least a large positive constant, and therefore also that\ud835\udc5a\u2265\ud835\udc5b. To begin the analysis, let us boundE[\ud835\udc34\ud835\udc56]. By a Chernoff bound, we have for all\ud835\udc57\u2265 \u221a \u210ethat Pr[|\ud835\udc4b \ud835\udc56 \u2212\ud835\udf07| \u2265\ud835\udc57] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57/ \u221a\ud835\udf07) .(3) 9 We can therefore bound E[\ud835\udc34\ud835\udc56] \u2264\ud835\udf07 3/4 \u00b7Pr[\ud835\udc34 \ud835\udc56 \u22600] +", "source": "loadbalancing.pdf"}, {"text": "By a Chernoff bound, we have for all\ud835\udc57\u2265 \u221a \u210ethat Pr[|\ud835\udc4b \ud835\udc56 \u2212\ud835\udf07| \u2265\ud835\udc57] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57/ \u221a\ud835\udf07) .(3) 9 We can therefore bound E[\ud835\udc34\ud835\udc56] \u2264\ud835\udf07 3/4 \u00b7Pr[\ud835\udc34 \ud835\udc56 \u22600] + \u2211\ufe01 \ud835\udc57>\ud835\udf07 3/4 Pr[\ud835\udc34\ud835\udc56 \u2265\ud835\udc57] =\ud835\udf07 3/4 \u00b7Pr[|\ud835\udc4b \ud835\udc56 \u2212\ud835\udf07|>\ud835\udf07 3/4] + \u2211\ufe01 \ud835\udc57>\ud835\udf07 3/4 Pr[|\ud835\udc4b \ud835\udc56 \u2212\ud835\udf07| \u2265\ud835\udc57] \u2264\ud835\udf07 3/4 \u00b7\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) + \u2211\ufe01 \ud835\udc57>\ud835\udf07 3/4 \ud835\udc52 \u2212\u03a9(\ud835\udc57/ \u221a\ud835\udf07) (by (3)) \u2264\ud835\udf07 3/4 \u00b7\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) +\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) \u2211\ufe01 \ud835\udc5f>0 \ud835\udc52 \u2212\u03a9(\ud835\udc5f/ \u221a\ud835\udf07) \u2264\ud835\udf07 3/4 \u00b7\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) +\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) \u00b7 1 1\u2212\ud835\udc52 \u03a9(1/ \u221a\ud835\udf07) (since \u00cd \ud835\udc61\ud835\udc5f =1/(1\u2212\ud835\udc61)for\ud835\udc61<1) \u2264\ud835\udf07 3/4 \u00b7\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) +\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) \u00b7\u0398( \u221a\ud835\udf07)(since1\u2212\ud835\udc52 1/\ud835\udc65 =\u0398(1/\ud835\udc65)for\ud835\udc65\u22651) \u2264\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 )+\ud835\udc42(log\ud835\udf07) \u2264\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 )", "source": "loadbalancing.pdf"}, {"text": "1 1\u2212\ud835\udc52 \u03a9(1/ \u221a\ud835\udf07) (since \u00cd \ud835\udc61\ud835\udc5f =1/(1\u2212\ud835\udc61)for\ud835\udc61<1) \u2264\ud835\udf07 3/4 \u00b7\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) +\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) \u00b7\u0398( \u221a\ud835\udf07)(since1\u2212\ud835\udc52 1/\ud835\udc65 =\u0398(1/\ud835\udc65)for\ud835\udc65\u22651) \u2264\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 )+\ud835\udc42(log\ud835\udf07) \u2264\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) . It follows that E \" \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc34\ud835\udc56 # \u2264\ud835\udc5b\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) . We complete the proof by considering two parameter regimes, depending on whether\ud835\udf07\u2265log 8 \ud835\udc5a. If \ud835\udf07\u2265log 8 \ud835\udc5a, then E \" \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc34\ud835\udc56 # \u2264\ud835\udc5b\ud835\udc52 \u2212\u03a9(log 2 \ud835\udc5a) \u2264\ud835\udc5a \u2212\ud835\udf14(1) , where the final inequality uses the fact that\ud835\udc5a\u2265\ud835\udc5b . By Markov\u2019s inequality, it follows thatPr[\u00cd \ud835\udc34\ud835\udc56 \u2265 1] \u2264 \ud835\udc5a\u2212\ud835\udf14(1) . Since\u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 is either0or at least1, it follows that \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 = 0 \u2264\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) with high probability in\ud835\udc5a. On", "source": "loadbalancing.pdf"}, {"text": "follows thatPr[\u00cd \ud835\udc34\ud835\udc56 \u2265 1] \u2264 \ud835\udc5a\u2212\ud835\udf14(1) . Since\u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 is either0or at least1, it follows that \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56 = 0 \u2264\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) with high probability in\ud835\udc5a. On the other hand, if \ud835\udf07\u2264log 8 \ud835\udc5a, then we can complete the proof with McDiarmid\u2019s inequality. Define \ud835\udc4c1, . . . , \ud835\udc4c\ud835\udc5a to be the independent random bin choices for each of the balls1 , . . . , \ud835\udc5a, and define \ud835\udc39(\ud835\udc4c 1, . . . , \ud835\udc4c\ud835\udc5a)= \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc34\ud835\udc56. Since \ud835\udc39 is a \ud835\udf073/4-Lipschitz function determined by\ud835\udc5a independent random variables, we have by McDiarmid\u2019s inequality (Theorem B.1) that Pr[|\ud835\udc39\u2212E[\ud835\udc39] | \u2265\ud835\udc58\ud835\udf07 3/4\u221a\ud835\udc5a] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc58 2 ) . Recalling that\ud835\udf07\u2264polylog\ud835\udc5band\ud835\udc5a\u2264\ud835\udc5bpolylog\ud835\udc5b, it follows with high probability in\ud835\udc5athat |\ud835\udc39\u2212E[\ud835\udc39]", "source": "loadbalancing.pdf"}, {"text": "by\ud835\udc5a independent random variables, we have by McDiarmid\u2019s inequality (Theorem B.1) that Pr[|\ud835\udc39\u2212E[\ud835\udc39] | \u2265\ud835\udc58\ud835\udf07 3/4\u221a\ud835\udc5a] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc58 2 ) . Recalling that\ud835\udf07\u2264polylog\ud835\udc5band\ud835\udc5a\u2264\ud835\udc5bpolylog\ud835\udc5b, it follows with high probability in\ud835\udc5athat |\ud835\udc39\u2212E[\ud835\udc39] | \u2264 \u02dc\ud835\udc42( \u221a\ud835\udc5b). This implies that \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc34\ud835\udc56 \u2264\ud835\udc5b\ud835\udc52 \u2212\u03a9(\ud835\udf07 1/4 ) + \u02dc\ud835\udc42( \u221a\ud835\udc5b) \u2264\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) + \u02dc\ud835\udc42( \u221a\ud835\udc5b). Again using the fact that\ud835\udf07\u2264log 8 \ud835\udc5a, this is at most\ud835\udc5b/\ud835\udf07 \ud835\udf14(1) .\u25a1 Next we prove a basic lemma relating the error\ud835\udf00\ud835\udc61 to the number of balls thrown in the spreading stage of round\ud835\udc61. Lemma 4.4.For \ud835\udc61\u2208 { 0, . . . , \ud835\udc47}, if \ud835\udf00\ud835\udc61 =\ud835\udc42(\ud835\udc5b) , then the number of balls that get thrown in the spreading stage of round\ud835\udc61is\ud835\udc5a \ud835\udc61 \u00b1\ud835\udc42(\ud835\udc5b). 10 Proof. This holds", "source": "loadbalancing.pdf"}, {"text": "0, . . . , \ud835\udc47}, if \ud835\udf00\ud835\udc61 =\ud835\udc42(\ud835\udc5b) , then the number of balls that get thrown in the spreading stage of round\ud835\udc61is\ud835\udc5a \ud835\udc61 \u00b1\ud835\udc42(\ud835\udc5b). 10 Proof. This holds trivially for\ud835\udc61= 0, since the number of balls thrown in the spreading stage of round0is exactly \ud835\udc5a=\ud835\udc5a 0. Fix a \ud835\udc61\u2208 { 1, . . . , \ud835\udc47}, and let\ud835\udc64= \u00cd \ud835\udc56\u2208 [\ud835\udc5b] \u02dc\u2113 (\ud835\udc61) \ud835\udc56 be the number of balls in bins immediately prior to the spreading stage of round\ud835\udc61. We have \ud835\udc64= \u2211\ufe01 \ud835\udc56\u2208 [\ud835\udc5b] \u02dc\u2113 (\ud835\udc61) \ud835\udc56 = \u2211\ufe01 \ud835\udc56\u2208 [\ud835\udc5b] \u02dc\u2113 (\ud835\udc61) \ud835\udc56 \u2212\u2113 (\ud835\udc61) +\u2113 (\ud835\udc61) =\ud835\udc5b\u2113 (\ud835\udc61) + \u2211\ufe01 \ud835\udc56| \u02dc\u2113 (\ud835\udc61) \ud835\udc56 >\u2113 (\ud835\udc61) \u02dc\u2113 (\ud835\udc61) \ud835\udc56 \u2212\u2113 (\ud835\udc61) + \u2211\ufe01 \ud835\udc56| \u02dc\u2113", "source": "loadbalancing.pdf"}, {"text": "= \u2211\ufe01 \ud835\udc56\u2208 [\ud835\udc5b] \u02dc\u2113 (\ud835\udc61) \ud835\udc56 \u2212\u2113 (\ud835\udc61) +\u2113 (\ud835\udc61) =\ud835\udc5b\u2113 (\ud835\udc61) + \u2211\ufe01 \ud835\udc56| \u02dc\u2113 (\ud835\udc61) \ud835\udc56 >\u2113 (\ud835\udc61) \u02dc\u2113 (\ud835\udc61) \ud835\udc56 \u2212\u2113 (\ud835\udc61) + \u2211\ufe01 \ud835\udc56| \u02dc\u2113 (\ud835\udc61) \ud835\udc56 <\u2113 (\ud835\udc61) \u2212(\u2113 (\ud835\udc61) \u2212 \u02dc\u2113 (\ud835\udc61) \ud835\udc56 ) =\ud835\udc5b\u2113 (\ud835\udc61) +\ud835\udf00 + \ud835\udc61 \u2212\ud835\udf00 \u2212 \ud835\udc61 . The number of balls thrown in the spreading stage of round\ud835\udc61is therefore \ud835\udc5a\u2212\ud835\udc64=\ud835\udc5a\u2212\ud835\udc5b\u2113 (\ud835\udc61) \u2212\ud835\udf00 + \ud835\udc61 +\ud835\udf00 \u2212 \ud835\udc61 =\ud835\udc5a \ud835\udc61 \u2212\ud835\udf00 + \ud835\udc61 +\ud835\udf00 \u2212 \ud835\udc61 =\ud835\udc5a \ud835\udc61 \u00b1\ud835\udc42(\ud835\udc5b). \u25a1 Bounding the underfilled badness.The next lemma allows us to bound how much \ud835\udf00 \u2212 \ud835\udc61 grows when we increment\ud835\udc61. Lemma 4.5.Let \ud835\udc61\u2208 { 0, . . . , \ud835\udc47\u2212 1}. With high probability in \ud835\udc5a\ud835\udc61 , if the", "source": "loadbalancing.pdf"}, {"text": "to bound how much \ud835\udf00 \u2212 \ud835\udc61 grows when we increment\ud835\udc61. Lemma 4.5.Let \ud835\udc61\u2208 { 0, . . . , \ud835\udc47\u2212 1}. With high probability in \ud835\udc5a\ud835\udc61 , if the number of balls thrown in the spreading stage of round\ud835\udc61is\ud835\udc5a \ud835\udc61 \u00b1\ud835\udc42(\ud835\udc5b), then\ud835\udf00 \u2212 \ud835\udc61+1 \u2264\ud835\udf00 \u2212 \ud835\udc61 +\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Proof. Focus on a fixed round\ud835\udc61\u2208 { 0, . . . , \ud835\udc47\u2212 1}. For each bin \ud835\udc56, define \ud835\udc5e\ud835\udc56 so that \u02dc\u2113 (\ud835\udc61) \ud835\udc56 =\u2113 (\ud835\udc61) \u2212\ud835\udc5e \ud835\udc56. (We can think of \ud835\udc5e\ud835\udc56 as simply being a signed version of the round-\ud835\udc61 error for bin\ud835\udc56.) Let \ud835\udc58\ud835\udc56 be the number of balls that the spreading stage in round\ud835\udc61places into bin\ud835\udc56. Then we can express\ud835\udf00 \u2212 \ud835\udc61+1 as \ud835\udf00", "source": "loadbalancing.pdf"}, {"text": "version of the round-\ud835\udc61 error for bin\ud835\udc56.) Let \ud835\udc58\ud835\udc56 be the number of balls that the spreading stage in round\ud835\udc61places into bin\ud835\udc56. Then we can express\ud835\udf00 \u2212 \ud835\udc61+1 as \ud835\udf00 \u2212 \ud835\udc61+1 = \u2211\ufe01 \ud835\udc56 max(\u2113 (\ud835\udc61+1) \u2212 (\u2113 (\ud835\udc61) \u2212\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56),0) = \u2211\ufe01 \ud835\udc56 max(\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 +\ud835\udc5e \ud835\udc56 \u2212\ud835\udc58 \ud835\udc56,0)(since\u2113 (\ud835\udc61+1) \u2212\u2113 (\ud835\udc61) =\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 ) \u2264 \u2211\ufe01 \ud835\udc56 max(\ud835\udc5e\ud835\udc56,0) + \u2211\ufe01 \ud835\udc56 max(\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc58 \ud835\udc56,0) =\ud835\udf00 \u2212 \ud835\udc61 + \u2211\ufe01 \ud835\udc56 max(\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc58 \ud835\udc56,0). Finally, by Lemma 4.3, we have with high probability in\ud835\udc5a\ud835\udc61 that \u2211\ufe01 \ud835\udc56 max(\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc58 \ud835\udc56,0) \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ), which completes the proof.\u25a1 Bounding", "source": "loadbalancing.pdf"}, {"text": "\u2212\ud835\udc58 \ud835\udc56,0). Finally, by Lemma 4.3, we have with high probability in\ud835\udc5a\ud835\udc61 that \u2211\ufe01 \ud835\udc56 max(\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc58 \ud835\udc56,0) \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ), which completes the proof.\u25a1 Bounding the overfilled badness.Next we bound the growth of\ud835\udf00 + \ud835\udc61 as\ud835\udc61increases. Lemma 4.6.Let \ud835\udc61\u2208 { 0, . . . , \ud835\udc47\u2212 1}. With high probability in \ud835\udc5a\ud835\udc61 , if the number of balls thrown in the spreading stage of round\ud835\udc61is\ud835\udc5a \ud835\udc61 \u00b1\ud835\udc42(\ud835\udc5b), then\ud835\udf00 + \ud835\udc61+1 \u2264\ud835\udf00 + \ud835\udc61 +\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). 11 To understand\ud835\udf00+ \ud835\udc61+1, it is helpful to consider the contribution of each bin\ud835\udc56 to \ud835\udf00+ \ud835\udc61 . Recall that bin\ud835\udc56 contributes to \ud835\udf00+ \ud835\udc61 if it is overfilled bad in round\ud835\udc61. In order for bin\ud835\udc56 to be overfilled", "source": "loadbalancing.pdf"}, {"text": "the contribution of each bin\ud835\udc56 to \ud835\udf00+ \ud835\udc61 . Recall that bin\ud835\udc56 contributes to \ud835\udf00+ \ud835\udc61 if it is overfilled bad in round\ud835\udc61. In order for bin\ud835\udc56 to be overfilled bad, it must have come across at least one of two failure modes: (1) The total number of round-\ud835\udc61 balls in the bin is too small or (2) the total number of balls in the bin is too large. Before proving Lemma 4.6, we show that the accumulation of these failure events across all bins is small with high probability. As we will see, we can prove this for both failure events via straightforward applications of Lemma 4.3. We begin with the first failure event: Claim 4.7.Let \ud835\udc50\ud835\udc56 :=|C (\ud835\udc61+1)", "source": "loadbalancing.pdf"}, {"text": "As we will see, we can prove this for both failure events via straightforward applications of Lemma 4.3. We begin with the first failure event: Claim 4.7.Let \ud835\udc50\ud835\udc56 :=|C (\ud835\udc61+1) \ud835\udc56 | be the number of balls in bin \ud835\udc56 with round assignment \ud835\udc61+ 1. Let \ud835\udc4f\ud835\udc56 := max(2\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc50 \ud835\udc56,0). \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4f\ud835\udc56 \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 )(4) with high probability in\ud835\udc5a \ud835\udc61 . Proof. Note that (4) is trivial for \ud835\udf07\ud835\udc61 =\ud835\udc42( 1), so we will assume without loss of generality that\ud835\udf07\ud835\udc61 is at least a large positive constant. To establish(4), let us first consider thetotal number \ud835\udc50 (\ud835\udc61+1) of balls with round assignment\ud835\udc61+ 1in the system. Since each ball has a \u0398(\ud835\udc5a\ud835\udc61 /\ud835\udc5a) probability of having round", "source": "loadbalancing.pdf"}, {"text": "constant. To establish(4), let us first consider thetotal number \ud835\udc50 (\ud835\udc61+1) of balls with round assignment\ud835\udc61+ 1in the system. Since each ball has a \u0398(\ud835\udc5a\ud835\udc61 /\ud835\udc5a) probability of having round assignment\ud835\udc61+ 1, the expected value E[\ud835\udc50 (\ud835\udc61+1) ]is\u0398(\ud835\udc5a \ud835\udc61 ). By a Chernoff bound, it follows with high probability in\ud835\udc5a\ud835\udc61 that\ud835\udc50 (\ud835\udc61+1) =\u0398(\ud835\udc5a \ud835\udc61 ). Conditioning on \ud835\udc50 (\ud835\udc61+1) =\u0398(\ud835\udc5a \ud835\udc61 ), and applying Lemma 4.3, we have with high probability in\ud835\udc50 (\ud835\udc61+1) (and thus with high probability in\ud835\udc5a\ud835\udc61) that \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 max(\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b\u2212\ud835\udc50 \ud835\udc56 \u2212 (\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b)3/4,0) \u2264\ud835\udc5b/poly(\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b) \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Since\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b=\u0398(\ud835\udf07 \ud835\udc61 ), and since\ud835\udf07 \ud835\udc61 is at least a sufficiently large positive constant, we have that \ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b\u2212\ud835\udc50 \ud835\udc56 \u2212", "source": "loadbalancing.pdf"}, {"text": "\u2264\ud835\udc5b/poly(\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b) \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Since\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b=\u0398(\ud835\udf07 \ud835\udc61 ), and since\ud835\udf07 \ud835\udc61 is at least a sufficiently large positive constant, we have that \ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b\u2212\ud835\udc50 \ud835\udc56 \u2212 (\ud835\udc50 (\ud835\udc61+1) /\ud835\udc5b)3/4 \u22652\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc50 \ud835\udc56. It follows that \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4f\ud835\udc56 = \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 max(2\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc50 \ud835\udc56,0) \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ), and therefore that (4) holds.\u25a1 The second failure event follows by a straightforward application of Lemma 4.3: Claim 4.8.Suppose that the total number of balls thrown in round \ud835\udc61 is \ud835\udc5a\ud835\udc61 \u00b1\ud835\udc42(\ud835\udc5b) . Let \ud835\udc58\ud835\udc56 be the number of balls that the spreading stage in round \ud835\udc61 places into bin \ud835\udc56, and let \ud835\udc4e\ud835\udc56 =max(\ud835\udc58 \ud835\udc56 \u2212 (\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 ), 0). Then,", "source": "loadbalancing.pdf"}, {"text": "be the number of balls that the spreading stage in round \ud835\udc61 places into bin \ud835\udc56, and let \ud835\udc4e\ud835\udc56 =max(\ud835\udc58 \ud835\udc56 \u2212 (\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 ), 0). Then, with high probability in\ud835\udc5a \ud835\udc61 , \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4e\ud835\udc56 \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Proof.By Lemma 4.3, \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4e\ud835\udc56 = \u2211\ufe01 \ud835\udc56=1 max(\ud835\udc58\ud835\udc56 \u2212 (\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 ),0) \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 )(5) with high probability in\ud835\udc5a\ud835\udc61.\u25a1 We can now prove Lemma 4.6. 12 Proof of Lemma 4.6. Focus on a bin\ud835\udc56 in round \ud835\udc61, and define\ud835\udc5e\ud835\udc56 so that \u02dc\u2113 (\ud835\udc61) \ud835\udc56 =\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56. Let \ud835\udc58\ud835\udc56 be the number of balls that the spreading stage in round\ud835\udc61 places into the bin\ud835\udc56. After the spreading stage in round\ud835\udc61, bin \ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc56 =\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56. Let \ud835\udc58\ud835\udc56 be the number of balls that the spreading stage in round\ud835\udc61 places into the bin\ud835\udc56. After the spreading stage in round\ud835\udc61, bin \ud835\udc56 has \u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56 balls. During the slice step in round\ud835\udc61+ 1, the bin would like to evictmax(\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56 \u2212\u2113 (\ud835\udc61+1) , 0) balls with round assignment\ud835\udc61+ 1. So, if the bin has\ud835\udc50\ud835\udc56 :=|C (\ud835\udc61+1) \ud835\udc56 | balls with round assignment\ud835\udc61+ 1, its overfill error in round\ud835\udc61+1(i.e., its contribution to\ud835\udf00 + \ud835\udc61+1) will be max(max(\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56 \u2212\u2113 (\ud835\udc61+1) ,0) \u2212\ud835\udc50 \ud835\udc56,0)=max(\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56 \u2212\u2113 (\ud835\udc61+1) \u2212\ud835\udc50 \ud835\udc56,0). Since\u2113 (\ud835\udc61+1) \u2212\u2113 (\ud835\udc61) =\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61", "source": "loadbalancing.pdf"}, {"text": "be max(max(\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56 \u2212\u2113 (\ud835\udc61+1) ,0) \u2212\ud835\udc50 \ud835\udc56,0)=max(\u2113 (\ud835\udc61) +\ud835\udc5e \ud835\udc56 +\ud835\udc58 \ud835\udc56 \u2212\u2113 (\ud835\udc61+1) \u2212\ud835\udc50 \ud835\udc56,0). Since\u2113 (\ud835\udc61+1) \u2212\u2113 (\ud835\udc61) =\ud835\udf07 \ud835\udc61 \u2212\ud835\udf07 3/4 \ud835\udc61 , our bound on the bin\u2019s overfill error is at most max(\ud835\udc5e\ud835\udc56,0) +max(\ud835\udc58 \ud835\udc56 \u2212\ud835\udc50 \ud835\udc56 \u2212\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 ,0). Note that if \ud835\udc58\ud835\udc56 \u2264\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 and \ud835\udc50\ud835\udc56 \u2265 2\ud835\udf073/4 \ud835\udc61 , then max(\ud835\udc58\ud835\udc56 \u2212\ud835\udc50 \ud835\udc56 \u2212\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 , 0)= 0. Thus we will bound max(\ud835\udc58\ud835\udc56 \u2212\ud835\udc50 \ud835\udc56 \u2212\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 , 0) by bounding the amount by which\ud835\udc58\ud835\udc56 exceeds \ud835\udf07\ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 or by which\ud835\udc50\ud835\udc56 goes below 2\ud835\udf073/4 \ud835\udc61 . Specifically, if we define\ud835\udc4e\ud835\udc56 :=max(\ud835\udc58 \ud835\udc56 \u2212 (\ud835\udf07", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc61 , 0) by bounding the amount by which\ud835\udc58\ud835\udc56 exceeds \ud835\udf07\ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 or by which\ud835\udc50\ud835\udc56 goes below 2\ud835\udf073/4 \ud835\udc61 . Specifically, if we define\ud835\udc4e\ud835\udc56 :=max(\ud835\udc58 \ud835\udc56 \u2212 (\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 ),0)and\ud835\udc4f \ud835\udc56 :=max(2\ud835\udf07 3/4 \ud835\udc61 \u2212\ud835\udc50 \ud835\udc56,0), then we have max(\ud835\udc5e\ud835\udc56,0) +max(\ud835\udc58 \ud835\udc56 \u2212\ud835\udc50 \ud835\udc56 \u2212\ud835\udf07 \ud835\udc61 +\ud835\udf07 3/4 \ud835\udc61 ,0) \u2264max(\ud835\udc5e \ud835\udc56,0) +\ud835\udc4e \ud835\udc56 +\ud835\udc4f \ud835\udc56 . This bounds each bin\ud835\udc56\u2019s contribution to\ud835\udf00+ \ud835\udc61+1 bymax(\ud835\udc5e \ud835\udc56,0) +\ud835\udc4e \ud835\udc56 +\ud835\udc4f \ud835\udc56. Summing over the bins gives \ud835\udf00+ \ud835\udc61+1 \u2264 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 (max(\ud835\udc5e \ud835\udc56,0) +\ud835\udc4e \ud835\udc56 +\ud835\udc4f \ud835\udc56)=\ud835\udf00 + \ud835\udc61 + \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4e\ud835\udc56 + \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4f\ud835\udc56 . By Claim 4.8 and Claim 4.7, each of\u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc4e\ud835\udc56 and \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc4f\ud835\udc56 are of the form\ud835\udc5b/poly(\ud835\udf07", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc56)=\ud835\udf00 + \ud835\udc61 + \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4e\ud835\udc56 + \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4f\ud835\udc56 . By Claim 4.8 and Claim 4.7, each of\u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc4e\ud835\udc56 and \u00cd\ud835\udc5b \ud835\udc56=1 \ud835\udc4f\ud835\udc56 are of the form\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ) with high probability in \ud835\udc5a\ud835\udc61, completing the proof. \u25a1 Putting the previous lemmas together, we can now obtain a good overall bound on\ud835\udf00\ud835\udc61. Lemma 4.9.With high probability in\ud835\udc5a \ud835\udc61 , we have \ud835\udf00\ud835\udc61 \u2264\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Proof. By Lemmas 4.4, 4.5, and 4.6 for each\ud835\udc61\u2208 { 0, . . . , \ud835\udc47\u2212 1}, we have that, with high probability in\ud835\udc5a\ud835\udc61, if \ud835\udf00\ud835\udc61 =\ud835\udc42(\ud835\udc5b), then \ud835\udf00\ud835\udc61+1 \u2264\ud835\udf00 \ud835\udc61 +\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Since\ud835\udf00 0 =0=\ud835\udc42(\ud835\udc5b), it follows by induction on\ud835\udc61for every\ud835\udc61\u2208 {0, . . . , \ud835\udc47\u22121}that, with probability at", "source": "loadbalancing.pdf"}, {"text": "probability in\ud835\udc5a\ud835\udc61, if \ud835\udf00\ud835\udc61 =\ud835\udc42(\ud835\udc5b), then \ud835\udf00\ud835\udc61+1 \u2264\ud835\udf00 \ud835\udc61 +\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ). Since\ud835\udf00 0 =0=\ud835\udc42(\ud835\udc5b), it follows by induction on\ud835\udc61for every\ud835\udc61\u2208 {0, . . . , \ud835\udc47\u22121}that, with probability at least 1\u2212 \ud835\udc61\u2211\ufe01 \ud835\udc5e=0 1/poly(\ud835\udf07 \ud835\udc5e), we have \ud835\udf00\ud835\udc61+1 \u2264 \ud835\udc61\u2211\ufe01 \ud835\udc5e=0 \ud835\udc5b/poly(\ud835\udf07 \ud835\udc5e) \u2264\ud835\udc42(\ud835\udc5b). Since \u00cd\ud835\udc61 \ud835\udc5e=0 1/poly(\ud835\udc5a\ud835\udc5e)=1/poly(\ud835\udc5a \ud835\udc61 )and \u00cd\ud835\udc61 \ud835\udc5e=0 \ud835\udc5b/poly(\ud835\udf07 \ud835\udc5e)=\ud835\udc5b/poly(\ud835\udf07 \ud835\udc61 ), the lemma follows.\u25a1 Finally, we can establish a bound on the total number of balls above height\ud835\udf07=\ud835\udc5a/\ud835\udc5b at the end of the construction: 13 Proposition 4.2.For each S \u2286 U , if |S| \u2264\ud835\udc5a , then after applying Slice and Spread to S, the cumulative overload is\ud835\udc42(\ud835\udc5b)with high probability in\ud835\udc5b. Proof. Applying Lemma 4.9 to\ud835\udc61=\ud835\udc47 , we have with high probability in\ud835\udc5b that", "source": "loadbalancing.pdf"}, {"text": "\u2264\ud835\udc5a , then after applying Slice and Spread to S, the cumulative overload is\ud835\udc42(\ud835\udc5b)with high probability in\ud835\udc5b. Proof. Applying Lemma 4.9 to\ud835\udc61=\ud835\udc47 , we have with high probability in\ud835\udc5b that \ud835\udf00\ud835\udc47 \u2264\ud835\udc42(\ud835\udc5b) . Since \ud835\udf00\ud835\udc47 \u2264\ud835\udc42(\ud835\udc5b) , we know from Lemma 4.4 that the number\ud835\udc3e of balls thrown in the spreading stage of round\ud835\udc47 is at most \ud835\udc5a\ud835\udc47 =\ud835\udc42(\ud835\udc5b) . The total number of balls above height\u2113 (\ud835\udc47) \u2264\ud835\udc5a/\ud835\udc5b at the end of the algorithm is therefore at most\ud835\udf00 \ud835\udc47+1 +\ud835\udc3e\u2264\ud835\udc42(\ud835\udc5b). \u25a1 4.4 The Recourse of Slice and Spread In this section, we bound the expected recourse of Slice and Spread. To do so, we introduce Proposition 4.10 which bounds the recourse for each(S,S \u2032, \u210e1, \u210e2, \ud835\udc45): Proposition 4.10.Fix", "source": "loadbalancing.pdf"}, {"text": "this section, we bound the expected recourse of Slice and Spread. To do so, we introduce Proposition 4.10 which bounds the recourse for each(S,S \u2032, \u210e1, \u210e2, \ud835\udc45): Proposition 4.10.Fix any neighboring datasets S and S\u2032, and \u210e1, \u210e2, and \ud835\udc45. The recourse of (S,S \u2032, \u210e1, \u210e2, \ud835\udc45) incurred by Slice and Spread is\ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)). To prove Proposition 4.10, we will make use of some additional notation and definitions. As a convention, let|S \u2032|>|S| . Let theextra ball \ud835\udc65 \u2217 be the ball that appears inS\u2032 but not S. Letworld 0be the scenario that considers S, and letworld1be the scenario that considers S\u2032. In general, to distinguish between the two worlds, we will add the notation of(\ud835\udc64) to the", "source": "loadbalancing.pdf"}, {"text": "Letworld 0be the scenario that considers S, and letworld1be the scenario that considers S\u2032. In general, to distinguish between the two worlds, we will add the notation of(\ud835\udc64) to the end of the variable, where\ud835\udc64 is the variable for world. For example, \u2113 (\ud835\udc61) \ud835\udc56 (\ud835\udc64) , C (\ud835\udc61) \ud835\udc56 (\ud835\udc64) , and P (\ud835\udc61) \ud835\udc56 (\ud835\udc64) . Let \ud835\udc56\u2217 :=\u210e 1(\ud835\udc65 \u2217) be the first-choice bin of the extra ball. If\ud835\udc65 \u2217 has a round assignment, then let\ud835\udc61\u2217 :=\ud835\udc50(\ud835\udc65 \u2217)be the round assignment. Otherwise, let\ud835\udc61 \u2217 =0. Much of the analysis will be spent discussing the differences between the states of the data structure between world0and world1. Let \ud835\udc51 (\ud835\udc61) \ud835\udc56 :=|\u2113 (\ud835\udc61) \ud835\udc56 (0) \u2212\u2113 (\ud835\udc61) \ud835\udc56 (1)|", "source": "loadbalancing.pdf"}, {"text": "the analysis will be spent discussing the differences between the states of the data structure between world0and world1. Let \ud835\udc51 (\ud835\udc61) \ud835\udc56 :=|\u2113 (\ud835\udc61) \ud835\udc56 (0) \u2212\u2113 (\ud835\udc61) \ud835\udc56 (1)| be thediscrepancy of bin \ud835\udc56. Let \ud835\udc51 (\ud835\udc61) := \u00cd \ud835\udc56 \ud835\udc51 (\ud835\udc61) \ud835\udc56 be thetotal discrepancyacross the two worlds in round\ud835\udc61. Finally, call a ball\ud835\udc65 ared ballif \ud835\udc65 was evicted from a bin in some world\ud835\udc64 but not simultaneously in \u00af\ud835\udc64 . Let \ud835\udc38 (\ud835\udc61) \ud835\udc56 be the number of red balls evicted from bin \ud835\udc56 during round \ud835\udc61 in world0and world1. That is, \ud835\udc38 (\ud835\udc61) \ud835\udc56 =|P (\ud835\udc61) \ud835\udc56 (0)\u0394P (\ud835\udc61) \ud835\udc56 (1)|. With these conventions in mind, we can now begin the analysis. We start with a", "source": "loadbalancing.pdf"}, {"text": "in world0and world1. That is, \ud835\udc38 (\ud835\udc61) \ud835\udc56 =|P (\ud835\udc61) \ud835\udc56 (0)\u0394P (\ud835\udc61) \ud835\udc56 (1)|. With these conventions in mind, we can now begin the analysis. We start with a few simple lemmas that will help us reason about the recourse. The first lemma says that the number of balls\ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) that get assigned to different bins in worlds0and1is at most\ud835\udc38 (\ud835\udc61) \ud835\udc56 . Lemma 4.11.Let \ud835\udf190 and \ud835\udf191 represent the final allocations of the balls to bins in world0and world1, respectively. For every(\ud835\udc61, \ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b], \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) \u2264\ud835\udc38 (\ud835\udc61) \ud835\udc56 .(6) Proof. For a given ball\ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0), and a given world\ud835\udc64\u2208 {", "source": "loadbalancing.pdf"}, {"text": "\u00d7 [\ud835\udc5b], \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) \u2264\ud835\udc38 (\ud835\udc61) \ud835\udc56 .(6) Proof. For a given ball\ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0), and a given world\ud835\udc64\u2208 { 0, 1}, the final bin\ud835\udf19\ud835\udc64 (\ud835\udc65) that ball \ud835\udc65 gets assigned to in world\ud835\udc64is ( \u210e1(\ud835\udc65)if\ud835\udc65\u2209P (\ud835\udc61) \ud835\udc56 (\ud835\udc64) \u210e2(\ud835\udc65)if\ud835\udc65\u2208 P (\ud835\udc61) \ud835\udc56 (\ud835\udc64) . It follows that I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) =I \u0010 \ud835\udc65\u2208 P (\ud835\udc61) \ud835\udc56 (0)\u25b3P (\ud835\udc61) \ud835\udc56 (1) \u0011 , which implies that \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) = \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) I \u0010 \ud835\udc65\u2208 P (\ud835\udc61) \ud835\udc56 (0)\u25b3P (\ud835\udc61) \ud835\udc56 (1) \u0011 \u2264 |P (\ud835\udc61) \ud835\udc56 (0)\u25b3P (\ud835\udc61) \ud835\udc56 (1)|=\ud835\udc38 (\ud835\udc61) \ud835\udc56 .(7) We note that the", "source": "loadbalancing.pdf"}, {"text": "C (\ud835\udc61) \ud835\udc56 (0) I \u0010 \ud835\udc65\u2208 P (\ud835\udc61) \ud835\udc56 (0)\u25b3P (\ud835\udc61) \ud835\udc56 (1) \u0011 \u2264 |P (\ud835\udc61) \ud835\udc56 (0)\u25b3P (\ud835\udc61) \ud835\udc56 (1)|=\ud835\udc38 (\ud835\udc61) \ud835\udc56 .(7) We note that the inequality in(7) is due to the fact that the summation is only over balls shared in common between world0and world1(which does not include\ud835\udc65 \u2217), but\ud835\udc38 (\ud835\udc61) \ud835\udc56 may contain\ud835\udc65 \u2217.\u25a1 14 This next lemma says that, if we want to bound\ud835\udc38(\ud835\udc61) \ud835\udc56 , it suffices to bound\ud835\udc51(\ud835\udc61) \ud835\udc56 . Lemma 4.12.Let(\ud835\udc61, \ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b]such that(\ud835\udc61, \ud835\udc56)\u2260(\ud835\udc61 \u2217, \ud835\udc56\u2217). Then, \ud835\udc38 (\ud835\udc61) \ud835\udc56 \u2264\ud835\udc51 (\ud835\udc61) \ud835\udc56 . This lemma makes use of the following simple claim: Claim 4.13.Let(\ud835\udc61, \ud835\udc56)\u2260(\ud835\udc61 \u2217, \ud835\udc56\u2217), and let\ud835\udc64\u2208 {0,1}be the world\ud835\udc64for which\u2113 (\ud835\udc61) \ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "Then, \ud835\udc38 (\ud835\udc61) \ud835\udc56 \u2264\ud835\udc51 (\ud835\udc61) \ud835\udc56 . This lemma makes use of the following simple claim: Claim 4.13.Let(\ud835\udc61, \ud835\udc56)\u2260(\ud835\udc61 \u2217, \ud835\udc56\u2217), and let\ud835\udc64\u2208 {0,1}be the world\ud835\udc64for which\u2113 (\ud835\udc61) \ud835\udc56 (\ud835\udc64)=\u2113 (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) +\ud835\udc51 (\ud835\udc61) \ud835\udc56 . Then P (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) \u2286 P (\ud835\udc61) \ud835\udc56 (\ud835\udc64).(8) Proof. Since (\ud835\udc61, \ud835\udc56)\u2260(\ud835\udc61 \u2217, \ud835\udc56\u2217), the sets C (\ud835\udc61) \ud835\udc56 (0) and C (\ud835\udc61) \ud835\udc56 (1) are equal and furthermore have the same ordering as determined by the canonical ordering. SinceP (\ud835\udc61) \ud835\udc56 (\ud835\udc64) and P (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) are prefixes of the same ordered set, and since\u2113 (\ud835\udc61) \ud835\udc56 (\ud835\udc64) \u2265\u2113 (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64), it follows thatP (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) \u2286 P (\ud835\udc61) \ud835\udc56 (\ud835\udc64).\u25a1", "source": "loadbalancing.pdf"}, {"text": "\u00af\ud835\udc64) are prefixes of the same ordered set, and since\u2113 (\ud835\udc61) \ud835\udc56 (\ud835\udc64) \u2265\u2113 (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64), it follows thatP (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) \u2286 P (\ud835\udc61) \ud835\udc56 (\ud835\udc64).\u25a1 Proof of Lemma 4.12.By Claim 4.13, \ud835\udc38 (\ud835\udc61) \ud835\udc56 =||P (\ud835\udc61) \ud835\udc56 (0)| \u2212 |P (\ud835\udc61) \ud835\udc56 (1)||. Recall that, in each world \ud835\udc64\u2208 { 0, 1}, the number |P (\ud835\udc61) \ud835\udc56 (\ud835\udc64)| of balls that we evict from bin\ud835\udc56 in round \ud835\udc61 is min(|C (\ud835\udc61) \ud835\udc56 (\ud835\udc64)|, \u2113 (\ud835\udc61) \ud835\udc56 (\ud835\udc64) \u2212\u2113 (\ud835\udc61) ). It follows that ||P (\ud835\udc61) \ud835\udc56 (\ud835\udc64)| \u2212 |P (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64)||=|min(|C (\ud835\udc61) \ud835\udc56 (0)|, \u2113 (\ud835\udc61) \ud835\udc56 (0) \u2212\u2113 (\ud835\udc61) ) \u2212min(|C (\ud835\udc61) \ud835\udc56 (1)|, \u2113 (\ud835\udc61) \ud835\udc56 (1) \u2212\u2113 (\ud835\udc61) )|. Since", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc56 (\ud835\udc64)| \u2212 |P (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64)||=|min(|C (\ud835\udc61) \ud835\udc56 (0)|, \u2113 (\ud835\udc61) \ud835\udc56 (0) \u2212\u2113 (\ud835\udc61) ) \u2212min(|C (\ud835\udc61) \ud835\udc56 (1)|, \u2113 (\ud835\udc61) \ud835\udc56 (1) \u2212\u2113 (\ud835\udc61) )|. Since (\ud835\udc61, \ud835\udc56)\u2260(\ud835\udc61 \u2217, \ud835\udc56\u2217), we have C (\ud835\udc61) \ud835\udc56 (0)=C (\ud835\udc61) \ud835\udc56 (1). Using the identity |min(\ud835\udc4e, \ud835\udc4f) \u2212min(\ud835\udc4e, \ud835\udc50)| \u2264 |\ud835\udc4f\u2212\ud835\udc50| , we can conclude that |min(|C (\ud835\udc61) \ud835\udc56 (0)|, \u2113 (\ud835\udc61) \ud835\udc56 (0) \u2212\u2113 (\ud835\udc61) ) \u2212min(|C (\ud835\udc61) \ud835\udc56 (1)|, \u2113 (\ud835\udc61) \ud835\udc56 (1) \u2212\u2113 (\ud835\udc61) )| \u2264 |\u2113 (\ud835\udc61) \ud835\udc56 (0) \u2212\u2113 (\ud835\udc61) \ud835\udc56 (1)|=\ud835\udc51 (\ud835\udc61) \ud835\udc56 . \u25a1 We now get to the meat of the section, where, in the next two lemmas, we show how to bound the discrepancy\ud835\udc51 (\ud835\udc61) at the beginning of each", "source": "loadbalancing.pdf"}, {"text": ". \u25a1 We now get to the meat of the section, where, in the next two lemmas, we show how to bound the discrepancy\ud835\udc51 (\ud835\udc61) at the beginning of each round\ud835\udc61. Lemma 4.14.For all\ud835\udc61\u2260\ud835\udc61 \u2217,\ud835\udc51 (\ud835\udc61+1) \u2264\ud835\udc51 (\ud835\udc61) . Proof. We can decompose any round\ud835\udc61 into the slicing stage and the spreading stage. Recall that\ud835\udc38 (\ud835\udc61) \ud835\udc56 is the total number of balls that bin\ud835\udc56 evicts in round\ud835\udc61 in one world but not the other. Let\ud835\udc45 (\ud835\udc61) \ud835\udc56 := \u00cd \ud835\udc56\u2208 [\ud835\udc5b] |{\ud835\udc65\u2208\ud835\udc38 (\ud835\udc61) \ud835\udc56 | \u210e2(\ud835\udc65)=\ud835\udc56}| be the number of red balls that bin\ud835\udc56 receives in round\ud835\udc61 (in either world) during the spreading stage. We begin by showing the following inequality: \ud835\udc51 (\ud835\udc61+1) \ud835\udc56 \u2264\ud835\udc51 (\ud835\udc61) \ud835\udc56 \u2212\ud835\udc38 (\ud835\udc61)", "source": "loadbalancing.pdf"}, {"text": "of red balls that bin\ud835\udc56 receives in round\ud835\udc61 (in either world) during the spreading stage. We begin by showing the following inequality: \ud835\udc51 (\ud835\udc61+1) \ud835\udc56 \u2264\ud835\udc51 (\ud835\udc61) \ud835\udc56 \u2212\ud835\udc38 (\ud835\udc61) \ud835\udc56 +\ud835\udc45 (\ud835\udc61) \ud835\udc56 .(9) We first consider the effect of the slicing stage in round\ud835\udc61 on the discrepancy of bin\ud835\udc56. Since \ud835\udc61\u2260\ud835\udc61 \u2217, we know by Lemma 4.12 and Claim 4.13 that there is some\ud835\udc64\u2208 { 0, 1} such that\u2113 (\ud835\udc61) \ud835\udc56 (\ud835\udc64)=\u2113 (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) +\ud835\udc51 (\ud835\udc61) \ud835\udc56 \u2265\u2113 (\ud835\udc61) \ud835\udc56 ( \u00af\ud835\udc64) +\ud835\udc38 (\ud835\udc61) \ud835\udc56 and such that, during the slicing stage of round\ud835\udc61, \ud835\udc38 (\ud835\udc61) \ud835\udc56 more balls are sliced from bin\ud835\udc56 in world \ud835\udc64 than in world \u00af\ud835\udc64. Thus, the slicing stage in round\ud835\udc61decreases", "source": "loadbalancing.pdf"}, {"text": "such that, during the slicing stage of round\ud835\udc61, \ud835\udc38 (\ud835\udc61) \ud835\udc56 more balls are sliced from bin\ud835\udc56 in world \ud835\udc64 than in world \u00af\ud835\udc64. Thus, the slicing stage in round\ud835\udc61decreases the the discrepancy of bin\ud835\udc56by\ud835\udc38 (\ud835\udc61) \ud835\udc56 . We now show that the spreading stage increases the discrepancy of bin\ud835\udc56 by at most\ud835\udc45 (\ud835\udc61) \ud835\udc56 . This is because, during the spreading stage, the only balls that can be placed into bin\ud835\udc56 in one world but not the other are red 15 balls. Since the number of red balls placed into bin\ud835\udc56 (in either world) during the spreading stage of round \ud835\udc61is\ud835\udc45 (\ud835\udc61) \ud835\udc56 , it follows that the discrepancy of bin\ud835\udc56increases by at most\ud835\udc45 (\ud835\udc61) \ud835\udc56 during the spreading", "source": "loadbalancing.pdf"}, {"text": "into bin\ud835\udc56 (in either world) during the spreading stage of round \ud835\udc61is\ud835\udc45 (\ud835\udc61) \ud835\udc56 , it follows that the discrepancy of bin\ud835\udc56increases by at most\ud835\udc45 (\ud835\udc61) \ud835\udc56 during the spreading stage. Having established (9), we can sum across the bins to get \ud835\udc51 (\ud835\udc61+1) = \u2211\ufe01 \ud835\udc56 \ud835\udc51 (\ud835\udc61+1) \ud835\udc56 \u2264 \u2211\ufe01 \ud835\udc56 \u0010 \ud835\udc51 (\ud835\udc61) \ud835\udc56 \u2212\ud835\udc38 (\ud835\udc61) \ud835\udc56 +\ud835\udc45 (\ud835\udc61) \ud835\udc56 \u0011 =\ud835\udc51 (\ud835\udc61) \u2212 \u2211\ufe01 \ud835\udc56 \ud835\udc38 (\ud835\udc61) \ud835\udc56 + \u2211\ufe01 \ud835\udc56 \ud835\udc45 (\ud835\udc61) \ud835\udc56 =\ud835\udc51 (\ud835\udc61) . \u25a1 Lemma 4.15.Suppose that \ud835\udc61 \u2217 \u2260 0. We have \ud835\udc38 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 + 2. Furthermore, if \ud835\udc61 \u2217 <\ud835\udc47 , then \ud835\udc51 (\ud835\udc61 \u2217+1) \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) + 2. Proof.", "source": "loadbalancing.pdf"}, {"text": "have \ud835\udc38 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 + 2. Furthermore, if \ud835\udc61 \u2217 <\ud835\udc47 , then \ud835\udc51 (\ud835\udc61 \u2217+1) \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) + 2. Proof. To prove this lemma, let us also introduce a hybrid world2, in which everything is the same as world 1except the extra ball is not assigned a round (i.e.\ud835\udc61\u2217 =0). At the beginning of round\ud835\udc61 \u2217, worlds 1 and 2 share the same exact state (but with a difference in round assignment for the extra ball). During the slicing step in round\ud835\udc61 \u2217, all of the bins except for bin\ud835\udc56\u2217 evict the same set of balls as each other in worlds 1 and 2. The only bin that behaves differently", "source": "loadbalancing.pdf"}, {"text": "in round\ud835\udc61 \u2217, all of the bins except for bin\ud835\udc56\u2217 evict the same set of balls as each other in worlds 1 and 2. The only bin that behaves differently in the two worlds is bin \ud835\udc56\u2217, which may evict up to one ball in each of worlds 1 and 2 that it doesn\u2019t evict in the other (call this the World-Difference Observation). We now argue that\ud835\udc38 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 + 2. Define \ud835\udc380,1, \ud835\udc381,2, \ud835\udc380,2 so that \ud835\udc38\ud835\udc64,\ud835\udc64 \u2032 is the number of balls\ud835\udc65 such that \u210e1(\ud835\udc65)=\ud835\udc56 \u2217, and such that\ud835\udc65 is evicted in round\ud835\udc61 \u2217 in one of worlds\ud835\udc64, \ud835\udc64 \u2032 but not in the other (i.e.,\ud835\udc65 is a round-\ud835\udc61 \u2217 red", "source": "loadbalancing.pdf"}, {"text": "balls\ud835\udc65 such that \u210e1(\ud835\udc65)=\ud835\udc56 \u2217, and such that\ud835\udc65 is evicted in round\ud835\udc61 \u2217 in one of worlds\ud835\udc64, \ud835\udc64 \u2032 but not in the other (i.e.,\ud835\udc65 is a round-\ud835\udc61 \u2217 red ball when comparing worlds\ud835\udc64and\ud835\udc64 \u2032). By the triangle inequality, we have that\ud835\udc380,1 \u2264\ud835\udc38 0,2 +\ud835\udc38 1,2. By Lemma 4.12, we know that \ud835\udc380,2 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 . Putting these together gives\ud835\udc380,1 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 +\ud835\udc38 1,2. Finally, we have by the World Difference Observation that, during round \ud835\udc61 \u2217, worlds1and2evict identical sets of balls except for up to one ball from bin \ud835\udc56\u2217 in each world. It follows that\ud835\udc38 1,2 \u22642, which implies that\ud835\udc38 0,1 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 +2. Next we argue that, if\ud835\udc61", "source": "loadbalancing.pdf"}, {"text": "up to one ball from bin \ud835\udc56\u2217 in each world. It follows that\ud835\udc38 1,2 \u22642, which implies that\ud835\udc38 0,1 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 +2. Next we argue that, if\ud835\udc61 \u2217 <\ud835\udc47 , then\ud835\udc51 (\ud835\udc61 \u2217+1) \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) + 2. Define \ud835\udc370,1 =\ud835\udc51 (\ud835\udc61 \u2217+1) to be the discrepancy between worlds 0 and 1 at the beginning of round\ud835\udc61 \u2217 + 1; define \ud835\udc371,2 to be the discrepancy between worlds 1 and 2 at the beginning of round\ud835\udc61 \u2217 + 1; and define\ud835\udc370,2 to be the discrepancy between worlds0and2at the beginning of round\ud835\udc61 \u2217 +1. By the triangle inequality, \ud835\udc370,1 \u2264\ud835\udc37 1,2 +\ud835\udc37 0,2. By Lemma 4.14, we know that \ud835\udc370,2 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ). Therefore, to complete", "source": "loadbalancing.pdf"}, {"text": "worlds0and2at the beginning of round\ud835\udc61 \u2217 +1. By the triangle inequality, \ud835\udc370,1 \u2264\ud835\udc37 1,2 +\ud835\udc37 0,2. By Lemma 4.14, we know that \ud835\udc370,2 \u2264\ud835\udc51 (\ud835\udc61 \u2217 ). Therefore, to complete the proof that \ud835\udc51 (\ud835\udc61 \u2217+1) \u2264\ud835\udc51 (\ud835\udc61 \u2217 ) + 2, it suffices to show that \ud835\udc371,2 \u2264 2. By the World Difference Observation, the states in worlds 1 and 2 at the beginning of round\ud835\udc61 \u2217 + 1differ in the positions of at most two balls. It follows that\ud835\udc371,2 \u22642, as desired.\u25a1 Finally, by putting the preceding lemmas together, we can prove Proposition 4.10. Proposition 4.10.Fix any neighboring datasets S and S\u2032, and \u210e1, \u210e2, and \ud835\udc45. The recourse of (S,S \u2032, \u210e1, \u210e2, \ud835\udc45) incurred by Slice", "source": "loadbalancing.pdf"}, {"text": "together, we can prove Proposition 4.10. Proposition 4.10.Fix any neighboring datasets S and S\u2032, and \u210e1, \u210e2, and \ud835\udc45. The recourse of (S,S \u2032, \u210e1, \u210e2, \ud835\udc45) incurred by Slice and Spread is\ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)). 16 Proof.We assume that\ud835\udc61 \u2217 \u22600. The proof follows by stringing together the previous lemmas as follows: 1+ \u2211\ufe01 \ud835\udc65\u2208 S\u2229S \u2032 I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) =1+ \u2211\ufe01 (\ud835\udc61,\ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b] I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) =1+ \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 (0) I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) + \u2211\ufe01 (\ud835\udc61,\ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b] (\ud835\udc61,\ud835\udc56)\u2260(\ud835\udc61 \u2217,\ud835\udc56\u2217 ) \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) \u22641+\ud835\udc38 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 + \u2211\ufe01 (\ud835\udc61,\ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b] (\ud835\udc61,\ud835\udc56)\u2260(\ud835\udc61 \u2217,\ud835\udc56\u2217 ) \ud835\udc38", "source": "loadbalancing.pdf"}, {"text": "[\ud835\udc5b] (\ud835\udc61,\ud835\udc56)\u2260(\ud835\udc61 \u2217,\ud835\udc56\u2217 ) \u2211\ufe01 \ud835\udc65\u2208 C (\ud835\udc61) \ud835\udc56 (0) I (\ud835\udf190(\ud835\udc65)\u2260\ud835\udf19 1(\ud835\udc65) ) \u22641+\ud835\udc38 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 + \u2211\ufe01 (\ud835\udc61,\ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b] (\ud835\udc61,\ud835\udc56)\u2260(\ud835\udc61 \u2217,\ud835\udc56\u2217 ) \ud835\udc38 (\ud835\udc61) \ud835\udc56 (by Lemma 4.11) \u22641+\ud835\udc51 (\ud835\udc61 \u2217 ) \ud835\udc56\u2217 +2+ \u2211\ufe01 (\ud835\udc61,\ud835\udc56) \u2208 [\ud835\udc47] \u00d7 [\ud835\udc5b] (\ud835\udc61,\ud835\udc56)\u2260(\ud835\udc61 \u2217,\ud835\udc56\u2217 ) \ud835\udc51 (\ud835\udc61) \ud835\udc56 (by Lemma 4.12 and Lemma 4.15) =3+ \u2211\ufe01 \ud835\udc61\u2208 [\ud835\udc47] \ud835\udc51 (\ud835\udc61) =3+ \u2211\ufe01 \ud835\udc61\u2208 [\ud835\udc47] (\ud835\udc51 (1) +2)(by Lemma 4.14 and Lemma 4.15) =3+\ud835\udc47\u00b73 =\ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)). We can prove for the case of \ud835\udc61 \u2217 = 0similarly. In this case, note that we can reuse the same sequence of inequalities above, except where the\ud835\udc61\u2217 terms disappear, since(\ud835\udc61 \u2217, \ud835\udc56\u2217)\u2209[\ud835\udc47] \u00d7 [\ud835\udc5b].\u25a1 We can now prove Theorem", "source": "loadbalancing.pdf"}, {"text": "0similarly. In this case, note that we can reuse the same sequence of inequalities above, except where the\ud835\udc61\u2217 terms disappear, since(\ud835\udc61 \u2217, \ud835\udc56\u2217)\u2209[\ud835\udc47] \u00d7 [\ud835\udc5b].\u25a1 We can now prove Theorem 4.1 Theorem 4.1.The expected recourse of Slice and Spread is \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Furthermore, for each state, the cumulative overload of that state under Slice and Spread is\ud835\udc42(\ud835\udc5b)with high probability in\ud835\udc5b. Proof.Follows from Proposition 4.10 and Proposition 4.2.\u25a1 5 Reducing the Overload to\ud835\udc42(1) In this section, we will show how to reduce the overload to\ud835\udc42( 1), while still achieving expected recourse \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Our approach will be through a black-box construction, taking any algorithm with the properties from the previous section, and turning it into an algorithm with overload\ud835\udc42(", "source": "loadbalancing.pdf"}, {"text": "recourse \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Our approach will be through a black-box construction, taking any algorithm with the properties from the previous section, and turning it into an algorithm with overload\ud835\udc42( 1). Note that, through- out the section, we will continue with the WLOG assumption from Section 2 that the number of balls is always either\ud835\udc5a\u22121or\ud835\udc5a. Throughout the section, label every ball\ud835\udc65 randomly one of three types: Type1, Type2, and Type3, where the probability\ud835\udc5d \ud835\udc56 of each Type\ud835\udc56is given by\ud835\udc5d 1 =0.89, \ud835\udc5d 2 =0.1, \ud835\udc5d 3 =0.01. Consider a history-independent algorithmA that is defined for sets of\ud835\udc5a\u2212 1and \ud835\udc5a balls, and that assigns them to\ud835\udc5bbins. CallAagood pre-baking algorithmif it has the following properties: \u2022 First Choices for Types1and2:For Type1and Type2balls", "source": "loadbalancing.pdf"}, {"text": "algorithmA that is defined for sets of\ud835\udc5a\u2212 1and \ud835\udc5a balls, and that assigns them to\ud835\udc5bbins. CallAagood pre-baking algorithmif it has the following properties: \u2022 First Choices for Types1and2:For Type1and Type2balls \ud835\udc65, A assigns \ud835\udc65 to \u210e1(\ud835\udc65) and does not evaluate\u210e 2(\ud835\udc65). \u2022 Small Cumulative Overload:For a given set S of \ud835\udc5a balls, we have with high probability in\ud835\udc5b that A (\ud835\udc46)has cumulative overload\ud835\udc42(\ud835\udc5b). 17 We will show that, given any good pre-baking algorithm, we can construct a new algorithm that has the same asymptotic expected recourse, but that has overload\ud835\udc42(1): Theorem 5.1.Given any good pre-baking algorithm A with expected recourse Recourse(A) , we can construct a new history-independent algorithm with expected recourse \ud835\udc42(Recourse(A)) and whose overload is \ud835\udc42( 1) with", "source": "loadbalancing.pdf"}, {"text": "Theorem 5.1.Given any good pre-baking algorithm A with expected recourse Recourse(A) , we can construct a new history-independent algorithm with expected recourse \ud835\udc42(Recourse(A)) and whose overload is \ud835\udc42( 1) with high probability in\ud835\udc5b. 5.1 Background: History Independent Cuckoo Hashing on\ud835\udc42(\ud835\udc5b)Elements One tool that we will need in this section is a simple but elegant paradigm due to Naor, Segev, and Wieder [27] for how to construct an efficient history-independent cuckoo hash table. We will describe their paradigm in the context of our balls-and-bins problem, where\ud835\udc5a=\ud835\udc42(\ud835\udc5b). Orienting each component separately.Given a set \ud835\udc35 of balls, define thecuckoo graph \ud835\udc3a(\ud835\udc35) to be the graph with vertices in[\ud835\udc5b] and edges{(\u210e 1(\ud835\udc65), \u210e2(\ud835\udc65)) |\ud835\udc65\u2208\ud835\udc35} . We can think of an assignment of balls to", "source": "loadbalancing.pdf"}, {"text": "set \ud835\udc35 of balls, define thecuckoo graph \ud835\udc3a(\ud835\udc35) to be the graph with vertices in[\ud835\udc5b] and edges{(\u210e 1(\ud835\udc65), \u210e2(\ud835\udc65)) |\ud835\udc65\u2208\ud835\udc35} . We can think of an assignment of balls to bins as an orientation on the graph\ud835\udc3a, where the edge corresponding to a ball\ud835\udc65 points to vertex\u210e\ud835\udc56 (\ud835\udc65) iff \ud835\udc65 is assigned to bin\u210e\ud835\udc56 (\ud835\udc65) . The maximum load across the bins corresponds to the maximum in-degree in the directed graph. Let OR be an arbitrary deterministic algorithm that, given a connected component \ud835\udc36 of a graph \ud835\udc3a, produces a minimum-in-degree orientation of \ud835\udc36. Define thecanonical orientation OR (\ud835\udc3a(\ud835\udc35) ) to be the orientation obtained by running OR on each connected component of \ud835\udc3a(\ud835\udc35) . We will also define the", "source": "loadbalancing.pdf"}, {"text": "minimum-in-degree orientation of \ud835\udc36. Define thecanonical orientation OR (\ud835\udc3a(\ud835\udc35) ) to be the orientation obtained by running OR on each connected component of \ud835\udc3a(\ud835\udc35) . We will also define the Canonical Orientation Procedureas the procedure for running OR on each connected component of\ud835\udc3a(\ud835\udc35) . Naor, Segev, and Wieder [27] make two key observations: 1. Fix a ball\ud835\udc65\u2208\ud835\udc35 , and define\ud835\udc36 to be the connected component in\ud835\udc3a(\ud835\udc35) containing edge(\u210e1(\ud835\udc65), \u210e2(\ud835\udc65)) . Let |\ud835\udc36| denote the number of edges in\ud835\udc36. If we look atOR (\ud835\udc3a(\ud835\udc35) ), the number of edges whose orientations change when we remove\ud835\udc65is at most|\ud835\udc36|. That is, |OR (\ud835\udc3a(\ud835\udc35) ) \u25b3OR (\ud835\udc3a(\ud835\udc35\\ {\ud835\udc65}) ) | \u2264 |\ud835\udc36|. 2. If every connected component in\ud835\udc3a(\ud835\udc35) contains \ud835\udc42( 1) cycles, then OR", "source": "loadbalancing.pdf"}, {"text": "change when we remove\ud835\udc65is at most|\ud835\udc36|. That is, |OR (\ud835\udc3a(\ud835\udc35) ) \u25b3OR (\ud835\udc3a(\ud835\udc35\\ {\ud835\udc65}) ) | \u2264 |\ud835\udc36|. 2. If every connected component in\ud835\udc3a(\ud835\udc35) contains \ud835\udc42( 1) cycles, then OR (\ud835\udc3a(\ud835\udc35) ) has maximum in-degree \ud835\udc42(1); and, thus, the corresponding balls-to-bins assignment has maximum load\ud835\udc42(1). Using these observations, Naor, Segev, and Wieder [27] showed that, for sets \ud835\udc35 of \ud835\udc5b/2 \u2212\u03a9(\ud835\udc5b) balls, the canonical orientation OR (\ud835\udc3a(\ud835\udc35) ) achieves both expected recourse\ud835\udc42( 1) and overload\ud835\udc42( 1) (with high probability). This gave them a simple way of constructing efficient history-independent cuckoo hashing. Generalizing to our setting.In this section, we will make repeated use of the Canonical Orientation Procedure. The main difficulty that we will need to overcome is that, a priori,", "source": "loadbalancing.pdf"}, {"text": "hashing. Generalizing to our setting.In this section, we will make repeated use of the Canonical Orientation Procedure. The main difficulty that we will need to overcome is that, a priori, the balls\ud835\udc35 to which we wish to apply the Canonical Orientation Procedure do not necessarily produce a graph\ud835\udc3a with small components (or even with few cycles per component). We will overcome this challenge through a mixture of algorithmic techniques (i.e., ways of changing which set of balls we need to apply the procedure to) and analytical techniques (i.e., analyses of graphs\ud835\udc3awhose edges may be neither independent nor uniformly random). Extending to slightly larger sets of balls.Before continuing, we remark that there is a simple way to extend the Canonical Orientation", "source": "loadbalancing.pdf"}, {"text": "graphs\ud835\udc3awhose edges may be neither independent nor uniformly random). Extending to slightly larger sets of balls.Before continuing, we remark that there is a simple way to extend the Canonical Orientation Procedure to allow for\ud835\udc5a=\ud835\udc42(\ud835\udc5b) balls, rather than requiring\ud835\udc5a\u2264\ud835\udc5b/ 2\u2212\u03a9(\ud835\udc5b) . We can simply (1) partition the balls at random into\ud835\udc42( 1) subsets, each of size\ud835\udc5b/2 \u2212\u03a9(\ud835\udc5b) ; and then (2) process each subset separately using the Canonical Orientation Procedure. We refer to this procedure as theExtended Canonical Orientation (ECO) Procedure. Formally, for a set of balls \ud835\udc35, we define the extended orientationOR \u2217(\ud835\udc3a(\ud835\udc35)) as follows. Let \ud835\udc5d\u2208 ( 0, 1) be a sufficiently 18 small positive constant satisfying the constraint that\ud835\udc5d \u22121 is an integer. Partition the graph\ud835\udc3a(\ud835\udc35) into \ud835\udc5d", "source": "loadbalancing.pdf"}, {"text": "extended orientationOR \u2217(\ud835\udc3a(\ud835\udc35)) as follows. Let \ud835\udc5d\u2208 ( 0, 1) be a sufficiently 18 small positive constant satisfying the constraint that\ud835\udc5d \u22121 is an integer. Partition the graph\ud835\udc3a(\ud835\udc35) into \ud835\udc5d \u22121 random subgraphs\ud835\udc3a1(\ud835\udc35), \ud835\udc3a2(\ud835\udc35), . . . , \ud835\udc3a\ud835\udc5d \u22121 (\ud835\udc35), where each edge in\ud835\udc3a(\ud835\udc35) gets assigned (via a hash function) to a random subgraph. Then the orientationOR \u2217(\ud835\udc3a(\ud835\udc35)) is obtained by computing the canonical orientation of each subgraph, that is, OR \u2217(\ud835\udc3a(\ud835\udc35)) := \ud835\udc5d \u22121 \u00d8 \ud835\udc57=1 OR (\ud835\udc3a\ud835\udc57 (\ud835\udc35)). As a convention throughout the section, we will use the notations\ud835\udc3a \ud835\udc57 (\ud835\udc35) , OR (\ud835\udc3a\ud835\udc57 (\ud835\udc35)) and OR \u2217(\ud835\udc3a(\ud835\udc35)) whenever we discuss an application of the ECO Procedure to a set\ud835\udc35 of balls. We will also use\ud835\udc5d to", "source": "loadbalancing.pdf"}, {"text": "the notations\ud835\udc3a \ud835\udc57 (\ud835\udc35) , OR (\ud835\udc3a\ud835\udc57 (\ud835\udc35)) and OR \u2217(\ud835\udc3a(\ud835\udc35)) whenever we discuss an application of the ECO Procedure to a set\ud835\udc35 of balls. We will also use\ud835\udc5d to refer to the small positive constant used within the procedure. 5.2 The Algorithm To present the algorithm that we will use to prove Theorem 5.1, we begin motivating the basic ideas which the algorithm will use. Idea 1: Apply the ECO Procedure to balls above height\ud835\udc5a/\ud835\udc5b.Let \ud835\udc35 denote the set of balls thatA places above height\ud835\udc5a/\ud835\udc5b (ties for which balls are in\ud835\udc35 can be broken arbitrarily). The first idea one might try is to rearrange the balls in\ud835\udc35 (i.e., reselect which hash function each ball uses) so that they are", "source": "loadbalancing.pdf"}, {"text": "are in\ud835\udc35 can be broken arbitrarily). The first idea one might try is to rearrange the balls in\ud835\udc35 (i.e., reselect which hash function each ball uses) so that they are in an arrangement with overload\ud835\udc42(1). Specifically, we could attempt to simply apply the ECO Procedure to\ud835\udc35. The problem with this idea is that the balls\ud835\udc35 havespoiled randomness. Because \ud835\udc35 is constructed by an algo- rithm A that gets to see the hashes of each ball, we cannot assume that the hashes{\u210e1(\ud835\udc65), \u210e2(\ud835\udc65) |\ud835\udc65\u2208\ud835\udc35} are independent or uniformly random. This means that the graphs\ud835\udc3a1(\ud835\udc35), \ud835\udc3a2(\ud835\udc35), . . . , \ud835\udc3a\ud835\udc5d \u22121 (\ud835\udc35) do not necessarily have small connected components (or, for that matter, a small number of cycles in each connected", "source": "loadbalancing.pdf"}, {"text": "that the graphs\ud835\udc3a1(\ud835\udc35), \ud835\udc3a2(\ud835\udc35), . . . , \ud835\udc3a\ud835\udc5d \u22121 (\ud835\udc35) do not necessarily have small connected components (or, for that matter, a small number of cycles in each connected component). Idea 2: Swapping the balls in \ud835\udc35 for balls with fresh randomness.To rectify the issue of spoiled randomness, we can attempt the following procedure for swapping the balls in\ud835\udc35 with new balls\ud835\udc35\u2032 that have fully random hashes. For this, we use what we call theTwo-Phase Swapping Procedure: \u2022 Phase 1:For each bin \ud835\udc56 with \ud835\udc4e\ud835\udc56 Type2balls and \ud835\udc58\ud835\udc56 balls from \ud835\udc35, removemin(\ud835\udc4e\ud835\udc56, \ud835\udc58\ud835\udc56) Type2balls from the bin and add them to an intermediate set\ud835\udc431 (when choosing which Type2balls to place in \ud835\udc431, we may use an arbitrary deterministic tie-breaking", "source": "loadbalancing.pdf"}, {"text": "from \ud835\udc35, removemin(\ud835\udc4e\ud835\udc56, \ud835\udc58\ud835\udc56) Type2balls from the bin and add them to an intermediate set\ud835\udc431 (when choosing which Type2balls to place in \ud835\udc431, we may use an arbitrary deterministic tie-breaking rule). If\ud835\udc4e\ud835\udc56 <\ud835\udc58 \ud835\udc56, then say that\ud835\udc58\ud835\udc56 \u2212\ud835\udc4e \ud835\udc56 of the balls from \ud835\udc35 in the bin experiencePhase 1 failures, and we place\ud835\udc58\ud835\udc56 \u2212\ud835\udc4e \ud835\udc56 dummy ballsinto \ud835\udc431, each of which has \u210e1(\ud835\udc65)=\ud835\udc56 and \u210e2(\ud835\udc65) generated independently and uniformly at random. The dummy balls take their second hashes from a random tape\ud835\udc45\ud835\udc56 for bin\ud835\udc56. \u2022 Phase 2:Place each \ud835\udc65\u2208\ud835\udc43 1 into its second-choice hash\u210e2(\ud835\udc65) . Now, for each bin\ud835\udc56 with \ud835\udc4f\ud835\udc56 Type1balls and \ud835\udc57\ud835\udc56 balls from\ud835\udc431, removemin(\ud835\udc4f\ud835\udc56, \ud835\udc57\ud835\udc56) Type1balls from the bin and add them to a set\ud835\udc432 (when choosing", "source": "loadbalancing.pdf"}, {"text": "into its second-choice hash\u210e2(\ud835\udc65) . Now, for each bin\ud835\udc56 with \ud835\udc4f\ud835\udc56 Type1balls and \ud835\udc57\ud835\udc56 balls from\ud835\udc431, removemin(\ud835\udc4f\ud835\udc56, \ud835\udc57\ud835\udc56) Type1balls from the bin and add them to a set\ud835\udc432 (when choosing which Type1balls to place in\ud835\udc432, we may use an arbitrary deterministic tie-breaking rule). If\ud835\udc4f\ud835\udc56 <\ud835\udc57 \ud835\udc56, then say that \ud835\udc57\ud835\udc56 \u2212\ud835\udc4f \ud835\udc56 of the Type2balls in the bin experiencePhase 2 failures, and place\ud835\udc57\ud835\udc56 \u2212\ud835\udc4f \ud835\udc56 dummy balls into \ud835\udc432, each of which has\u210e1(\ud835\udc65)=\ud835\udc56 and \u210e2(\ud835\udc65) generated independently and uniformly at random. The dummy balls take their second hashes from a second random tape\ud835\udc45\u2032 \ud835\udc56 for bin\ud835\udc56. Finally, define\ud835\udc35 \u2032 :=\ud835\udc43 2. The good news about the above procedure is that the balls\ud835\udc65\u2208\ud835\udc35 \u2032 have mutually independent and fully random hashes", "source": "loadbalancing.pdf"}, {"text": "second random tape\ud835\udc45\u2032 \ud835\udc56 for bin\ud835\udc56. Finally, define\ud835\udc35 \u2032 :=\ud835\udc43 2. The good news about the above procedure is that the balls\ud835\udc65\u2208\ud835\udc35 \u2032 have mutually independent and fully random hashes \u210e1(\ud835\udc65), \u210e2(\ud835\udc65) . This will allow us to, later in the section, successfully apply the ECO Procedure to the graph\ud835\udc3a(\ud835\udc35 \u2032). Lemma 5.2(Random Hashes in \ud835\udc35\u2032).Fix some set \ud835\udc35 of \ud835\udc42(\ud835\udc5b) balls. Condition on fixed values of \u210e1(\ud835\udc65) for each ball \ud835\udc65\u2208\ud835\udc35 , and on fixed values of\u210e2(\ud835\udc65) for each Type 3 ball in\ud835\udc35. Even with these conditions, the balls\ud835\udc65\u2208\ud835\udc35 \u2032 =\ud835\udc43 2 (including the dummy balls) have mutually independent and fully random hashes\u210e 1(\ud835\udc65), \u210e2(\ud835\udc65). 19 Proof. The balls\ud835\udc65\u2208\ud835\udc43 1 take their first hashes\u210e1(\ud835\udc65) from the first hashes of", "source": "loadbalancing.pdf"}, {"text": "\u2032 =\ud835\udc43 2 (including the dummy balls) have mutually independent and fully random hashes\u210e 1(\ud835\udc65), \u210e2(\ud835\udc65). 19 Proof. The balls\ud835\udc65\u2208\ud835\udc43 1 take their first hashes\u210e1(\ud835\udc65) from the first hashes of balls in\ud835\udc35 (these hashes are spoiled), but have second hashes\u210e2(\ud835\udc65) that are fully random. The balls\ud835\udc65\u2208\ud835\udc43 2 take their first hashes\u210e1(\ud835\udc65) from the second hashes of balls in\ud835\udc431 (these hashes are already random!) and have second hashes\u210e2(\ud835\udc65) that are fully ran- dom. The result is that the balls\ud835\udc65\u2208\ud835\udc43 2 have mutually independent and fully random hashes\u210e1(\ud835\udc65), \u210e2(\ud835\udc65) . \u25a1 The other good news is that there will likely not be too many balls that incur Phase 1 or Phase 2 failures. In fact, if\ud835\udc5a/\ud835\udc5b=\ud835\udf14(log\ud835\udc5b) , one can argue with high", "source": "loadbalancing.pdf"}, {"text": "other good news is that there will likely not be too many balls that incur Phase 1 or Phase 2 failures. In fact, if\ud835\udc5a/\ud835\udc5b=\ud835\udf14(log\ud835\udc5b) , one can argue with high probability in\ud835\udc5b that there arenoPhase 1 or Phase 2 failures. On the other hand, if\ud835\udc5a/\ud835\udc5b=\ud835\udc42(log\ud835\udc5b) , then there may be many balls that incur Phase 1 or Phase 2 failures. These balls may still sit above height\ud835\udc5a/\ud835\udc5b, and could cause a large overload. Idea 3: Over-approximating Phase 1 and Phase 2 failures with a well-behaved set.Let \ud835\udc39 denote the set of balls that incur Phase 1 and Phase 2 failures (within each bin, we may break ties arbitrarily when deciding which balls are considered to have experienced a failure). One", "source": "loadbalancing.pdf"}, {"text": "of balls that incur Phase 1 and Phase 2 failures (within each bin, we may break ties arbitrarily when deciding which balls are considered to have experienced a failure). One challenge in reasoning about\ud835\udc39 is that the distribution of the hashes of the balls in\ud835\udc39 may be influenced by the behavior of the algorithmA. A critical step in this section is to define a larger set\ud835\udc39 \u2032 that does not depend on the algorithm A, and that can be used as a proxy for\ud835\udc39. Let \ud835\udf07=\ud835\udc5a/\ud835\udc5b , let \ud835\udf00= 0.0001, and recall that each ball has probability\ud835\udc5d \ud835\udc57 of being Type \ud835\udc57, where \ud835\udc5d1 = 0.89, \ud835\udc5d2 = 0.1, \ud835\udc5d3 = 0.01. We say that a bin\ud835\udc56 is Type \ud835\udc8b", "source": "loadbalancing.pdf"}, {"text": "and recall that each ball has probability\ud835\udc5d \ud835\udc57 of being Type \ud835\udc57, where \ud835\udc5d1 = 0.89, \ud835\udc5d2 = 0.1, \ud835\udc5d3 = 0.01. We say that a bin\ud835\udc56 is Type \ud835\udc8b overloaded from hash \ud835\udc89\ud835\udc8c if the number of balls\ud835\udc65 satisfying \u210e\ud835\udc58 (\ud835\udc65)=\ud835\udc56 is at least (\ud835\udc5d \ud835\udc57 +\ud835\udf00)\ud835\udf07 . Likewise, we say that a bin\ud835\udc56 isType \ud835\udc8b underloaded from hash \ud835\udc89\ud835\udc8c if the number of balls\ud835\udc65 satisfying \u210e\ud835\udc58 (\ud835\udc65)=\ud835\udc56 is at most (\ud835\udc5d \ud835\udc57 \u2212\ud835\udf00)\ud835\udf07 . A bin \ud835\udc56 is said to beType \ud835\udc8b overloaded(resp.Type \ud835\udc8b underloaded) if it is Type-\ud835\udc57 overloaded (resp. Type\ud835\udc57underloaded) from at least one of the hash functions\u210e 1, \u210e2. With these definitions in mind, we construct\ud835\udc39\u2032 to consist of all balls\ud835\udc65such that: \u2022\ud835\udc65 is a", "source": "loadbalancing.pdf"}, {"text": "Type-\ud835\udc57 overloaded (resp. Type\ud835\udc57underloaded) from at least one of the hash functions\u210e 1, \u210e2. With these definitions in mind, we construct\ud835\udc39\u2032 to consist of all balls\ud835\udc65such that: \u2022\ud835\udc65 is a Type\ud835\udc56 ball for some\ud835\udc56, and there is some\ud835\udc57\u2208 { 1, 2} such that\u210e \ud835\udc57 (\ud835\udc65) is a Type\ud835\udc56 overloaded bin from\u210e \ud835\udc57. \u2022\ud835\udc65 is a Type\ud835\udc56 ball for some\ud835\udc56, and at least one of\u210e1(\ud835\udc65), \u210e2(\ud835\udc65) is a Type \ud835\udc57 underloaded bin for some \ud835\udc57<\ud835\udc56 . Say that the system is\ud835\udc6d \u2032-safeif removing the balls \ud835\udc39 \u2032 from the system results in every bin having load at most\ud835\udc5a/\ud835\udc5b. A key step in the analysis will be to argue (in Lemma 5.3) that, after we apply the Two-Phase Swapping Procedure, the state", "source": "loadbalancing.pdf"}, {"text": "in every bin having load at most\ud835\udc5a/\ud835\udc5b. A key step in the analysis will be to argue (in Lemma 5.3) that, after we apply the Two-Phase Swapping Procedure, the state of the system is\ud835\udc39 \u2032-safe. (Note that, at this point in time, the balls\ud835\udc35\u2032 are also not in the system.) This captures the way in which\ud835\udc39\u2032 is a good \u201cproxy\u201d for\ud835\udc39. For describing our algorithm, it will be helpful to think of\ud835\udc39\u2032 as a union of sets defined as follows: \u2022 For each \ud835\udc4e, \ud835\udc4f\u2208 [ 3] with \ud835\udc4e>\ud835\udc4f , and for each\ud835\udc50, \ud835\udc51\u2208 [ 2], define \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) to be the set of Type\ud835\udc4e balls \ud835\udc65 for which\u210e \ud835\udc50 (\ud835\udc65)is a bin that is Type\ud835\udc4funderloaded due to\u210e \ud835\udc51. \u2022", "source": "loadbalancing.pdf"}, {"text": "and for each\ud835\udc50, \ud835\udc51\u2208 [ 2], define \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) to be the set of Type\ud835\udc4e balls \ud835\udc65 for which\u210e \ud835\udc50 (\ud835\udc65)is a bin that is Type\ud835\udc4funderloaded due to\u210e \ud835\udc51. \u2022 For each \ud835\udc4e\u2208 [ 3] and \ud835\udc50\u2208 [ 2], define\ud835\udc4c (\ud835\udc4e,\ud835\udc50) to be the set of Type\ud835\udc4e balls \ud835\udc65 for which \u210e\ud835\udc50 (\ud835\udc65) is a bin that is Type\ud835\udc4eoverloaded due to\u210e \ud835\udc50. \u2022Then\ud835\udc39 \u2032 is the union of the\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) s and\ud835\udc4c (\ud835\udc4e,\ud835\udc50) s. The final step in our algorithm will be to apply the ECO Procedure to each\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) and to each\ud835\udc4c (\ud835\udc4e,\ud835\udc50) . This will be somewhat tricky because the edges in a given\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (or \ud835\udc4c (\ud835\udc4e,\ud835\udc50) ) are not independent or uniformly random. Nevertheless, the edges", "source": "loadbalancing.pdf"}, {"text": "and to each\ud835\udc4c (\ud835\udc4e,\ud835\udc50) . This will be somewhat tricky because the edges in a given\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (or \ud835\udc4c (\ud835\udc4e,\ud835\udc50) ) are not independent or uniformly random. Nevertheless, the edges in a given\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) or \ud835\udc4c (\ud835\udc4e,\ud835\udc50) will turn out to have a nice enough combinatorial structure that we will be able to succeed in applying the ECO Procedure to them. The full algorithm.We can now describe the full algorithm for proving Theorem 5.1. Let \ud835\udc5d\u2208 ( 0, 1) be a sufficiently small positive constant to be used in the ECO Procedure. 1. First, applyAto get an initial orientationA (\ud835\udc46)of the balls. 20 2. Apply the Two-Phase Swapping Procedure toA (\ud835\udc35), and let\ud835\udc43 1, \ud835\udc432, \ud835\udc35\u2032 be as defined in", "source": "loadbalancing.pdf"}, {"text": "ECO Procedure. 1. First, applyAto get an initial orientationA (\ud835\udc46)of the balls. 20 2. Apply the Two-Phase Swapping Procedure toA (\ud835\udc35), and let\ud835\udc43 1, \ud835\udc432, \ud835\udc35\u2032 be as defined in the procedure. 3. Next, apply the ECO Procedure to the balls in\ud835\udc35 \u2032 (\ud835\udc46). 4. Finally, define\ud835\udc39 \u2032 =( \u00d0 \ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51 \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) ) \u222a (\u00d0 \ud835\udc4e,\ud835\udc50 \ud835\udc4c (\ud835\udc4e,\ud835\udc50) ) as above, and reorient the edges in\ud835\udc39 \u2032 as follows: For each \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) and for each\ud835\udc4c (\ud835\udc4e,\ud835\udc50) , one after another, apply the ECO Procedure to the graph induced by those balls. If a ball is in multiple such sets, it ends up using the orientation given by the final of the sets that the ball is in. Note", "source": "loadbalancing.pdf"}, {"text": "by those balls. If a ball is in multiple such sets, it ends up using the orientation given by the final of the sets that the ball is in. Note that \ud835\udc431, \ud835\udc432, \ud835\udc35\u2032, \ud835\udc39 \u2032, \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) , and \ud835\udc4c (\ud835\udc4e,\ud835\udc50) are all implicit functions of \ud835\udc46 and will sometimes also be written with\ud835\udc46as an argument (e.g.,\ud835\udc43 1(\ud835\udc46)). 5.3 Bounding Overload To prove Theorem 5.1, the first step is to bound the overload of our final algorithm by \ud835\udc42( 1) (with high probability). To do this, we begin by analyzing the set\ud835\udc39 \u2032. Recall that the system is said to be\ud835\udc39 \u2032-safe if removing the balls \ud835\udc39 \u2032 from the system results in every bin having load at most\ud835\udc5a/\ud835\udc5b.", "source": "loadbalancing.pdf"}, {"text": "analyzing the set\ud835\udc39 \u2032. Recall that the system is said to be\ud835\udc39 \u2032-safe if removing the balls \ud835\udc39 \u2032 from the system results in every bin having load at most\ud835\udc5a/\ud835\udc5b. The next lemma argues that, after the Two-Phase Swapping Procedure, the system is\ud835\udc39\u2032-safe. Lemma 5.3.After applying the Two-Phase Swapping Procedure to A (\ud835\udc35), the system is \ud835\udc39 \u2032-safe. (Note that, at this point in time, the balls\ud835\udc35 \u2032 are also not in the system.) Proof.We begin by arguing the following claim: Claim 5.4.The system is \ud835\udc39 \u2032-safe after Phase 1. (Note that, at this point, the balls \ud835\udc431(S) have been removed from the system). To rephrase Claim 5.4, we would like to show that, after removing the balls in\ud835\udc431 from", "source": "loadbalancing.pdf"}, {"text": "(Note that, at this point, the balls \ud835\udc431(S) have been removed from the system). To rephrase Claim 5.4, we would like to show that, after removing the balls in\ud835\udc431 from the system, and then removing the balls in\ud835\udc39 \u2032 \\\ud835\udc43 1, each bin has load at most\ud835\udf07=\ud835\udc5a/\ud835\udc5b . To prove this, it is helpful to think of the equivalent process of first removing\ud835\udc39 \u2032 and then removing\ud835\udc431 \\\ud835\udc39 \u2032, where \ud835\udc39 \u2032 and \ud835\udc431 are known in advance. Before proving the claim, we introduce some notation. We fix a bin\ud835\udc56 and define the variables\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56, \ud835\udc67\ud835\udc56, respectively, to be the number of Type 3, Type 2, and Type 1 balls in the bin after the\ud835\udc39 \u2032 balls are removed but", "source": "loadbalancing.pdf"}, {"text": "and define the variables\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56, \ud835\udc67\ud835\udc56, respectively, to be the number of Type 3, Type 2, and Type 1 balls in the bin after the\ud835\udc39 \u2032 balls are removed but before the\ud835\udc43 1 \\\ud835\udc39 \u2032 balls are removed: \u2022\ud835\udc65 \ud835\udc56 is the number of non-\ud835\udc39\u2032 Type3balls in bin\ud835\udc56before Phase 1. \u2022\ud835\udc66 \ud835\udc56 is the number of non-\ud835\udc39\u2032 Type2balls in bin\ud835\udc56before Phase 1. \u2022\ud835\udc67 \ud835\udc56 is the number of non-\ud835\udc39\u2032 Type1balls in bin\ud835\udc56before Phase 1. Let\ud835\udc5c \ud835\udc56 be the number of balls in bin\ud835\udc56above height\ud835\udf07after removing\ud835\udc39 \u2032 from the system, i.e. \ud835\udc5c\ud835\udc56 =max(0, \ud835\udc65 \ud835\udc56 +\ud835\udc66 \ud835\udc56 +\ud835\udc67 \ud835\udc56 \u2212\ud835\udf07).(10) Proof of Claim 5.4. We prove the claim for each bin\ud835\udc56. We divide the proof of the claim into two cases", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc5c\ud835\udc56 =max(0, \ud835\udc65 \ud835\udc56 +\ud835\udc66 \ud835\udc56 +\ud835\udc67 \ud835\udc56 \u2212\ud835\udf07).(10) Proof of Claim 5.4. We prove the claim for each bin\ud835\udc56. We divide the proof of the claim into two cases based on\ud835\udc66 \ud835\udc56: (1)\ud835\udc66 \ud835\udc56 =0, and (2)\ud835\udc66 \ud835\udc56 \u22600. 21 Case1:It suffices to show that\ud835\udc5c \ud835\udc56 =0. To see why\ud835\udc5c \ud835\udc56 =0in Case1, it is helpful to note that \ud835\udc65\ud835\udc56 \u22642(\ud835\udc5d 3 +\ud835\udf00)\ud835\udf07.(11) and \ud835\udc67\ud835\udc56 \u2264 (\ud835\udc5d 1 +\ud835\udf00)\ud835\udf07,(12) where (11) comes from the fact that, for each\ud835\udc58\u2208 { 1, 2}, \ud835\udc39 \u2032 contains Type 3 balls\ud835\udc65 for which \u210e\ud835\udc58 (\ud835\udc65) is in a Type 3 overloaded bin from\u210e\ud835\udc58; and (12) comes from the fact that \ud835\udc39 \u2032 contains all Type 1 balls\ud835\udc65 for which \u210e1(\ud835\udc65)is a bin that", "source": "loadbalancing.pdf"}, {"text": "\u210e\ud835\udc58 (\ud835\udc65) is in a Type 3 overloaded bin from\u210e\ud835\udc58; and (12) comes from the fact that \ud835\udc39 \u2032 contains all Type 1 balls\ud835\udc65 for which \u210e1(\ud835\udc65)is a bin that is Type 1 overloaded from\u210e1. Since\ud835\udc66 \ud835\udc56 =0, it follows that\ud835\udc65 \ud835\udc56 +\ud835\udc66 \ud835\udc56 +\ud835\udc67 \ud835\udc56 \u2264 (\ud835\udc5d 1 +2\ud835\udc5d 3 +2\ud835\udf00)\ud835\udf07<\ud835\udf07, which implies by (10) that\ud835\udc5c \ud835\udc56 =0. Case2:It\u2019s helpful to first establish two inequalities. First, since \ud835\udc39 \u2032 contains every Type 2 ball\ud835\udc65 for which \u210e1(\ud835\udc65)is a Type 2 overloaded bin from\u210e 1, we know that \ud835\udc66\ud835\udc56 \u2264 (\ud835\udc5d 2 +\ud835\udf00)\ud835\udf07(13) Combining (13) and (12) gives\ud835\udc66\ud835\udc56 +\ud835\udc67 \ud835\udc56 \u2264\ud835\udc5d 1\ud835\udf07+\ud835\udc5d 2\ud835\udf07+2\ud835\udf00\ud835\udf07\u2264\ud835\udf07, which implies by (10) that \ud835\udc5c\ud835\udc56 \u2264\ud835\udc65 \ud835\udc56 .(14) We now subdivide Case2into two more cases: (2a)\ud835\udc66\ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "(\ud835\udc5d 2 +\ud835\udf00)\ud835\udf07(13) Combining (13) and (12) gives\ud835\udc66\ud835\udc56 +\ud835\udc67 \ud835\udc56 \u2264\ud835\udc5d 1\ud835\udf07+\ud835\udc5d 2\ud835\udf07+2\ud835\udf00\ud835\udf07\u2264\ud835\udf07, which implies by (10) that \ud835\udc5c\ud835\udc56 \u2264\ud835\udc65 \ud835\udc56 .(14) We now subdivide Case2into two more cases: (2a)\ud835\udc66\ud835\udc56 <\ud835\udc5d 2\ud835\udf07\u2212\ud835\udf00\ud835\udf07 and (2b)\ud835\udc66\ud835\udc56 \u2265\ud835\udc5d 2\ud835\udf07\u2212\ud835\udf00\ud835\udf07 . In Case2\ud835\udc4e, the fact that\ud835\udc66\ud835\udc56 <\ud835\udc5d 2\ud835\udf07\u2212\ud835\udf00\ud835\udf07 causes all balls of Type3to be added to\ud835\udc39 \u2032 and therefore removed from the bin, i.e.\ud835\udc65\ud835\udc56 = 0(recall that \ud835\udc39 \u2032 contains every Type 3 ball in a Type 2 overloaded bin). Thus, by(14), \ud835\udc5c\ud835\udc56 = 0, completing the proof in this case. In Case2\ud835\udc4f, the fact that\ud835\udc66\ud835\udc56 \u2265\ud835\udc5d 2\ud835\udf07\u2212\ud835\udf00\ud835\udf07 combined with(11) implies\ud835\udc66\ud835\udc56 \u2265\ud835\udc65 \ud835\udc56, which combined with(14) shows \ud835\udc66\ud835\udc56 \u2265\ud835\udc5c \ud835\udc56. This means that at least\ud835\udc5c\ud835\udc56 Type2balls from bin \ud835\udc56 are removed from the bin,", "source": "loadbalancing.pdf"}, {"text": "fact that\ud835\udc66\ud835\udc56 \u2265\ud835\udc5d 2\ud835\udf07\u2212\ud835\udf00\ud835\udf07 combined with(11) implies\ud835\udc66\ud835\udc56 \u2265\ud835\udc65 \ud835\udc56, which combined with(14) shows \ud835\udc66\ud835\udc56 \u2265\ud835\udc5c \ud835\udc56. This means that at least\ud835\udc5c\ud835\udc56 Type2balls from bin \ud835\udc56 are removed from the bin, completing the proof. \u25a1 Using Claim 5.4, we can now argue that each bin\ud835\udc56contains at most\ud835\udf07non-\ud835\udc39 \u2032 balls at the end of Phase2. Since, for each\ud835\udc58\u2208 { 1, 2} and \ud835\udc57\u2208 { 1, 2, 3}, \ud835\udc39 \u2032 contains all Type \ud835\udc57 balls \ud835\udc65 for which \u210e\ud835\udc58 (\ud835\udc65) is a bin that is Type \ud835\udc57 overloaded from\u210e\ud835\udc58, the number of non-\ud835\udc39 \u2032 balls of Types 3 and 2 that are in bin\ud835\udc56 after the second phase (or at any point during the swapping process) can be at most 2(\ud835\udc5d 3 +\ud835\udf00)\ud835\udf07+2(\ud835\udc5d", "source": "loadbalancing.pdf"}, {"text": "\u2032 balls of Types 3 and 2 that are in bin\ud835\udc56 after the second phase (or at any point during the swapping process) can be at most 2(\ud835\udc5d 3 +\ud835\udf00)\ud835\udf07+2(\ud835\udc5d 2 +\ud835\udf00)\ud835\udf07<\ud835\udf07. Therefore, the only way for bin\ud835\udc56 to have more than\ud835\udf07 non-\ud835\udc39 \u2032 balls at the end of the second phase is if its Type 1balls (all of which are using hash\u210e1) are not in\ud835\udc39 \u2032. We will therefore take as given for the rest of the proof that the Type1balls in bin\ud835\udc56are not in\ud835\udc39 \u2032, meaning that\ud835\udc67\ud835\udc56 is just the number of Type 1 ball in bin\ud835\udc56. Let \ud835\udc64 \ud835\udc56 be the number of non-\ud835\udc39 \u2032 Type 2 balls\ud835\udc65 satisfying \u210e2(\ud835\udc65)=\ud835\udc56 . Since \ud835\udc39 \u2032 contains all Type", "source": "loadbalancing.pdf"}, {"text": "just the number of Type 1 ball in bin\ud835\udc56. Let \ud835\udc64 \ud835\udc56 be the number of non-\ud835\udc39 \u2032 Type 2 balls\ud835\udc65 satisfying \u210e2(\ud835\udc65)=\ud835\udc56 . Since \ud835\udc39 \u2032 contains all Type 2 balls\ud835\udc65 for which bin\u210e2(\ud835\udc65)is Type 2 overloaded from\u210e 2, we know that \ud835\udc64 \ud835\udc56 \u2264 (\ud835\udc5d 2 +\ud835\udf00)\ud835\udf07. Since \ud835\udc39 \u2032 contains all Type 2 balls that hash to any Type 1 underloaded bins, the only way for\ud835\udc64 \ud835\udc56 to be non-zero is if \ud835\udc67\ud835\udc56 \u2265 (\ud835\udc5d 1 \u2212\ud835\udf00)\ud835\udf07. Therefore, \ud835\udc67\ud835\udc56 \u2265\ud835\udc64 \ud835\udc56. This implies that at least\ud835\udc64 \ud835\udc56 Type 1 balls are removed from bin\ud835\udc56 during the second phase. Since the number of non-\ud835\udc39 \u2032 balls at the beginning of the phase was at most\ud835\udf07, since \ud835\udc64 \ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "Type 1 balls are removed from bin\ud835\udc56 during the second phase. Since the number of non-\ud835\udc39 \u2032 balls at the beginning of the phase was at most\ud835\udf07, since \ud835\udc64 \ud835\udc56 non-\ud835\udc39 \u2032 balls were added to the bin, and since at least\ud835\udc64 \ud835\udc56 non-\ud835\udc39 \u2032 balls were removed from the bin, we can conclude that the number of non-\ud835\udc39\u2032 balls in the bin remains at most\ud835\udf07at the end of the phase.\u25a1 22 Next, we state two lemmas to analyze the various graphs to which we apply the Canonical Orientation Procedure \u2013 roughly speaking, these lemmas tell us that the graphs at hand are \u201cvalid candidates\u201d for the procedure. (The same lemmas will also be useful later on when we are", "source": "loadbalancing.pdf"}, {"text": "\u2013 roughly speaking, these lemmas tell us that the graphs at hand are \u201cvalid candidates\u201d for the procedure. (The same lemmas will also be useful later on when we are bounding recourse.) Both lemmas require nontrivial machinery to prove, and we defer this machinery to Section 5.5. For the next lemma, recall that for a graph\ud835\udc3a(\ud835\udc35) , we define the random subgraphs\ud835\udc3a1(\ud835\udc35), . . . , \ud835\udc3a\ud835\udc5d \u22121 (\ud835\udc35) which partition the edges in\ud835\udc3a(\ud835\udc35)(each edge in\ud835\udc3a(\ud835\udc35)is assigned to one subgraph at random). Lemma 5.5.For each \ud835\udc57\u2208 [\ud835\udc5d \u22121], we have with high probability in \ud835\udc5b that the graph \ud835\udc3a \ud835\udc57 (\ud835\udc432(\ud835\udc46)) satisfies the following properties: \u2022each connected component contains at most\ud835\udc42(1)cycles; \u2022the expected size of the connected component containing a", "source": "loadbalancing.pdf"}, {"text": "with high probability in \ud835\udc5b that the graph \ud835\udc3a \ud835\udc57 (\ud835\udc432(\ud835\udc46)) satisfies the following properties: \u2022each connected component contains at most\ud835\udc42(1)cycles; \u2022the expected size of the connected component containing a given vertex\ud835\udc63\u2208 [\ud835\udc5b]is\ud835\udc42(1). Proof. Since |\ud835\udc432(\ud835\udc46)|=|\ud835\udc35(\ud835\udc46)| is \ud835\udc42(\ud835\udc5b) with high probability, we have by Lemma 5.2 that, with high probability, \ud835\udc3a is a graph with\ud835\udc42(\ud835\udc5b) independent and uniformly random edges from[\ud835\udc5b] \u00d7 [\ud835\udc5b] . The result therefore follows from Lemma 5.17. \u25a1 Lemma 5.6.For each \ud835\udc57\u2208 [\ud835\udc5d \u22121], for each graph \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) ) (where \ud835\udc4e, \ud835\udc4f\u2208 [ 3], \ud835\udc4e>\ud835\udc4f , and \ud835\udc50, \ud835\udc51\u2208 [ 2]), and for each graph \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) ) (where \ud835\udc4e\u2208 [ 3] and \ud835\udc50\u2208 [ 2]), we have with high probability", "source": "loadbalancing.pdf"}, {"text": "3], \ud835\udc4e>\ud835\udc4f , and \ud835\udc50, \ud835\udc51\u2208 [ 2]), and for each graph \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) ) (where \ud835\udc4e\u2208 [ 3] and \ud835\udc50\u2208 [ 2]), we have with high probability in \ud835\udc5b that the graph satisfies the following properties: \u2022each connected component contains at most\ud835\udc42(1)cycles; \u2022the expected size of the connected component containing a given vertex\ud835\udc63\u2208 [\ud835\udc5b]is\ud835\udc42(1); \u2022for any ball\ud835\udc60\u2208\ud835\udc46, the expected size of the connected component containing vertex\u210e 1(\ud835\udc60)is also\ud835\udc42(1). Proof. Let \ud835\udc5a1, \ud835\udc5a2, \ud835\udc5a3 be the number of Type 1, Type 2, and Type 3 balls in the system, respectively. First note that, by Chernoff bounds, we have with high probability in\ud835\udc5b that\ud835\udc5a1 =( 1\u00b1\ud835\udc5c( 1))\ud835\udc5d 1\ud835\udc5a, that\ud835\udc5a2 =( 1\u00b1\ud835\udc5c( 1))\ud835\udc5d 2\ud835\udc5a, and that\ud835\udc5a 3 =(1\u00b1\ud835\udc5c(1))\ud835\udc5d 3\ud835\udc5a. It", "source": "loadbalancing.pdf"}, {"text": "system, respectively. First note that, by Chernoff bounds, we have with high probability in\ud835\udc5b that\ud835\udc5a1 =( 1\u00b1\ud835\udc5c( 1))\ud835\udc5d 1\ud835\udc5a, that\ud835\udc5a2 =( 1\u00b1\ud835\udc5c( 1))\ud835\udc5d 2\ud835\udc5a, and that\ud835\udc5a 3 =(1\u00b1\ud835\udc5c(1))\ud835\udc5d 3\ud835\udc5a. It follows that each of the graphs in the lemma fits the assumptions of one of Lemmas 5.18 (for the \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) s) or 5.19 (for the\ud835\udc4c (\ud835\udc4e,\ud835\udc50) s). The claimed result therefore follows from those lemmas (for the first two claimed bullet points) and Corollary 5.20 (for the third claimed bullet point). \u25a1 Putting the previous lemmas together, we can now bound the overload of our algorithm: Proposition 5.7.With high probability in\ud835\udc5b, the final state produced by our algorithm has overload\ud835\udc42(1). Proof. By Lemmas 5.5 and 5.6, with high probability,", "source": "loadbalancing.pdf"}, {"text": "now bound the overload of our algorithm: Proposition 5.7.With high probability in\ud835\udc5b, the final state produced by our algorithm has overload\ud835\udc42(1). Proof. By Lemmas 5.5 and 5.6, with high probability, for each \ud835\udc57\u2208 [\ud835\udc5d \u22121], the graphs \ud835\udc3a \ud835\udc57 (\ud835\udc35 \u2032), the graphs \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) ), and the graphs\ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) ) all have the property that each connected component contains\ud835\udc42( 1) cycles. It follows that our applications of the ECO Procedure result in an orientation in which each bin has at most \ud835\udc42( 1) balls from \ud835\udc39 \u2032 \u222a\ud835\udc35 \u2032. By Lemma 5.3, each bin contains at most\ud835\udc5a/\ud835\udc5b balls that are neither in\ud835\udc39 \u2032 nor in\ud835\udc35 \u2032. Thus, we have with high probability that each bin", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc39 \u2032 \u222a\ud835\udc35 \u2032. By Lemma 5.3, each bin contains at most\ud835\udc5a/\ud835\udc5b balls that are neither in\ud835\udc39 \u2032 nor in\ud835\udc35 \u2032. Thus, we have with high probability that each bin contains a total of\ud835\udc5a/\ud835\udc5b+\ud835\udc42(1)balls.\u25a1 23 5.4 Bounding Recourse To complete the proof of Theorem 5.1, we must also prove a bound on recourse. We begin by analyzing the recourse within a given graph\ud835\udc3a\ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) ). Lemma 5.8.Let \ud835\udc57\u2208 [\ud835\udc5d \u22121] and consider one of the sets \ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46) . Let \ud835\udc46 and \ud835\udc46 \u2032 be neighboring sets of \ud835\udc5a balls, with\ud835\udc46=\ud835\udc46 \u2032 \\ {\ud835\udc651} \u222a {\ud835\udc65 2}. Then, E[| \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46))\u25b3 \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46 \u2032))|] \u2264\ud835\udc42(1). That is, the expected number of edges", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc5a balls, with\ud835\udc46=\ud835\udc46 \u2032 \\ {\ud835\udc651} \u222a {\ud835\udc65 2}. Then, E[| \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46))\u25b3 \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46 \u2032))|] \u2264\ud835\udc42(1). That is, the expected number of edges on which orientations\ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46)) and\ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46 \u2032)) differ is\ud835\udc42( 1). Proof. The orientations\ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46)) and \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46 \u2032)) differ at most on the edges in the following two connected components: \u2022the component in \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46))containing\ud835\udc65 1; \u2022and the component in \ud835\udc3a \ud835\udc57 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46 \u2032))containing\ud835\udc65 2. By Lemma 5.6, these components each have expected size\ud835\udc42(1).\u25a1 By the same argument, we can analyze the recourse within a given graph\ud835\udc3a\ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) ). Lemma 5.9.Let \ud835\udc57\u2208", "source": "loadbalancing.pdf"}, {"text": "\u2032))containing\ud835\udc65 2. By Lemma 5.6, these components each have expected size\ud835\udc42(1).\u25a1 By the same argument, we can analyze the recourse within a given graph\ud835\udc3a\ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) ). Lemma 5.9.Let \ud835\udc57\u2208 [\ud835\udc5d \u22121] and consider one of the sets \ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46) . Let \ud835\udc46 and \ud835\udc46 \u2032 be neighboring sets of \ud835\udc5a balls, with\ud835\udc46=\ud835\udc46 \u2032 \\ {\ud835\udc651} \u222a {\ud835\udc65 2}. Then, E[| \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46))\u25b3 \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032))|] \u2264\ud835\udc42(1). That is, the expected number of edges on which orientations \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46))and \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032))differ is\ud835\udc42(1). Proof. The orientations \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46)) and \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032)) differ at most on the edges in the following", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032))differ is\ud835\udc42(1). Proof. The orientations \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46)) and \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032)) differ at most on the edges in the following two connected components: \u2022the component in \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46))containing\ud835\udc65 1; \u2022and the component in \ud835\udc3a \ud835\udc57 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032))containing\ud835\udc65 2. By Lemma 5.6, these components each have expected size\ud835\udc42(1).\u25a1 Finally, by a slightly more intricate argument, we can bound the expected recourse from changes to the set\ud835\udc43 1(\ud835\udc46)and from changes to the orientation \ud835\udc3a(\ud835\udc35 \u2032 (\ud835\udc46)). Lemma 5.10.Consider two neighboring sets\ud835\udc46and\ud835\udc46 \u2032 of\ud835\udc5aballs. Then, E[|\ud835\udc43 1(\ud835\udc46)\u25b3\ud835\udc43 1(\ud835\udc46 \u2032)|] \u2264\ud835\udc42(Recourse(A)), and the expected number of edges on which the orientations \ud835\udc3a1(\ud835\udc432(\ud835\udc46))and \ud835\udc3a2(\ud835\udc432(\ud835\udc46 \u2032))disagree is \ud835\udc42(Recourse(A)). Proof.Let us imagine", "source": "loadbalancing.pdf"}, {"text": "5.10.Consider two neighboring sets\ud835\udc46and\ud835\udc46 \u2032 of\ud835\udc5aballs. Then, E[|\ud835\udc43 1(\ud835\udc46)\u25b3\ud835\udc43 1(\ud835\udc46 \u2032)|] \u2264\ud835\udc42(Recourse(A)), and the expected number of edges on which the orientations \ud835\udc3a1(\ud835\udc432(\ud835\udc46))and \ud835\udc3a2(\ud835\udc432(\ud835\udc46 \u2032))disagree is \ud835\udc42(Recourse(A)). Proof.Let us imagine thatA (\ud835\udc46)is determined by an adversary as follows: 1. The adversary selects sets\ud835\udc461, \ud835\udc462 of Type 1 and Type 2 balls, and chooses\u210e1(\ud835\udc60) for each ball\ud835\udc60\u2208\ud835\udc46 1\u222a\ud835\udc462. Each ball \ud835\udc60\u2208\ud835\udc46 1 \u222a\ud835\udc46 2 is then placed into bin\u210e1(\ud835\udc60). Next, the adversary selects numbers\ud835\udc65\ud835\udc56, \ud835\udc56\u2208 [\ud835\udc5b] , where each \ud835\udc65\ud835\udc56 represents the number of Type 3 balls that go in bin\ud835\udc56. Call \ud835\udc461, \ud835\udc462, \u210e1(\ud835\udc461), \u210e1(\ud835\udc462),{\ud835\udc65 \ud835\udc56 } thecore input \ud835\udc36. 24 2. Once the core input is determined, the hashes\u210e2(\ud835\udc461) and \u210e2(\ud835\udc462) are determined, and the random tapes used in", "source": "loadbalancing.pdf"}, {"text": "Call \ud835\udc461, \ud835\udc462, \u210e1(\ud835\udc461), \u210e1(\ud835\udc462),{\ud835\udc65 \ud835\udc56 } thecore input \ud835\udc36. 24 2. Once the core input is determined, the hashes\u210e2(\ud835\udc461) and \u210e2(\ud835\udc462) are determined, and the random tapes used in the Two-Phase Swapping Procedure are generated. We refer to the randomness determined in this step as theShuffle Randomness. 3. Finally, the Two-Phase Swapping Procedure is performed, generating\ud835\udc431, \ud835\udc432. We will now think of these as being a function of the core input\ud835\udc36, rather than a set\ud835\udc46of balls. By Lemma 5.2, if we fix any core input\ud835\udc36, the remaining randomness (i.e., the shuffle randomness) is enough to ensure that the hashes of the balls in\ud835\udc432(\ud835\udc36)are independent and uniformly random. To complete the proof, we will consider what happens if two core", "source": "loadbalancing.pdf"}, {"text": "the shuffle randomness) is enough to ensure that the hashes of the balls in\ud835\udc432(\ud835\udc36)are independent and uniformly random. To complete the proof, we will consider what happens if two core inputs\ud835\udc36 and \ud835\udc36 \u2032 differ from each other by only a small amount. Say that two core inputs\ud835\udc36 and \ud835\udc36 \u2032 differ by aunit changeif we can get from \ud835\udc36 to \ud835\udc36 \u2032 (or possibly from\ud835\udc36 \u2032 to\ud835\udc36) by one of the following three basic changes: \u2022Incrementing/decrementing\ud835\udc65 \ud835\udc56 for some bin\ud835\udc56. \u2022Adding/removing some ball\ud835\udc4fto\ud835\udc46 2 with\u210e 1(\ud835\udc4f)=\ud835\udc56for some\ud835\udc56. \u2022Adding/removing some ball\ud835\udc4fto\ud835\udc46 1 with\u210e 1(\ud835\udc4f)=\ud835\udc56for some\ud835\udc56. To complete the proof, it suffices to show that, when\ud835\udc36and\ud835\udc36\u2032 differ by a unit change, E[|\ud835\udc43 2(\ud835\udc36)\u25b3\ud835\udc43 2(\ud835\udc36\u2032)|] \u2264\ud835\udc42(1) and the expected number of edges", "source": "loadbalancing.pdf"}, {"text": "ball\ud835\udc4fto\ud835\udc46 1 with\u210e 1(\ud835\udc4f)=\ud835\udc56for some\ud835\udc56. To complete the proof, it suffices to show that, when\ud835\udc36and\ud835\udc36\u2032 differ by a unit change, E[|\ud835\udc43 2(\ud835\udc36)\u25b3\ud835\udc43 2(\ud835\udc36\u2032)|] \u2264\ud835\udc42(1) and the expected number of edges on which the orientations\ud835\udc3a1(\ud835\udc432(\ud835\udc36))and \ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032))disagree is\ud835\udc42(1). We now prove this for each type of unit change, where the change takes us from\ud835\udc36to\ud835\udc36\u2032: \u2022 Case 1: Some \ud835\udc65\ud835\udc56 is incremented/decremented.Without loss of generality, \ud835\udc65\ud835\udc56 is incremented. If bin \ud835\udc56 has \u2264\u210e balls in\ud835\udc36 \u2032, then \ud835\udc431(\ud835\udc36)=\ud835\udc43 1(\ud835\udc36\u2032) and \ud835\udc432(\ud835\udc36)=\ud835\udc43 2(\ud835\udc36\u2032). Otherwise, \ud835\udc431(\ud835\udc36\u2032)=\ud835\udc43 1(\ud835\udc36) \u222a {\ud835\udc65 1} for some ball \ud835\udc651, where \u210e1(\ud835\udc651)=\ud835\udc56 and where \u210e2(\ud835\udc651) is determined by the Shuffle Randomness; and \ud835\udc432(\ud835\udc36\u2032)=\ud835\udc43 2(\ud835\udc36) \u222a {\ud835\udc65 2} for some ball\ud835\udc652 where \u210e1(\ud835\udc652)=\u210e 2(\ud835\udc651) and where\u210e2(\ud835\udc652) is determined by the Shuffle Randomness.", "source": "loadbalancing.pdf"}, {"text": "\u210e1(\ud835\udc651)=\ud835\udc56 and where \u210e2(\ud835\udc651) is determined by the Shuffle Randomness; and \ud835\udc432(\ud835\udc36\u2032)=\ud835\udc43 2(\ud835\udc36) \u222a {\ud835\udc65 2} for some ball\ud835\udc652 where \u210e1(\ud835\udc652)=\u210e 2(\ud835\udc651) and where\u210e2(\ud835\udc652) is determined by the Shuffle Randomness. Note that the values\ud835\udc57 :=\u210e 1(\ud835\udc652)=\u210e 2(\ud835\udc651) and \ud835\udc58 :=\u210e 2(\ud835\udc652) are independent of the randomness used to construct graph\ud835\udc3a1(\ud835\udc432(\ud835\udc36)). By Lemma 5.6, the expected size of the connected components in\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) containing nodes \ud835\udc57 and \ud835\udc58 are each \ud835\udc42( 1). It follows that the connected component in\ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) containing edge (\ud835\udc57, \ud835\udc58) is \ud835\udc42( 1). As this component contains all edges on which the orientations\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) and \ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) disagree, the expected number of such edges is\ud835\udc42(1). \u2022 Case 2: Some ball \ud835\udc4f is added/removed to \ud835\udc462 with hash \u210e1(\ud835\udc56) for some \ud835\udc56:Without", "source": "loadbalancing.pdf"}, {"text": "on which the orientations\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) and \ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) disagree, the expected number of such edges is\ud835\udc42(1). \u2022 Case 2: Some ball \ud835\udc4f is added/removed to \ud835\udc462 with hash \u210e1(\ud835\udc56) for some \ud835\udc56:Without loss of generality the ball \ud835\udc4f is added to\ud835\udc36 (rather than removed). If bin\u210e1(\ud835\udc4f) has \u2264\u210e balls in\ud835\udc36 \u2032, then \ud835\udc431(\ud835\udc36)=\ud835\udc43 1(\ud835\udc36\u2032) and \ud835\udc432(\ud835\udc36)=\ud835\udc43 2(\ud835\udc36\u2032). Otherwise, \ud835\udc431(\ud835\udc36\u2032)=\ud835\udc43 1(\ud835\udc36) \u222a {\ud835\udc65 1} for some ball\ud835\udc651 (possibly \ud835\udc4f), where \u210e1(\ud835\udc651)=\u210e 1(\ud835\udc4f) and where\u210e2(\ud835\udc651) is determined by the Shuffle Randomness; and\ud835\udc432(\ud835\udc36\u2032)=\ud835\udc43 2(\ud835\udc36) \u222a {\ud835\udc65 2} for some ball \ud835\udc652 where \u210e1(\ud835\udc652)=\u210e 2(\ud835\udc651) and where\u210e2(\ud835\udc652) is determined by the Shuffle Randomness. Note that the values \ud835\udc57 :=\u210e 1(\ud835\udc652)=\u210e 2(\ud835\udc651) and \ud835\udc58 :=\u210e 2(\ud835\udc652) are independent of the randomness used to construct graph \ud835\udc3a1(\ud835\udc432(\ud835\udc36)). By", "source": "loadbalancing.pdf"}, {"text": "where\u210e2(\ud835\udc652) is determined by the Shuffle Randomness. Note that the values \ud835\udc57 :=\u210e 1(\ud835\udc652)=\u210e 2(\ud835\udc651) and \ud835\udc58 :=\u210e 2(\ud835\udc652) are independent of the randomness used to construct graph \ud835\udc3a1(\ud835\udc432(\ud835\udc36)). By Lemma 5.6, the expected size of the connected components in\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) containing nodes \ud835\udc57 and \ud835\udc58 are each \ud835\udc42( 1). It follows that the connected component in\ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) containing edge (\ud835\udc57, \ud835\udc58) is \ud835\udc42( 1). As this component contains all edges on which the orientations\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) and \ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) disagree, the expected number of such edges is\ud835\udc42(1). 25 \u2022 Case 3: Some ball \ud835\udc4f is added to \ud835\udc461 with \u210e1(\ud835\udc4f) :=\ud835\udc56 for some \ud835\udc56:To simplify the analysis of this option, let us decompose this modifiction into two sub-modifications. Submodification 1 increments\ud835\udc65\ud835\udc56. Then, Submodification 2", "source": "loadbalancing.pdf"}, {"text": "is added to \ud835\udc461 with \u210e1(\ud835\udc4f) :=\ud835\udc56 for some \ud835\udc56:To simplify the analysis of this option, let us decompose this modifiction into two sub-modifications. Submodification 1 increments\ud835\udc65\ud835\udc56. Then, Submodification 2 inserts the ball \ud835\udc4f and decrements \ud835\udc65\ud835\udc56. Since Submodification 1 has already been analyzed in Case 1, we can focus here on analyzing Submodification 2. That is, we redefine\ud835\udc36 to be the core input before Submodification 2, and\ud835\udc36\u2032 to be the core input after. Because Submodification 2 does not change the number of balls in any given bin, we have\ud835\udc431(\ud835\udc36)=\ud835\udc43 1(\ud835\udc36\u2032). Moreover, either\ud835\udc432(\ud835\udc36)=\ud835\udc43 2(\ud835\udc36\u2032), or\ud835\udc432(\ud835\udc36\u2032)=\ud835\udc43 2(\ud835\udc36) \\{\ud835\udc4f \u2032}\u222a{\ud835\udc4f} for some ball\ud835\udc4f\u2032 \u2208\ud835\udc46 1 such that\u210e1(\ud835\udc4f)=\ud835\udc56 . By Lemma 5.6, the expected size of the connected component in\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) containing vertex \ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "1(\ud835\udc36\u2032). Moreover, either\ud835\udc432(\ud835\udc36)=\ud835\udc43 2(\ud835\udc36\u2032), or\ud835\udc432(\ud835\udc36\u2032)=\ud835\udc43 2(\ud835\udc36) \\{\ud835\udc4f \u2032}\u222a{\ud835\udc4f} for some ball\ud835\udc4f\u2032 \u2208\ud835\udc46 1 such that\u210e1(\ud835\udc4f)=\ud835\udc56 . By Lemma 5.6, the expected size of the connected component in\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) containing vertex \ud835\udc56 is \ud835\udc42( 1), and the expected size of the connected component\ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) containing vertex\ud835\udc56 is also \ud835\udc42( 1). Since every edge on which the orientations\ud835\udc3a1(\ud835\udc432(\ud835\udc36)) and \ud835\udc3a1(\ud835\udc432(\ud835\udc36\u2032)) disagree is in at least one of those two components, the expected number of disagreements between the two orientations is\ud835\udc42(1). \u25a1 Putting the pieces together, we can establish our desired bound on recourse: Proposition 5.11.The final algorithm has expected recourse\ud835\udc42(Recourse(A)). Proof. Consider two neighboring sets\ud835\udc46 and \ud835\udc46 \u2032. The expected recourse of our algorithm is at most the expected recourse ofRecourse(A)from runningA, plus the", "source": "loadbalancing.pdf"}, {"text": "5.11.The final algorithm has expected recourse\ud835\udc42(Recourse(A)). Proof. Consider two neighboring sets\ud835\udc46 and \ud835\udc46 \u2032. The expected recourse of our algorithm is at most the expected recourse ofRecourse(A)from runningA, plus the following terms: 1.E[|\ud835\udc43 1(\ud835\udc46)\u25b3\ud835\udc43 1(\ud835\udc46 \u2032)|]; 2.E[| \ud835\udc3a\ud835\udc56 (\ud835\udc432(\ud835\udc46))\u25b3 \ud835\udc3a\ud835\udc56 (\ud835\udc432(\ud835\udc46 \u2032))|]for each\ud835\udc56\u2208 [\ud835\udc5d \u22121]; 3.E[| \ud835\udc3a\ud835\udc56 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46))\u25b3 \ud835\udc3a\ud835\udc56 (\ud835\udc4b (\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51) (\ud835\udc46 \u2032))|] for each \ud835\udc56\u2208 [\ud835\udc5d \u22121] and each \ud835\udc4e, \ud835\udc4f\u2208 [ 3] with \ud835\udc4e>\ud835\udc4f and \ud835\udc50, \ud835\udc51\u2208 [2]; 4.E[| \ud835\udc3a\ud835\udc56 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46))\u25b3 \ud835\udc3a\ud835\udc56 (\ud835\udc4c (\ud835\udc4e,\ud835\udc50) (\ud835\udc46 \u2032))|]for each\ud835\udc56\u2208 [\ud835\udc5d \u22121]and\ud835\udc4e\u2208 [3]and\ud835\udc50\u2208 [2]. By Lemmas 5.8, 5.9, and 5.10, along with the fact that \ud835\udc5d \u22121 =\ud835\udc42( 1), we have that these terms sum to \ud835\udc42(Recourse(A)), as desired.\u25a1 Combining our analyses of overload (Proposition 5.7) and", "source": "loadbalancing.pdf"}, {"text": "5.9, and 5.10, along with the fact that \ud835\udc5d \u22121 =\ud835\udc42( 1), we have that these terms sum to \ud835\udc42(Recourse(A)), as desired.\u25a1 Combining our analyses of overload (Proposition 5.7) and recourse (Proposition 5.11), the proof of Theorem 5.1 is complete. 5.5 Machinery for Analyzing Graphs Finally, in this section, we develop the machinery needed to analyze the graphs constructed in the previous section. Specifically, the results that we develop in this section are what allow us to prove Lemmas 5.5 and 5.6 in Section 5.3. We begin with the following lemma: Lemma 5.12.Let \ud835\udc50 be a sufficiently small positive constant, and let0 \u2264\ud835\udc58\u2264\ud835\udc42( 1). Consider a directed graph \ud835\udc3a on \ud835\udc5b vertices, where each vertex \ud835\udc63\u2208 [\ud835\udc5b] has \ud835\udc4b\ud835\udc63 outgoing", "source": "loadbalancing.pdf"}, {"text": "lemma: Lemma 5.12.Let \ud835\udc50 be a sufficiently small positive constant, and let0 \u2264\ud835\udc58\u2264\ud835\udc42( 1). Consider a directed graph \ud835\udc3a on \ud835\udc5b vertices, where each vertex \ud835\udc63\u2208 [\ud835\udc5b] has \ud835\udc4b\ud835\udc63 outgoing edges, where each \ud835\udc4b\ud835\udc63 independently satisfies Pr[\ud835\udc4b \ud835\udc63 \u2265\ud835\udc57] \u2264\ud835\udc50 \ud835\udc57 for all \ud835\udc57\u2265 0, and where each edge (\ud835\udc63, \ud835\udc62) has an independent and uniformly random endpoint \ud835\udc62\u2208 [\ud835\udc5b] . Consider the (undirected) connected components of\ud835\udc3a. For any given vertex\ud835\udc63 and for \ud835\udc57\u2265 1and \ud835\udc58\u2265 0, the probability that \ud835\udc63 is part of a connected component \ud835\udc36 of size |\ud835\udc36|=\ud835\udc57 containing at least \ud835\udc57\u2212 1 +\ud835\udc58 edges is at most \ud835\udc52 \u2212\u03a9(\ud835\udc57) /\ud835\udc5b\ud835\udc58/2 +1/\ud835\udc5b \ud835\udf14(1) . 26 Proof. Since \ud835\udc36 must contain \ud835\udc63, there are\u0000\ud835\udc5b\u22121 \ud835\udc57\u22121 \u0001 \u2264\ud835\udc5b \ud835\udc57\u22121", "source": "loadbalancing.pdf"}, {"text": "|\ud835\udc36|=\ud835\udc57 containing at least \ud835\udc57\u2212 1 +\ud835\udc58 edges is at most \ud835\udc52 \u2212\u03a9(\ud835\udc57) /\ud835\udc5b\ud835\udc58/2 +1/\ud835\udc5b \ud835\udf14(1) . 26 Proof. Since \ud835\udc36 must contain \ud835\udc63, there are\u0000\ud835\udc5b\u22121 \ud835\udc57\u22121 \u0001 \u2264\ud835\udc5b \ud835\udc57\u22121 \ud835\udc52 \ud835\udc57\u22121 /\ud835\udc57 \ud835\udc57\u22121 options for\ud835\udc36. For a given choice of\ud835\udc36, let \ud835\udc4c be the random variable denoting the sum of the out-degrees of the vertices in\ud835\udc36. In order for\ud835\udc36 to have \ud835\udc57\u2212 1 +\ud835\udc58 edges, we would need\ud835\udc4c=\ud835\udc57\u2212 1 +\ud835\udc58 . And, in order for\ud835\udc36 to be a connected component, we would need all those edges to have right endpoints in\ud835\udc36, an event that happens with probability at most (|\ud835\udc36|/\ud835\udc5b) \ud835\udc57\u22121+\ud835\udc58 =(\ud835\udc57/\ud835\udc5b) \ud835\udc57\u22121+\ud835\udc58 . Thus, the probability of \ud835\udc63 is part of a connected component\ud835\udc36 of size |\ud835\udc36|=\ud835\udc57 containing", "source": "loadbalancing.pdf"}, {"text": "endpoints in\ud835\udc36, an event that happens with probability at most (|\ud835\udc36|/\ud835\udc5b) \ud835\udc57\u22121+\ud835\udc58 =(\ud835\udc57/\ud835\udc5b) \ud835\udc57\u22121+\ud835\udc58 . Thus, the probability of \ud835\udc63 is part of a connected component\ud835\udc36 of size |\ud835\udc36|=\ud835\udc57 containing at least \ud835\udc57\u2212 1 +\ud835\udc58 edges is at most \ud835\udc5b \ud835\udc57\u22121 \ud835\udc52 \ud835\udc57\u22121 /\ud835\udc57 \ud835\udc57\u22121 \u00b7Pr[\ud835\udc4c=\ud835\udc57\u2212\ud835\udc58+1] \u00b7 (\ud835\udc57/\ud835\udc5b) \ud835\udc57\u22121+\ud835\udc58 . By a Chernoff bound for sums of independent geometric random variables7, we havePr[\ud835\udc4c=\ud835\udc57\u2212 1 +\ud835\udc58] \u2264 1/8\ud835\udc57\u22121+\ud835\udc58 . The entire expression is therefore at most \ud835\udc5b \ud835\udc57\u22121 \ud835\udc52 \ud835\udc57\u22121 /\ud835\udc57 \ud835\udc57\u22121 \u00b78 \u2212 (\ud835\udc57\u22121+\ud835\udc58) \u00b7 (\ud835\udc57/\ud835\udc5b) \ud835\udc57\u22121+\ud835\udc58 =\ud835\udc42(\ud835\udc52 \ud835\udc57\u22121 8\u2212\ud835\udc57+\ud835\udc58 (\ud835\udc57/\ud835\udc5b) \ud835\udc58 ). \ud835\udc52 \u2212\u03a9(\ud835\udc57) /\ud835\udc5b\ud835\udc58/2 +1/\ud835\udc5b \ud835\udf14(1) . \u25a1 As an immediate corollary, we get the following: Corollary 5.13.Let \ud835\udc50 be a sufficiently small positive constant, and let0 \u2264\ud835\udc58\u2264\ud835\udc42(", "source": "loadbalancing.pdf"}, {"text": "(\ud835\udc57/\ud835\udc5b) \ud835\udc58 ). \ud835\udc52 \u2212\u03a9(\ud835\udc57) /\ud835\udc5b\ud835\udc58/2 +1/\ud835\udc5b \ud835\udf14(1) . \u25a1 As an immediate corollary, we get the following: Corollary 5.13.Let \ud835\udc50 be a sufficiently small positive constant, and let0 \u2264\ud835\udc58\u2264\ud835\udc42( 1). Consider a directed graph \ud835\udc3a on \ud835\udc5b vertices, where each vertex \ud835\udc63\u2208 [\ud835\udc5b] has \ud835\udc4b\ud835\udc63 outgoing edges, where each \ud835\udc4b\ud835\udc63 independently satisfies Pr[\ud835\udc4b \ud835\udc63 \u2265\ud835\udc57] \u2264\ud835\udc50 \ud835\udc57 for all \ud835\udc57\u2265 0, and where each edge (\ud835\udc63, \ud835\udc62) has an independent and uniformly random endpoint \ud835\udc62\u2208 [\ud835\udc5b]. Finally, let\ud835\udc3a \u2032 be the undirected version of the graph. With high probability in \ud835\udc5b, every connected component in \ud835\udc3a \u2032 contains at most \ud835\udc42( 1) cycles. Moreover, for a given vertex\ud835\udc56, the expected size of the component containing\ud835\udc56is\ud835\udc42(1)edges. We will also need", "source": "loadbalancing.pdf"}, {"text": "in \ud835\udc5b, every connected component in \ud835\udc3a \u2032 contains at most \ud835\udc42( 1) cycles. Moreover, for a given vertex\ud835\udc56, the expected size of the component containing\ud835\udc56is\ud835\udc42(1)edges. We will also need the following helper claim, which analyzes the effect of down-sampling the edges at a given vertex: Lemma 5.14.Let0 <\ud835\udc5d< 1be a parameter. Suppose we have \ud835\udc4b balls, where Pr[\ud835\udc4b\u2265\ud835\udc56] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc56) for all positive \ud835\udc56. Sample each of these balls independently with probability \ud835\udc5d, and let \ud835\udc4c be the number of sampled balls. Then, for all positive\ud835\udc56, Pr[\ud835\udc4c\u2265\ud835\udc56] \u2264\ud835\udc42(1) \u00b7\ud835\udc5d \ud835\udc56/2 . Proof.We can upper boundPr[\ud835\udc4c\u2265\ud835\udc56]by \u2211\ufe01 \ud835\udc57\u2265\ud835\udc56 Pr[\ud835\udc4b=\ud835\udc57] \u2211\ufe01 \ud835\udc46\u2286 [\ud835\udc57],|\ud835\udc46|=\ud835\udc56 Pr[\ud835\udc46sampled] = \u2211\ufe01 \ud835\udc57\u2265\ud835\udc56 Pr[\ud835\udc4b=\ud835\udc57] \u0012\ud835\udc57 \ud835\udc56 \u0013 \ud835\udc5d\ud835\udc56 \u2264 \u2211\ufe01 \ud835\udc57\u2265\ud835\udc56 \ud835\udc52 \u2212\u03a9(\ud835\udc57) \u00b7 (\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56)", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc56/2 . Proof.We can upper boundPr[\ud835\udc4c\u2265\ud835\udc56]by \u2211\ufe01 \ud835\udc57\u2265\ud835\udc56 Pr[\ud835\udc4b=\ud835\udc57] \u2211\ufe01 \ud835\udc46\u2286 [\ud835\udc57],|\ud835\udc46|=\ud835\udc56 Pr[\ud835\udc46sampled] = \u2211\ufe01 \ud835\udc57\u2265\ud835\udc56 Pr[\ud835\udc4b=\ud835\udc57] \u0012\ud835\udc57 \ud835\udc56 \u0013 \ud835\udc5d\ud835\udc56 \u2264 \u2211\ufe01 \ud835\udc57\u2265\ud835\udc56 \ud835\udc52 \u2212\u03a9(\ud835\udc57) \u00b7 (\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56) \u00b7\ud835\udc5d \ud835\udc56 . 7Here, we are using the following bound. Let\ud835\udc4b=\ud835\udc4b 1 + \u00b7 \u00b7 \u00b7 +\ud835\udc4b\ud835\udc57 be a sum of iid geometric random variables, where each\ud835\udc4b\ud835\udc56\u2019s expectation is at most a sufficiently small positive constant. For any\ud835\udc5a\u2265\ud835\udc57/2, we havePr[\ud835\udc4b\u2265\ud835\udc5a] \u22641/8 \ud835\udc5a. 27 We can complete the proof by demonstrating that each summand is at most\ud835\udc52 \u2212\u03a9(\ud835\udc57) \u00b7\ud835\udc50 \ud835\udc56/2, as this would imply that the entire sum is at most\ud835\udc42(1) \u00b7\ud835\udc5d \ud835\udc56/2. We can bound each summand by considering two cases. For\ud835\udc57=\u0398(\ud835\udc56) , the \ud835\udc50\ud835\udc56 term dominates the\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "this would imply that the entire sum is at most\ud835\udc42(1) \u00b7\ud835\udc5d \ud835\udc56/2. We can bound each summand by considering two cases. For\ud835\udc57=\u0398(\ud835\udc56) , the \ud835\udc50\ud835\udc56 term dominates the\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56 term so that\ud835\udc52 \u2212\u03a9(\ud835\udc57) \u00b7 (\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56) \u00b7\ud835\udc50 \ud835\udc56 \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57) \ud835\udc50\ud835\udc56/2. For \ud835\udc57=\ud835\udf14(\ud835\udc56) , the \ud835\udc52 \u2212\u03a9(\ud835\udc57) term dominates the\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56 term so that \ud835\udc52 \u2212\u03a9(\ud835\udc57) \u00b7 (\ud835\udc52\ud835\udc56 (\ud835\udc57/\ud835\udc56) \ud835\udc56) \u00b7\ud835\udc50 \ud835\udc56 \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57) \ud835\udc50\ud835\udc56. Either way, the entire summand is at most\ud835\udc52 \u2212\u03a9(\ud835\udc57) \u00b7\ud835\udc50 \ud835\udc56/2, as desired. \u25a1 Finally, we will want to apply Corollary 5.13 in settings where the vertices of the graph do not quite behave independently. To rectify this issue, we will make use of the following Poissonization lemmas: Lemma 5.15(Poisson Approximation for", "source": "loadbalancing.pdf"}, {"text": "in settings where the vertices of the graph do not quite behave independently. To rectify this issue, we will make use of the following Poissonization lemmas: Lemma 5.15(Poisson Approximation for Expectations [25]).Let \ud835\udc53(\ud835\udc4e 1, . . . , \ud835\udc4e\ud835\udc5b) : (N\u222a { 0})\ud835\udc5b \u2192R be a mono- tonic function (either monotonically increasing or monotonically decreasing). Throw \ud835\udc5b balls (resp. Poisson(\ud835\udc5b) balls) independently and uniformly into\ud835\udc5b bins, and let\ud835\udc4e\ud835\udc56 (resp. \ud835\udc4e\u2032 \ud835\udc56 ) denote the number of balls in the\ud835\udc56-th bin. Then, E[\ud835\udc53(\ud835\udc4e 1, . . . , \ud835\udc4e\ud835\udc5b)] \u22642E[\ud835\udc53(\ud835\udc4e \u2032 1, . . . , \ud835\udc4e\u2032 \ud835\udc5b)]. As a slight abuse of notation, although technically the function\ud835\udc53 in Lemma 5.15 is monotonic in the quantities\ud835\udc4e 1, \ud835\udc4e2, . . .", "source": "loadbalancing.pdf"}, {"text": "1, . . . , \ud835\udc4e\u2032 \ud835\udc5b)]. As a slight abuse of notation, although technically the function\ud835\udc53 in Lemma 5.15 is monotonic in the quantities\ud835\udc4e 1, \ud835\udc4e2, . . . , \ud835\udc4e\ud835\udc5b, we will say as a shorthand that it is monotonic with ball throws. Lemma 5.16(Poisson Approximation for Probabilities [25]).Let \ud835\udc38(\ud835\udc4e 1, . . . , \ud835\udc4e\ud835\udc5b) be an event determined by non-negative integers \ud835\udc4e1, . . . , \ud835\udc4e\ud835\udc5b. Throw \ud835\udc5b balls (resp. Poisson(\ud835\udc5b) balls) independently and uniformly into \ud835\udc5b bins, and let\ud835\udc4e \ud835\udc56 (resp.\ud835\udc4e \u2032 \ud835\udc56 ) denote the number of balls in the\ud835\udc56-th bin. Then, Pr[\ud835\udc38(\ud835\udc4e 1, . . . , \ud835\udc4e\ud835\udc5b)] \u2264\ud835\udc42( \u221a\ud835\udc5b) \u00b7Pr[\ud835\udc38(\ud835\udc4e \u2032 1, . . . , \ud835\udc4e\u2032 \ud835\udc5b)]. It follows that,", "source": "loadbalancing.pdf"}, {"text": "denote the number of balls in the\ud835\udc56-th bin. Then, Pr[\ud835\udc38(\ud835\udc4e 1, . . . , \ud835\udc4e\ud835\udc5b)] \u2264\ud835\udc42( \u221a\ud835\udc5b) \u00b7Pr[\ud835\udc38(\ud835\udc4e \u2032 1, . . . , \ud835\udc4e\u2032 \ud835\udc5b)]. It follows that, if the event\ud835\udc38(\ud835\udc4e \u2032 1, . . . , \ud835\udc4e\u2032 \ud835\udc5b) occurs with high probability in\ud835\udc5b, then the event\ud835\udc38(\ud835\udc4e 1, . . . , \ud835\udc4e\ud835\udc5b) also occurs with high probability in\ud835\udc5b. Finally, with the preceding lemmas in place, we are prepared prove the main results of the section. Each of the following lemmas analyzes one of the random processes that we use to construct random graphs in our algorithm \u2013 in each case, we argue that the resulting random graph is well behaved. Lemma 5.17.Let \ud835\udc51\u2208\ud835\udc42( 1), and let \ud835\udc5d", "source": "loadbalancing.pdf"}, {"text": "we use to construct random graphs in our algorithm \u2013 in each case, we argue that the resulting random graph is well behaved. Lemma 5.17.Let \ud835\udc51\u2208\ud835\udc42( 1), and let \ud835\udc5d be a sufficiently small positive constant relative to\ud835\udc51. Suppose we throw \ud835\udc50\ud835\udc5b balls into \ud835\udc5b bins, independently and uniformly at random, and then we remove each ball from the system independently with probability1\u2212\ud835\udc5d. Let\ud835\udc4d \ud835\udc56 be the number of remaining balls in bin\ud835\udc56. Construct a (multi-)graph \ud835\udc3a on vertices [\ud835\udc5b] by adding for each vertex \ud835\udc56\u2208 [\ud835\udc5b]\ud835\udc4d \ud835\udc56 edges from \ud835\udc56 to independent and uniformly random vertices in[\ud835\udc5b]. Then,\ud835\udc3asatisfies the following two properties: \u2022With high probability in\ud835\udc5b, every connected component in\ud835\udc3acontains\ud835\udc42(1)cycles. \u2022For any given vertex\ud835\udc56, the expected size of the", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc56 to independent and uniformly random vertices in[\ud835\udc5b]. Then,\ud835\udc3asatisfies the following two properties: \u2022With high probability in\ud835\udc5b, every connected component in\ud835\udc3acontains\ud835\udc42(1)cycles. \u2022For any given vertex\ud835\udc56, the expected size of the connected component\ud835\udc36 \ud835\udc56 containing\ud835\udc56is\ud835\udc42(1). Proof. By Poissonization (Lemmas 5.15 and 5.16), we may assume without loss of generality that, rather than \ud835\udc5a balls being thrown, \ud835\udc40\u223cPoisson(\ud835\udc5a) balls are thrown. (Here we use that |\ud835\udc36\ud835\udc56 | monotonically increases with ball throws.) With this modification, the bins are now independent, each independently receiving Poisson(\ud835\udc51) balls. Since the \ud835\udc4d\ud835\udc56s are obtained by sampling each ball with probability\ud835\udc43, it follows that the\ud835\udc4d\ud835\udc56s are independent Poisson(\ud835\udc5d\ud835\udc51) random variables. Supposing that \ud835\udc5d is a sufficiently small positive constant, this implies that\ud835\udc4d \ud835\udc56s independently satisfy Pr[\ud835\udc4d \ud835\udc56", "source": "loadbalancing.pdf"}, {"text": "each ball with probability\ud835\udc43, it follows that the\ud835\udc4d\ud835\udc56s are independent Poisson(\ud835\udc5d\ud835\udc51) random variables. Supposing that \ud835\udc5d is a sufficiently small positive constant, this implies that\ud835\udc4d \ud835\udc56s independently satisfy Pr[\ud835\udc4d \ud835\udc56 \u2265\ud835\udc57] \u2264\ud835\udc50 \ud835\udc57,(15) where \ud835\udc50 is a positive constant determined by \ud835\udc5d that goes to0as \ud835\udc5d goes to0. Applying Corollary 5.13, we obtain the desired properties of the graph\ud835\udc3a.\u25a1 28 Lemma 5.18.Let \u210e\u2265 1and let \ud835\udf00\u2208 ( 0, 1) be a positive constant, and let\ud835\udc5d be a sufficiently small positive constant as a function of \ud835\udf00. Suppose we throw \ud835\udc5a=\u210e\ud835\udc5b balls into \ud835\udc5b bins, independently and uniformly at random. Let \ud835\udc4b\ud835\udc56 denote the number of balls in bin \ud835\udc56. Now suppose that we remove each ball from the system independently", "source": "loadbalancing.pdf"}, {"text": "balls into \ud835\udc5b bins, independently and uniformly at random. Let \ud835\udc4b\ud835\udc56 denote the number of balls in bin \ud835\udc56. Now suppose that we remove each ball from the system independently with probability1\u2212\ud835\udc5d. Let\ud835\udc4c \ud835\udc56 be the number of remaining balls in bin\ud835\udc56, and define \ud835\udc4d\ud835\udc56 = ( 0if\ud835\udc4b \ud835\udc56 \u2264 (1+\ud835\udf00)\u210e \ud835\udc4c\ud835\udc56 otherwise. Finally, construct a (multi-)graph\ud835\udc3a on vertices [\ud835\udc5b] by adding for each vertex\ud835\udc56\u2208 [\ud835\udc5b]\ud835\udc4d \ud835\udc56 edges from\ud835\udc56 to independent and uniformly random vertices in[\ud835\udc5b]. Then,\ud835\udc3asatisfies the following two properties: \u2022With high probability in\ud835\udc5b, every connected component in\ud835\udc3acontains\ud835\udc42(1)cycles. \u2022For any given vertex\ud835\udc56, the expected size of the connected component\ud835\udc36 \ud835\udc56 containing\ud835\udc56is\ud835\udc42(1). Proof. By Poissonization (Lemmas 5.15 and 5.16), we may assume without loss of generality that, rather than \ud835\udc5a", "source": "loadbalancing.pdf"}, {"text": "any given vertex\ud835\udc56, the expected size of the connected component\ud835\udc36 \ud835\udc56 containing\ud835\udc56is\ud835\udc42(1). Proof. By Poissonization (Lemmas 5.15 and 5.16), we may assume without loss of generality that, rather than \ud835\udc5a balls being thrown, \ud835\udc40\u223cPoisson(\ud835\udc5a) balls are thrown. (Here we use that |\ud835\udc36\ud835\udc56 | monotonically increases with ball throws.) With this modification, the bins are now independent, each independently receiving Poisson(\ud835\udc5a/\ud835\udc5b)balls. Define \ud835\udc4b \ud835\udc56 to be0if\ud835\udc4b \ud835\udc56 \u2264 (1+\ud835\udf00)\u210e=(1+\ud835\udf00)\ud835\udc5a/\ud835\udc5band to be\ud835\udc4b \ud835\udc56 otherwise. For any\ud835\udc57\u22650, we have Pr[\ud835\udc4b\ud835\udc56 \u2265\ud835\udc57]= ( 0if\ud835\udc57\u2264 (1+\ud835\udf00)\u210e Pr[Poisson(\ud835\udc5a/\ud835\udc5b) \u2265\ud835\udc57]otherwise \u2264\ud835\udc52 \u2212\u0398(\ud835\udc57) , sincePr[Poisson(\ud835\udf06) \u2265\ud835\udc57] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57) for all\ud835\udc57\u2265 (1+\u03a9(1))\ud835\udf06. Since\ud835\udc4d \ud835\udc56 is obtained from\ud835\udc4b \ud835\udc56 by sampling every ball with probability\ud835\udc5d, it follows from Lemma 5.14 that Pr[\ud835\udc4d \ud835\udc56 \u2265\ud835\udc57] \u2264\ud835\udc50 \ud835\udc57,(16) where\ud835\udc50is a positive constant", "source": "loadbalancing.pdf"}, {"text": "for all\ud835\udc57\u2265 (1+\u03a9(1))\ud835\udf06. Since\ud835\udc4d \ud835\udc56 is obtained from\ud835\udc4b \ud835\udc56 by sampling every ball with probability\ud835\udc5d, it follows from Lemma 5.14 that Pr[\ud835\udc4d \ud835\udc56 \u2265\ud835\udc57] \u2264\ud835\udc50 \ud835\udc57,(16) where\ud835\udc50is a positive constant determined by\ud835\udc5dthat goes to0as\ud835\udc5dgoes to0. Combining (16) with the fact that the \ud835\udc4d\ud835\udc56s are independent (thanks to Poissonization), we can apply Corollary 5.13 to obtain the desired properties of the graph\ud835\udc3a.\u25a1 Lemma 5.19.Let \u210e1 \u2265\u210e 2, let\ud835\udf00\u2208 ( 0, 1) be a positive constant, and let\ud835\udc5d be a sufficiently small positive constant as a function of \ud835\udf00. Suppose we throw \ud835\udc5a1 =\u210e 1\ud835\udc5b blue balls into \ud835\udc5b bins and \ud835\udc5a2 =\u210e 2\ud835\udc5b red balls in \ud835\udc5b bins, independently and uniformly at random. Let\ud835\udc4b\ud835\udc56 denote the number of blue balls in", "source": "loadbalancing.pdf"}, {"text": "\ud835\udc5a1 =\u210e 1\ud835\udc5b blue balls into \ud835\udc5b bins and \ud835\udc5a2 =\u210e 2\ud835\udc5b red balls in \ud835\udc5b bins, independently and uniformly at random. Let\ud835\udc4b\ud835\udc56 denote the number of blue balls in bin\ud835\udc56. Now suppose that we remove each blue ball from the system independently with probability1\u2212\ud835\udc5d. Let \ud835\udc4c\ud835\udc56 be the number of remaining red balls in bin\ud835\udc56, and define \ud835\udc4d\ud835\udc56 = ( 0if\ud835\udc4b \ud835\udc56 \u2265 (1\u2212\ud835\udf00)\u210e 1 \ud835\udc4c\ud835\udc56 otherwise. Finally, construct a (multi-)graph\ud835\udc3a on vertices [\ud835\udc5b] by adding for each vertex\ud835\udc56\u2208 [\ud835\udc5b]\ud835\udc4d \ud835\udc56 edges from\ud835\udc56 to independent and uniformly random vertices in[\ud835\udc5b]. Then,\ud835\udc3asatisfies the following two properties: \u2022With high probability in\ud835\udc5b, every connected component in\ud835\udc3acontains\ud835\udc42(1)cycles. \u2022For any given vertex\ud835\udc56, the expected size|\ud835\udc36 \ud835\udc56 |of the connected component containing\ud835\udc56is\ud835\udc42(1). 29 Proof. By", "source": "loadbalancing.pdf"}, {"text": "in[\ud835\udc5b]. Then,\ud835\udc3asatisfies the following two properties: \u2022With high probability in\ud835\udc5b, every connected component in\ud835\udc3acontains\ud835\udc42(1)cycles. \u2022For any given vertex\ud835\udc56, the expected size|\ud835\udc36 \ud835\udc56 |of the connected component containing\ud835\udc56is\ud835\udc42(1). 29 Proof. By applying Poissonization (Lemmas 5.15 and 5.16) to the blue balls, we may assume without loss of generality that, rather than\ud835\udc5a1 blue balls being thrown,\ud835\udc401 \u223cPoisson(\ud835\udc5a 1) blue balls are thrown. (Here we use that |\ud835\udc36\ud835\udc56 | monotonically decreases with blue ball throws.) By then applying Poissonization again, but this time to the red balls, we may also assume that the number of red balls is not\ud835\udc5a2 =\u210e 2\ud835\udc5b but is instead \ud835\udc402 \u223cPoisson(\ud835\udc5a 2). (Here we use that|\ud835\udc36 \ud835\udc56 |monotonically increases with red ball throws.) Define\ud835\udc4a\ud835\udc56 to be0if \ud835\udc4b\ud835\udc56 \u2265", "source": "loadbalancing.pdf"}, {"text": "number of red balls is not\ud835\udc5a2 =\u210e 2\ud835\udc5b but is instead \ud835\udc402 \u223cPoisson(\ud835\udc5a 2). (Here we use that|\ud835\udc36 \ud835\udc56 |monotonically increases with red ball throws.) Define\ud835\udc4a\ud835\udc56 to be0if \ud835\udc4b\ud835\udc56 \u2265 ( 1 \u2212\ud835\udf00)\u210e 1 and be the number of red balls in bin\ud835\udc56 otherwise. For any \ud835\udc57\u2265 0, we have Pr[\ud835\udc4a\ud835\udc56 \u2265\ud835\udc57]=Pr[Poisson(\u210e 1) \u2264 (1\u2212\ud835\udf00)\u210e 1] \u00b7Pr[Poisson(\u210e 2) \u2265\ud835\udc57] \u2264min(Pr[Poisson(\u210e 1) \u2264 (1\u2212\ud835\udf00)\u210e 1],Pr[Poisson(\u210e 2) \u2264\ud835\udc57]). For \ud835\udc57\u2265 2\u210e1 \u2265 2\u210e2, we have Pr[Poisson(\u210e 2) \u2265\ud835\udc57] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57) , and for \ud835\udc57\u2264 2\u210e1, we have Pr[Poisson(\u210e 1) \u2264 (1\u2212\ud835\udf00)\u210e 1] \u2264\ud835\udc52 \u2212\u03a9(\u210e 1 ) =\ud835\udc52 \u2212\u03a9(\ud835\udc57) . Thus, for all values of\ud835\udc57\u22650, we have Pr[\ud835\udc4a\ud835\udc56 \u2265\ud835\udc57] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57) . Since\ud835\udc4d \ud835\udc56 is obtained from\ud835\udc4a\ud835\udc56 by sampling every ball with probability\ud835\udc5d,", "source": "loadbalancing.pdf"}, {"text": "\u2264\ud835\udc52 \u2212\u03a9(\u210e 1 ) =\ud835\udc52 \u2212\u03a9(\ud835\udc57) . Thus, for all values of\ud835\udc57\u22650, we have Pr[\ud835\udc4a\ud835\udc56 \u2265\ud835\udc57] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc57) . Since\ud835\udc4d \ud835\udc56 is obtained from\ud835\udc4a\ud835\udc56 by sampling every ball with probability\ud835\udc5d, it follows from Lemma 5.14 that Pr[\ud835\udc4d \ud835\udc56 \u2265\ud835\udc57] \u2264\ud835\udc50 \ud835\udc57,(17) where\ud835\udc50is a positive constant determined by\ud835\udc5dthat goes to0as\ud835\udc5dgoes to0. Combining (17) with the fact that the \ud835\udc4d\ud835\udc56s are independent (thanks to Poissonization), we can apply Corollary 5.13 to obtain the desired properties of the graph\ud835\udc3a.\u25a1 Finally, we also establish an extension of the previous three lemmas, which allows the vertex\ud835\udc56 that we are considering to be selected not arbitrarily but by a specific ball. Corollary 5.20.Consider the setups in any of Lemmas 5.17, 5.18, and 5.19. Condition on some", "source": "loadbalancing.pdf"}, {"text": "vertex\ud835\udc56 that we are considering to be selected not arbitrarily but by a specific ball. Corollary 5.20.Consider the setups in any of Lemmas 5.17, 5.18, and 5.19. Condition on some specific ball \ud835\udc4f being thrown into bin1. Then, the expected size of the connected component containing vertex1remains \ud835\udc42( 1). Proof. We can follow the same proofs as before, but excluding\ud835\udc4f from the balls that we Poissonize (i.e., applying Poissonization to all of the ballsexceptfor \ud835\udc4f). Additionally, when establishing (15), (16), and (17), a small amount of additional care is needed to note that the deterministic presense of ball\ud835\udc4f in bin1does not violate the correctness of any of the three inequalities.\u25a1 5.6 Proving Theorem 1.1 We are now ready to prove", "source": "loadbalancing.pdf"}, {"text": "to note that the deterministic presense of ball\ud835\udc4f in bin1does not violate the correctness of any of the three inequalities.\u25a1 5.6 Proving Theorem 1.1 We are now ready to prove Theorem 1.1. Theorem 1.1.There exists a history-independent two-choice allocation algorithm A that achieves the following guarantees: The expected recourse of A is \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Furthermore, for each set S of balls, the overload induced byAis\ud835\udc42(1)with high probability in\ud835\udc5b. Proof. By Theorem 4.1, Slice and Spread has cumulative overload\ud835\udc42(\ud835\udc5b) with high probability in\ud835\udc5b and expected recourse \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) . Furthermore, Slice and Spread is a good pre-baking algorithm by construction. Applying Theorem 5.1 completes the proof.\u25a1 6 Conjectures and Open Questions In this paper, we present a history-independent two-choice algorithm", "source": "loadbalancing.pdf"}, {"text": "Slice and Spread is a good pre-baking algorithm by construction. Applying Theorem 5.1 completes the proof.\u25a1 6 Conjectures and Open Questions In this paper, we present a history-independent two-choice algorithm that achieves\ud835\udc42( 1) expected overload and \ud835\udc42(log log(\ud835\udc5a/\ud835\udc5b)) expected recourse. We conjecture that these bounds are optimal, and, in particular, that 30 \ud835\udc42( 1) overload requires \u03a9(log log(\ud835\udc5a/\ud835\udc5b)) recourse. On the lower-bound side, even proving any\ud835\udf14( 1) bound on expected recourse (for either weakly or strongly history independent solutions) would be quite interesting. Even if one does not care about history independence, the results in this paper offer a new state of the art for dynamic two-choice load balancing with recourse. Here, again, it is an interesting open question whether", "source": "loadbalancing.pdf"}, {"text": "history independence, the results in this paper offer a new state of the art for dynamic two-choice load balancing with recourse. Here, again, it is an interesting open question whether one can achieve \ud835\udc42( 1) expected overload with \ud835\udc5c(log log(\ud835\udc5a/\ud835\udc5b)) expected recourse. Note that, in general, one of the main challenges for this problem is the following: One must be careful to handlereappearance dependencies[ 5, 2], which occur if the same element is inserted, deleted, and resinserted many times, in which case the random hashes used by the element have already affected the state of the data structure in the past, and therefore can no longer be treated as random. History independence dodges this issue by eliminating history-related dependencies entirely.", "source": "loadbalancing.pdf"}, {"text": "already affected the state of the data structure in the past, and therefore can no longer be treated as random. History independence dodges this issue by eliminating history-related dependencies entirely. It is possible that history-dependent algorithms may be able to do better, but such algorithms would likely require nontrivial techniques to handle reappearance dependencies. Finally, closely related to the problem of constructing dynamic two-choice load-balancing strategies is the problem of designing time-efficient implementations of bucketized cuckoo hashing [14, 15]. Here, one cares both about time efficiency and space efficiency (including the space needed to store metadata used by the algorithm). One of the most natural open questions in this area [15] is whether there exists a solution that supports space", "source": "loadbalancing.pdf"}, {"text": "the space needed to store metadata used by the algorithm). One of the most natural open questions in this area [15] is whether there exists a solution that supports space efficiency1\u2212\ud835\udf00 , using buckets of size\ud835\udc42(\ud835\udf00 \u22121), and with\ud835\udc42( 1) expected-time insertions/deletions. This question, which is at least as hard as the dynamic two-choice load-balancing problem, remains open. Acknowledgments This work was supported in part by NSF grants CCF-2247577, CCF-2106827, CNS-2504471, 2212746, 2044679, 2128519, the Packard fellowship, the Carnegie Mellon University CyLab Presidential Fellowship, a grant from 0xPARC, and a grant from the late Nikolai Mushegian. References [1] U. A. Acar, G. E. Blelloch, R. Harper, J. L. Vittes, and S. L. M. Woo. Dynamizing static algorithms, with applications to", "source": "loadbalancing.pdf"}, {"text": "grant from the late Nikolai Mushegian. References [1] U. A. Acar, G. E. Blelloch, R. Harper, J. L. Vittes, and S. L. M. Woo. Dynamizing static algorithms, with applications to dynamic trees and history independence. InProc. of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 531\u2013540, 2004. [2] K. Agrawal, W. Kuszmaul, Z. Wang, and J. Zhao. Distributed load balancing in the face of reappearance dependencies. InProceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures, pages 321\u2013330, 2024. [3] O. Amble and D. E. Knuth. Ordered hash tables.The Computer Journal, 17(2):135\u2013142, 1974. [4] Y. Azar, A. Z. Broder, A. R. Karlin, and E. Upfal. Balanced allocations. InSymposium on theory of computing (STOC), pages 593\u2013602, 1994.", "source": "loadbalancing.pdf"}, {"text": "Ordered hash tables.The Computer Journal, 17(2):135\u2013142, 1974. [4] Y. Azar, A. Z. Broder, A. R. Karlin, and E. Upfal. Balanced allocations. InSymposium on theory of computing (STOC), pages 593\u2013602, 1994. [5] N. Bansal and W. Kuszmaul. Balanced allocations: The heavily loaded case with deletions. In2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 801\u2013812. IEEE, 2022. [6] S. Behnezhad, M. Derakhshan, M. Hajiaghayi, C. Stein, and M. Sudan. Fully dynamic maximal independent set with polylogarithmic update time. In2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS), pages 382\u2013405. IEEE, 2019. [7] M. A. Bender, J. W. Berry, R. Johnson, T. M. Kroeger, S. McCauley, C. A. Phillips, B. Simon, S. Singh, and D. Zage. Anti-persistence", "source": "loadbalancing.pdf"}, {"text": "(FOCS), pages 382\u2013405. IEEE, 2019. [7] M. A. Bender, J. W. Berry, R. Johnson, T. M. Kroeger, S. McCauley, C. A. Phillips, B. Simon, S. Singh, and D. Zage. Anti-persistence on persistent storage: History-independent sparse tables and dictionaries. InProc. 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS), pages 289\u2013302, 2016. 31 [8] M. A. Bender, A. Conway, M. Farach-Colton, H. Koml\u00f3s, W. Kuszmaul, and N. Wein. Online list labeling: Breaking the barrier.SIAM Journal on Computing, pages FOCS22\u201360, 2024. [9] P. Berenbrink, A. Czumaj, A. Steger, and B. V\u00f6cking. Balanced allocations: the heavily loaded case. InSymposium on Theory of Computing (STOC), pages 745\u2013754, 2000. [10] A. Berger, W. Kuszmaul, A. Polak, J. Tidor, and N. Wein. Memoryless worker-task assignment", "source": "loadbalancing.pdf"}, {"text": "Balanced allocations: the heavily loaded case. InSymposium on Theory of Computing (STOC), pages 745\u2013754, 2000. [10] A. Berger, W. Kuszmaul, A. Polak, J. Tidor, and N. Wein. Memoryless worker-task assignment with polylogarithmic switching cost. In49th International Colloquium on Automata, Languages, and Programming (ICALP 2022), pages 19\u20131. Schloss Dagstuhl\u2013Leibniz-Zentrum f\u00fcr Informatik, 2022. [11] G. E. Blelloch and D. Golovin. Strongly history-independent hashing with applications. InProc. 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 272\u2013282, 2007. [12] N. Buchbinder and E. Petrank. Lower and upper bounds on obtaining history independence. In Advances in Cryptology, pages 445\u2013462, 2003. [13] R. Cole, A. Frieze, B. M. Maggs, M. Mitzenmacher, A. W. Richa, R. Sitaraman, and E. Upfal. On balls and bins", "source": "loadbalancing.pdf"}, {"text": "independence. In Advances in Cryptology, pages 445\u2013462, 2003. [13] R. Cole, A. Frieze, B. M. Maggs, M. Mitzenmacher, A. W. Richa, R. Sitaraman, and E. Upfal. On balls and bins with deletions. InInternational Workshop on Randomization and Approximation Techniques in Computer Science, pages 145\u2013158. Springer, 1998. [14] M. Dietzfelbinger and C. Weidling. Balanced allocation and dictionaries with tightly packed constant size bins.Theoretical Computer Science, 380(1-2):47\u201368, 2007. [15] A. Frieze and S. Petti. Balanced allocation through random walk.Information Processing Letters, 131:39\u201343, 2018. [16] D. Golovin. B-treaps: A uniquely represented alternative to B-trees. InProc. 36th Annual International Colloquium on Automata, Languages, and Programming (ICALP), pages 487\u2013499, 2009. [17] D. Golovin. The B-skip-list: A simpler uniquely represented alternative to B-trees.arXiv preprint arXiv:1005.0662, 2010.", "source": "loadbalancing.pdf"}, {"text": "B-trees. InProc. 36th Annual International Colloquium on Automata, Languages, and Programming (ICALP), pages 487\u2013499, 2009. [17] D. Golovin. The B-skip-list: A simpler uniquely represented alternative to B-trees.arXiv preprint arXiv:1005.0662, 2010. [18] J. D. Hartline, E. S. Hong, A. E. Mohr, W. R. Pentney, and E. Rocke. Characterizing history independent data structures. InProceedings of the Algorithms and Computation, 13th International Symposium (ISAAC), pages 229\u2013240, November 2002. [19] J. D. Hartline, E. S. Hong, A. E. Mohr, W. R. Pentney, and E. C. Rocke. Characterizing history independent data structures.Algorithmica, 42(1):57\u201374, 2005. [20] W. Kuszmaul. Strongly history-independent storage allocation: New upper and lower bounds. In2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS), pages 1822\u20131841. IEEE, 2023. [21] C. McDiarmid et", "source": "loadbalancing.pdf"}, {"text": "W. Kuszmaul. Strongly history-independent storage allocation: New upper and lower bounds. In2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS), pages 1822\u20131841. IEEE, 2023. [21] C. McDiarmid et al. On the method of bounded differences.Surveys in combinatorics, 141(1):148\u2013188, 1989. [22] D. Micciancio. Oblivious data structures: applications to cryptography. InProc. 29th Annual ACM Symposium on Theory of Computing (STOC), pages 456\u2013464, 1997. [23] M. Mitzenmacher. Studying balanced allocations with differential equations.Combinatorics, Probability and Computing, 8(5):473\u2013482, 1999. [24] M. Mitzenmacher. The power of two choices in randomized load balancing.IEEE Transactions on Parallel and Distributed Systems, 12(10):1094\u20131104, 2001. 32 [25] M. Mitzenmacher and E. Upfal.Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017. [26]", "source": "loadbalancing.pdf"}, {"text": "on Parallel and Distributed Systems, 12(10):1094\u20131104, 2001. 32 [25] M. Mitzenmacher and E. Upfal.Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017. [26] M. Naor, G. Segev, and U. Wieder. History-independent cuckoo hashing. InProc. of the 35th International Colloquium on Automata, Languages and Programming (ICALP), pages 631\u2013642. Springer, 2008. [27] M. Naor, G. Segev, and U. Wieder. History-independent cuckoo hashing. InAutomata, Languages and Programming: 35th International Colloquium, ICALP 2008, Reykjavik, Iceland, July 7-11, 2008, Proceedings, Part II 35, pages 631\u2013642. Springer, 2008. [28] M. Naor and V. Teague. Anti-persistence: history independent data structures. InProc. 33rd Annual ACM Symposium on Theory of Computing (STOC), pages 492\u2013501, 2001. [29] R. Pagh and F. F. Rodler.", "source": "loadbalancing.pdf"}, {"text": "M. Naor and V. Teague. Anti-persistence: history independent data structures. InProc. 33rd Annual ACM Symposium on Theory of Computing (STOC), pages 492\u2013501, 2001. [29] R. Pagh and F. F. Rodler. Cuckoo hashing.Journal of Algorithms, 51(2):122\u2013144, 2004. [30] R. Seidel and C. R. Aragon. Randomized search trees.Algorithmica, 16(4):464\u2013497, 1996. [31] K. Talwar and U. Wieder. Balanced allocations: A simple proof for the heavily loaded case. InInternational Colloquium on Automata, Languages, and Programming (ICALP), pages 979\u2013990. Springer, 2014. [32] B. V\u00f6cking. How asymmetry helps load balancing. InFoundations of Computer Science (FOCS), page 131, 1999. A A Lower Bound for History Independent Greedy In this section, we prove the following lower bound: Proposition 3.3.If\ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , then the expected recourse of HI Greedy", "source": "loadbalancing.pdf"}, {"text": "131, 1999. A A Lower Bound for History Independent Greedy In this section, we prove the following lower bound: Proposition 3.3.If\ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , then the expected recourse of HI Greedy is\u03a9(\ud835\udc5a/\ud835\udc5b). Throughout the section, we will focus on a single insertion of an element\ud835\udc65 \u2217 into a set S of size\ud835\udc5a\u2212 1, and we will choose\ud835\udc65 \u2217 to be smaller than all of the elements inS according to the canonical ordering used by HI Greedy. As in the upper bound, we consider two parallel worlds, called world 0 and world 1, in which we insert the ballsS and S \u222a {\ud835\udc65 \u2217} according to the canonical ordering. We will think of the insertions as being performed at the same time", "source": "loadbalancing.pdf"}, {"text": "1, in which we insert the ballsS and S \u222a {\ud835\udc65 \u2217} according to the canonical ordering. We will think of the insertions as being performed at the same time in the two worlds\u2014time\ud835\udc56 refers to the time immediately after the(\ud835\udc56\u2212 1)th insertion in world 0 and immediately after the\ud835\udc56-th insertion in world1. By Fact 3.2, we know that the only difference between the loads of the bins in these two worlds is that, at any given moment (after the first insertion), there is exactly onespecial binthat contains one more ball in world 1 than it does in world 0. We will denote the load of the special bin at time\ud835\udc61, including the extra ball that it contains in world", "source": "loadbalancing.pdf"}, {"text": "ball in world 1 than it does in world 0. We will denote the load of the special bin at time\ud835\udc61, including the extra ball that it contains in world 1, asspecial(\ud835\udc61). Since the bin loads are essentially the same in the two world (with the exception of the special bin), we can feel free to focus our discussion on world 1. Throughout the rest of the section, when we refer to the load of a bin, we are implicitly referring to its load in world 1. As notation, we will use \u2113 (\ud835\udc61) \ud835\udc56 to refer to the load of bin \ud835\udc56 at time \ud835\udc56. We will also let \ud835\udc651 =\ud835\udc65 \u2217, and let \ud835\udc652, \ud835\udc653, . . .", "source": "loadbalancing.pdf"}, {"text": "will use \u2113 (\ud835\udc61) \ud835\udc56 to refer to the load of bin \ud835\udc56 at time \ud835\udc56. We will also let \ud835\udc651 =\ud835\udc65 \u2217, and let \ud835\udc652, \ud835\udc653, . . . , \ud835\udc65\ud835\udc5a\u22121 denote the balls inSin the canonical ordering used to insert them. Call a pair (\ud835\udc56, \ud835\udc57) \u2208 [\ud835\udc5b] \u00d7 [\ud835\udc5b] acritical tieat time \ud835\udc61 if, at time \ud835\udc61, bin \ud835\udc56 is the special bin and \u2113 (\ud835\udc61) \ud835\udc57 = special(\ud835\udc61) \u22121. Lemma A.1.If, immediately prior to ball \ud835\udc65\ud835\udc61 being inserted, one of the pairs (\u210e1(\ud835\udc65\ud835\udc61 ), \u210e2(\ud835\udc65\ud835\udc61 )) or (\u210e2(\ud835\udc65\ud835\udc61 ), \u210e1(\ud835\udc65\ud835\udc61 )) is a critical tie, then with 50% probability the ball\ud835\udc65 \ud835\udc61 gets allocated to a different bin in world 1 than in world 0. 33", "source": "loadbalancing.pdf"}, {"text": "or (\u210e2(\ud835\udc65\ud835\udc61 ), \u210e1(\ud835\udc65\ud835\udc61 )) is a critical tie, then with 50% probability the ball\ud835\udc65 \ud835\udc61 gets allocated to a different bin in world 1 than in world 0. 33 Proof. Without loss of generality, bin\u210e1(\ud835\udc65\ud835\udc61 ) is the special bin and bin\u210e2(\ud835\udc65\ud835\udc61 ) has load one smaller than that of the speical bin at time \ud835\udc61\u2212 1. Then, in world0the insertion sees equal loads in bins \u210e1(\ud835\udc65\ud835\udc61 ) and \u210e2(\ud835\udc65\ud835\udc61 ) and picks one at random; but in world1, the insertion sees a larger load in bin\u210e1(\ud835\udc65\ud835\udc61 ) and picks bin\u210e2(\ud835\udc65\ud835\udc61 ) deterministically. \u25a1 Say that a critical tie(\ud835\udc56, \ud835\udc57) iscreatedat time \ud835\udc61 if the critical tie occurs at time\ud835\udc61 but not at time\ud835\udc61\u2212 1. Using Lemma A.1, we", "source": "loadbalancing.pdf"}, {"text": "picks bin\u210e2(\ud835\udc65\ud835\udc61 ) deterministically. \u25a1 Say that a critical tie(\ud835\udc56, \ud835\udc57) iscreatedat time \ud835\udc61 if the critical tie occurs at time\ud835\udc61 but not at time\ud835\udc61\u2212 1. Using Lemma A.1, we can relate the total number of critical ties that are created (over all time) to the total recourse incurred by HI Greedy. Lemma A.2.Let \ud835\udc44 be the total number of critical ties created across all insertions, and let \ud835\udc45 be the total recourse of the algorithm. Then, E[\ud835\udc45] \u2265\u03a9(E[\ud835\udc44]/\ud835\udc5b) \u2212\ud835\udc42(1). Proof. To simplify the proof, let us assume that the final ball \ud835\udc65\ud835\udc5a to be inserted has hash \u210e1(\ud835\udc65\ud835\udc5a) equal to whatever the special bin is at time\ud835\udc5a\u2212 1. Note that this assumption is without loss of generality, as it", "source": "loadbalancing.pdf"}, {"text": "final ball \ud835\udc65\ud835\udc5a to be inserted has hash \u210e1(\ud835\udc65\ud835\udc5a) equal to whatever the special bin is at time\ud835\udc5a\u2212 1. Note that this assumption is without loss of generality, as it changes \ud835\udc45and\ud835\udc44each by at most one. Call this assumption theNice Final Insertion Assumption. Recall that a critical tie is created when a bin\ud835\udc57 has load special(\ud835\udc61) \u2212 1(and when this was not true for time \ud835\udc61\u2212 1). Suppose this happens at some time\ud835\udc61<\ud835\udc5a , and let\ud835\udc58 be the special bin at that time. Consider the next insertion \ud835\udc65 to have at least one of its hashes\u210e1(\ud835\udc65), \u210e2(\ud835\udc65) equal to at least one of\ud835\udc57 or \ud835\udc58 (such an \ud835\udc65 must exist by the Nice Final Insertion Assumption). Given that at least", "source": "loadbalancing.pdf"}, {"text": "at least one of its hashes\u210e1(\ud835\udc65), \u210e2(\ud835\udc65) equal to at least one of\ud835\udc57 or \ud835\udc58 (such an \ud835\udc65 must exist by the Nice Final Insertion Assumption). Given that at least one of the insertion\u2019s hashes are\ud835\udc57 or \ud835\udc58, the probability that the insertion\u2019s hashes areboth\ud835\udc57 and \ud835\udc58 is \u03a9(1/\ud835\udc5b). If this happens, then the insertion\ud835\udc65 is incurs recourse with probability at least one half (by Lemma A.1). The above process matches each critical tie that is created prior to time\ud835\udc5a to a (distinct) future insertion that, with probability at least\u03a9(1/\ud835\udc5b), incurs recourse. It follows thatE[\ud835\udc45] \u2265E[\ud835\udc44] \u00b7\u03a9(1/\ud835\udc5b) \u2212\u03a9(1).\u25a1 Thus, to prove Proposition 3.3, it suffices to show that the expected number of critical ties that are created over all insertions", "source": "loadbalancing.pdf"}, {"text": "incurs recourse. It follows thatE[\ud835\udc45] \u2265E[\ud835\udc44] \u00b7\u03a9(1/\ud835\udc5b) \u2212\u03a9(1).\u25a1 Thus, to prove Proposition 3.3, it suffices to show that the expected number of critical ties that are created over all insertions is\u03a9(\ud835\udc5a) . This means that a constant fraction of insertions (in expectation) create critical ties. The intuition for why this would be the case comes from the following lemma, which says that, at any given moment, we expect most of the bins to have heights within a band of width\ud835\udc42(1): Lemma A.3.Suppose we perform \ud835\udc5a insertions using the greedy algorithm. For any positive constant \ud835\udc501, there exists a positive constant \ud835\udc502 such that the following is true: after the \ud835\udc61-th insertion, the expected number of bins with loads in[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50 2,", "source": "loadbalancing.pdf"}, {"text": "For any positive constant \ud835\udc501, there exists a positive constant \ud835\udc502 such that the following is true: after the \ud835\udc61-th insertion, the expected number of bins with loads in[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50 2, \ud835\udc61/\ud835\udc5b+\ud835\udc50 2]is at least(1\u22121/\ud835\udc50 1)\ud835\udc5b. Proof. This follows directly from Theorem 2.1 of [31], which says that there exists a positive constant\ud835\udc4e for which E \" \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc52\ud835\udc4e\u00b7 |\ud835\udc61/\ud835\udc5b\u2212\u2113 (\ud835\udc61) \ud835\udc56 | # \u2264\ud835\udc42(\ud835\udc5b). \u25a1 With the help of Fact 3.2, we can use McDiarmid\u2019s inequality (see Theorem B.1 in Appendix B) to get a high-probability version of Lemma A.3. Lemma A.4.Suppose \ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , and suppose we perform \ud835\udc5a insertions using the greedy algorithm. For any positive constant \ud835\udc501, there exists a positive constant \ud835\udc502 such that the", "source": "loadbalancing.pdf"}, {"text": "A.3. Lemma A.4.Suppose \ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , and suppose we perform \ud835\udc5a insertions using the greedy algorithm. For any positive constant \ud835\udc501, there exists a positive constant \ud835\udc502 such that the following is true with high probability in \ud835\udc5b. After each insertion\ud835\udc61=1,2, . . . \ud835\udc5a, at least1\u22121/\ud835\udc50 1 fraction of the bins have loads within\ud835\udc50 2 of\ud835\udc61/\ud835\udc5b. 34 Proof.It suffices to prove the bound for\ud835\udc61=\ud835\udc5a, since it then follows by a union bound over\ud835\udc61\u2208 [1, \ud835\udc5a]. Define \ud835\udc4b1, \ud835\udc4b2, . . . , \ud835\udc4b\ud835\udc5a, where \ud835\udc4b\ud835\udc56 represents the hashes used by the\ud835\udc56-th insertion. Let \ud835\udc39(\ud835\udc4b 1, \ud835\udc4b2, . . . , \ud835\udc4b\ud835\udc5a) denote the number of bins with heights in the range[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50 2, \ud835\udc61/\ud835\udc5b+\ud835\udc50 2]. By Fact 3.2, \ud835\udc39 is2-Lipschitz", "source": "loadbalancing.pdf"}, {"text": "by the\ud835\udc56-th insertion. Let \ud835\udc39(\ud835\udc4b 1, \ud835\udc4b2, . . . , \ud835\udc4b\ud835\udc5a) denote the number of bins with heights in the range[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50 2, \ud835\udc61/\ud835\udc5b+\ud835\udc50 2]. By Fact 3.2, \ud835\udc39 is2-Lipschitz (i.e., changing a value of some \ud835\udc4b\ud835\udc56 changes \ud835\udc39 by at most2). Since \ud835\udc4b1, \ud835\udc4b2, . . . , \ud835\udc4b\ud835\udc5a are independent, it follows by McDiarmid\u2019s inequality that Pr[\ud835\udc39\u2212E[\ud835\udc39] \u2265 \u221a\ud835\udc5alog\ud835\udc5b] \u22641/\ud835\udc5b \ud835\udf14(1) . Since\ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , we have with high probability in\ud835\udc5bthat\ud835\udc39is within\ud835\udc5c(\ud835\udc5a)of its mean. To complete the proof, it suffices to show thatE[\ud835\udc39] \u2265 ( 1 \u2212\ud835\udc53(\ud835\udc50 2))\ud835\udc5b for some function \ud835\udc53 that goes to0 as\ud835\udc50 2 goes to infinity. This follows from Lemma A.3.\u25a1 Building on this, we can also get the following lemma, which says that,", "source": "loadbalancing.pdf"}, {"text": "for some function \ud835\udc53 that goes to0 as\ud835\udc50 2 goes to infinity. This follows from Lemma A.3.\u25a1 Building on this, we can also get the following lemma, which says that, although most bins have loads that are in a band of size\ud835\udc502 around \ud835\udc61/\ud835\udc5b, there will also still be a non-trivial\u03a9(1) fraction that are below the band. Lemma A.5.Suppose \ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) , and suppose we perform \ud835\udc5a insertions using the greedy algorithm. For any sufficiently large positive constant\ud835\udc502, there exists a positive constant \ud835\udc503 such that the following is true with high probability in\ud835\udc5b. At any time\ud835\udc61\u22653\ud835\udc50 2\ud835\udc5b, at least a1/\ud835\udc50 3 fraction of the bins have loads smaller than\ud835\udc61/\ud835\udc5b\u22122\ud835\udc50 2\ud835\udc5b. Proof. Since \ud835\udc502 is a sufficiently large positive constant,", "source": "loadbalancing.pdf"}, {"text": "with high probability in\ud835\udc5b. At any time\ud835\udc61\u22653\ud835\udc50 2\ud835\udc5b, at least a1/\ud835\udc50 3 fraction of the bins have loads smaller than\ud835\udc61/\ud835\udc5b\u22122\ud835\udc50 2\ud835\udc5b. Proof. Since \ud835\udc502 is a sufficiently large positive constant, we know by Lemma A.4 that with high probability in \ud835\udc5b the following is true: at time\ud835\udc61\u2212 3\ud835\udc502\ud835\udc5b, there are \u03a9(\ud835\udc5b) bins \ud835\udc34 with heights less than (\ud835\udc61\u2212 2\ud835\udc502)\ud835\udc5b. During insertions \ud835\udc61\u2212 3\ud835\udc502\ud835\udc5b+ 1, . . . , \ud835\udc61, the probability that a given bin \ud835\udc4e\u2208\ud835\udc34 manages toavoidany new elements hashing to it is at least (1 \u2212 1/\ud835\udc5b)\ud835\udc42(\ud835\udc5b) =\u03a9( 1). The expected number of bins that, at time \ud835\udc61, have load less than \ud835\udc61\u2212 2\ud835\udc502\ud835\udc5b is therefore at least\u03a9(\ud835\udc5b) . By McDiarmid\u2019s Inequality (applied in the same way as", "source": "loadbalancing.pdf"}, {"text": "1). The expected number of bins that, at time \ud835\udc61, have load less than \ud835\udc61\u2212 2\ud835\udc502\ud835\udc5b is therefore at least\u03a9(\ud835\udc5b) . By McDiarmid\u2019s Inequality (applied in the same way as in Lemma A.4), the number of such bins is within\ud835\udc5c(\ud835\udc5b) of its mean with high probability in\ud835\udc5b. It follows that, with high probability in\ud835\udc5b, there are\u03a9(\ud835\udc5b)such bins.\u25a1 The main difficulty in proving Proposition 3.3 is that thespecial(\ud835\udc61) is not simply\u2113 (\ud835\udc61) \ud835\udc56 for a uniformly random bin \ud835\udc56. Rather, the load of the special bin evolves according to a more intricate process. The next lemma says that, nonetheless, we do expectspecial(\ud835\udc61)to typically be within\ud835\udc42(1)of\ud835\udc5b/\ud835\udc61. Lemma A.6.Let \ud835\udc50 be a sufficiently large positive constant. The expected number of times \ud835\udc61\u2208 [\ud835\udc5a]", "source": "loadbalancing.pdf"}, {"text": "intricate process. The next lemma says that, nonetheless, we do expectspecial(\ud835\udc61)to typically be within\ud835\udc42(1)of\ud835\udc5b/\ud835\udc61. Lemma A.6.Let \ud835\udc50 be a sufficiently large positive constant. The expected number of times \ud835\udc61\u2208 [\ud835\udc5a] at which special(\ud835\udc61) \u2208 [\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50]is\u03a9(\ud835\udc5a). Proof. Let \ud835\udc4b(\ud835\udc61) be the event that, at time step\ud835\udc61, we have both thatspecial(\ud835\udc61)\u2209[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50] , and that at least 90% of bins have loads in the range[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50] . Define \u0394(\ud835\udc61)\u2254|special(\ud835\udc61) \u2212\ud835\udc61/\ud835\udc5b| \u2212 |special(\ud835\udc61\u2212 1) \u2212 (\ud835\udc61\u22121)/\ud835\udc5b|. We will prove that, E[\u0394(\ud835\udc61) |\ud835\udc4b(\ud835\udc61)] \u2264 \u22120.5/\ud835\udc5b,(18) and that E[\u0394(\ud835\udc61) | \ud835\udc4b(\ud835\udc61)] \u22643/\ud835\udc5b.(19) Before we prove(18) and (19), let us first take them for granted, and see how to prove the lemma at hand. If (18) and (19) hold, then it follows that E \"\u2211\ufe01", "source": "loadbalancing.pdf"}, {"text": "prove(18) and (19), let us first take them for granted, and see how to prove the lemma at hand. If (18) and (19) hold, then it follows that E \"\u2211\ufe01 \ud835\udc61 \u0394(\ud835\udc61) # \u2264 \u2211\ufe01 \ud835\udc61 \u0010 Pr[\ud835\udc4b(\ud835\udc61)] \u00b73/\ud835\udc5b\u2212Pr[\ud835\udc4b(\ud835\udc61)] \u00b71/(2\ud835\udc5b) \u0011 . But, by definition,\u00cd \ud835\udc61 \u0394(\ud835\udc61)=|special(\ud835\udc5a) \u2212\ud835\udc5a/\ud835\udc5b| \u22650. Thus \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4b(\ud835\udc61)] \u00b73/\ud835\udc5b\u2265 \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4b(\ud835\udc61)] \u00b71/(2\ud835\udc5b), 35 which implies that \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4b(\ud835\udc61)] \u2265\u03a9(\ud835\udc5a). By a union bound, \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4b(\ud835\udc61)] \u2264 \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4c(\ud835\udc61)] + \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4d(\ud835\udc61)], where\ud835\udc4c(\ud835\udc61)is the event thatspecial(\ud835\udc61) \u2208 [\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50]and\ud835\udc4d(\ud835\udc61)is the event that less than 90% of bins have loads in[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50]at time\ud835\udc61. By Lemma A.4,Pr[\ud835\udc4d(\ud835\udc61)] \u22641/poly(\ud835\udc5b), so we can conclude that \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4c(\ud835\udc61)] \u2265\u03a9(\ud835\udc5a), which implies the lemma. It remains", "source": "loadbalancing.pdf"}, {"text": "that less than 90% of bins have loads in[\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50]at time\ud835\udc61. By Lemma A.4,Pr[\ud835\udc4d(\ud835\udc61)] \u22641/poly(\ud835\udc5b), so we can conclude that \u2211\ufe01 \ud835\udc61 Pr[\ud835\udc4c(\ud835\udc61)] \u2265\u03a9(\ud835\udc5a), which implies the lemma. It remains to prove(18) and (19). To prove(18), let us condition on event\ud835\udc4b(\ud835\udc61) , and consider two cases, depending on whetherspecial(\ud835\udc61)<\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50orspecial(\ud835\udc61)>\ud835\udc61/\ud835\udc5b+\ud835\udc50. Suppose event \ud835\udc4b(\ud835\udc61) occurs and that special(\ud835\udc61)<\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50 . If the \ud835\udc61-th insertion has one hash equal to the special bin and the other equal to one of the\u2265 90%of bins with loads in[\ud835\udc61/\ud835\udc5b\u2212 1, \ud835\udc61/\ud835\udc5b+\ud835\udc50] , thenspecial(\ud835\udc61) gets incremented during the insertion. Such an increment occurs with probability at least2\u00b70.9/\ud835\udc5b, and contributes \u22121to\u0394(\ud835\udc61). The insertion also increases the quantity\ud835\udc61/\ud835\udc5bby1/\ud835\udc5b, which adds1/\ud835\udc5bto\u0394(\ud835\udc61). It follows that E[\u0394(\ud835\udc61) |\ud835\udc4b(\ud835\udc61)andspecial(\ud835\udc61)<\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50] \u2264 \u22121\u00b7 (2\u00b70.9/\ud835\udc5b) +1/\ud835\udc5b\u2264 \u22121/(2\ud835\udc5b).(20)", "source": "loadbalancing.pdf"}, {"text": "insertion. Such an increment occurs with probability at least2\u00b70.9/\ud835\udc5b, and contributes \u22121to\u0394(\ud835\udc61). The insertion also increases the quantity\ud835\udc61/\ud835\udc5bby1/\ud835\udc5b, which adds1/\ud835\udc5bto\u0394(\ud835\udc61). It follows that E[\u0394(\ud835\udc61) |\ud835\udc4b(\ud835\udc61)andspecial(\ud835\udc61)<\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50] \u2264 \u22121\u00b7 (2\u00b70.9/\ud835\udc5b) +1/\ud835\udc5b\u2264 \u22121/(2\ud835\udc5b).(20) Next, suppose that event \ud835\udc4b(\ud835\udc61) occurs and that special(\ud835\udc61)>\ud835\udc61/\ud835\udc5b+\ud835\udc50 . In this case, the only way that special(\ud835\udc61) can increment during the\ud835\udc61-th insertion is if the insertion has one hash equal to\ud835\udc4b(\ud835\udc61) and the other hash equal to one of the\u2264 10%of bins with loads >\ud835\udc61\ud835\udc5b+\ud835\udc50 . Such an increment occurs with probability at most2 \u00b70.1/\ud835\udc5b, and contributes1to \u0394(\ud835\udc61) . The insertion also increases the quantity\ud835\udc61/\ud835\udc5b by1 /\ud835\udc5b, which adds \u22121/\ud835\udc5bto\u0394(\ud835\udc61). It follows that E[\u0394(\ud835\udc61) |\ud835\udc4b(\ud835\udc61)andspecial(\ud835\udc61)>\ud835\udc61/\ud835\udc5b+\ud835\udc50] \u22640.2/\ud835\udc5b\u22121/\ud835\udc5b\u2264 \u22121/(2\ud835\udc5b).(21) Combined, (20) and (21) imply (18). Finally, to prove(19), suppose that event\ud835\udc4b(\ud835\udc61)", "source": "loadbalancing.pdf"}, {"text": "The insertion also increases the quantity\ud835\udc61/\ud835\udc5b by1 /\ud835\udc5b, which adds \u22121/\ud835\udc5bto\u0394(\ud835\udc61). It follows that E[\u0394(\ud835\udc61) |\ud835\udc4b(\ud835\udc61)andspecial(\ud835\udc61)>\ud835\udc61/\ud835\udc5b+\ud835\udc50] \u22640.2/\ud835\udc5b\u22121/\ud835\udc5b\u2264 \u22121/(2\ud835\udc5b).(21) Combined, (20) and (21) imply (18). Finally, to prove(19), suppose that event\ud835\udc4b(\ud835\udc61) does not occur. In this case, we can still use the following argument. The probability ofspecial(\ud835\udc61) incrementing during insertion \ud835\udc61 is at most2 /\ud835\udc5b, since at least one of the insertion\u2019s hashes would need to be the special bin; the expected contribution to \u0394(\ud835\udc61) from special(\ud835\udc61) changing is therefore at most2/\ud835\udc5b. The insertion also increases\ud835\udc61/\ud835\udc5b by1/\ud835\udc5b, and this change contributes at most1/\ud835\udc5bto\u0394(\ud835\udc61). It follows that E[\u0394(\ud835\udc61)] \u22642/\ud835\udc5b+1/\ud835\udc5b=3/\ud835\udc5b, as required by (19). \u25a1 We now get to the main argument in the section, where we lower bound the expected number of", "source": "loadbalancing.pdf"}, {"text": "at most1/\ud835\udc5bto\u0394(\ud835\udc61). It follows that E[\u0394(\ud835\udc61)] \u22642/\ud835\udc5b+1/\ud835\udc5b=3/\ud835\udc5b, as required by (19). \u25a1 We now get to the main argument in the section, where we lower bound the expected number of critical ties that are created over time. To simplify the argument, we begin by describing it in terms of the number of critical ties that occur during a random subwindow: Lemma A.7.Suppose \ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) \u2229\ud835\udf14(\ud835\udc5b) . Let \ud835\udc50 be a sufficiently large positive constant, an let\ud835\udc50 \u2032 be a sufficiently large constant as a function of \ud835\udc50. Consider a time window of the form [\ud835\udc61, \ud835\udc61+\ud835\udc50\ud835\udc5b] , where \ud835\udc61 is selected uniformly at random from \ud835\udc61\u2208 [ 3\ud835\udc50\ud835\udc5b, \ud835\udc5a\u2212\ud835\udc50\ud835\udc5b] . The expected number of critical ties that are created during", "source": "loadbalancing.pdf"}, {"text": "window of the form [\ud835\udc61, \ud835\udc61+\ud835\udc50\ud835\udc5b] , where \ud835\udc61 is selected uniformly at random from \ud835\udc61\u2208 [ 3\ud835\udc50\ud835\udc5b, \ud835\udc5a\u2212\ud835\udc50\ud835\udc5b] . The expected number of critical ties that are created during that window is \u03a9(\ud835\udc5b) . Proof. It suffices to show that, with probability\u03a9(1), there are \u03a9(\ud835\udc5b) bins whose heights at time\ud835\udc61 are less thanspecial(\ud835\udc61) \u22121, but whose heights at time\ud835\udc61+\ud835\udc50\ud835\udc5bare at leastspecial(\ud835\udc61+\ud835\udc50\ud835\udc5b). 36 By Lemma A.6 (and the fact that\ud835\udc61 is uniformly random across \ud835\udc5a\u2212\ud835\udc5c(\ud835\udc5a) values), we have with prob- ability \u03a9(1) that special(\ud835\udc61) \u2208 [\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50, \ud835\udc61/\ud835\udc5b+\ud835\udc50] . Conditioned on this, during the next \ud835\udc50 \u2032\ud835\udc5b insertions there is probability (1 \u2212 2/\ud835\udc5b)\ud835\udc50 \u2032\ud835\udc5b =\u03a9( 1) that no inserted balls hash to the special bin, and therefore that special(\ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b)=special(\ud835\udc61)", "source": "loadbalancing.pdf"}, {"text": "this, during the next \ud835\udc50 \u2032\ud835\udc5b insertions there is probability (1 \u2212 2/\ud835\udc5b)\ud835\udc50 \u2032\ud835\udc5b =\u03a9( 1) that no inserted balls hash to the special bin, and therefore that special(\ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b)=special(\ud835\udc61) \u2264\ud835\udc61/\ud835\udc5b+\ud835\udc50 . Putting together the events, we have with probability\u03a9(1) that special(\ud835\udc61) \u2265\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50andspecial(\ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b) \u2264\ud835\udc61/\ud835\udc5b+\ud835\udc50.(22) By Lemma A.5, there exists some\ud835\udefe=\u03a9( 1) such that, with high probability in\ud835\udc5b that at least\ud835\udefe\u00b7\ud835\udc5b bins have loads below\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50 at time \ud835\udc61. Supposing \ud835\udc50 \u2032 is sufficiently large as a function of\ud835\udefe, we have by Lemma A.4 that, with high probability in\ud835\udc5b, at least a(1 \u2212\ud835\udefe/ 2)\ud835\udc5b bins have loads above\ud835\udc61/\ud835\udc5b+\ud835\udc50 at time \ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b. It follows that, with high probability in\ud835\udc5b, there exist at least\ud835\udefe\ud835\udc5b/2bins\ud835\udc57with \u2113 (\ud835\udc61) \ud835\udc57 \u2264\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50and\u2113 (\ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b) \ud835\udc57 \u2265\ud835\udc61/\ud835\udc5b+\ud835\udc50.", "source": "loadbalancing.pdf"}, {"text": "a(1 \u2212\ud835\udefe/ 2)\ud835\udc5b bins have loads above\ud835\udc61/\ud835\udc5b+\ud835\udc50 at time \ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b. It follows that, with high probability in\ud835\udc5b, there exist at least\ud835\udefe\ud835\udc5b/2bins\ud835\udc57with \u2113 (\ud835\udc61) \ud835\udc57 \u2264\ud835\udc61/\ud835\udc5b\u2212\ud835\udc50and\u2113 (\ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b) \ud835\udc57 \u2265\ud835\udc61/\ud835\udc5b+\ud835\udc50. Combining this with (22), we can conclude that with probability\u03a9(1), there are at least\ud835\udefe\ud835\udc5b/2 =\u03a9(\ud835\udc5b) bins whose loads at time \ud835\udc61 are smaller than special(\ud835\udc61) but whose loads at time \ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b are greater than special(\ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b). Every such bin must have experienced a critical tie at some point in the time interval [\ud835\udc61, \ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b]. This completes the proof.\u25a1 As a corollary, we can lower bound the expected overall number of critical ties that occur. Corollary A.8.Suppose \ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) \u2229\ud835\udf14(\ud835\udc5b) . The expected number of critical ties created across", "source": "loadbalancing.pdf"}, {"text": "As a corollary, we can lower bound the expected overall number of critical ties that occur. Corollary A.8.Suppose \ud835\udc5a\u2264\ud835\udc5b 2\u2212\u03a9(1) \u2229\ud835\udf14(\ud835\udc5b) . The expected number of critical ties created across all insertions is\u03a9(\ud835\udc5a). Proof. Let \ud835\udc50 and \ud835\udc50 \u2032 be as in the previous lemma. We know that for \ud835\udc61 selected uniformly at random from [3\ud835\udc50\ud835\udc5b, \ud835\udc5a\u2212\ud835\udc50 \u2032\ud835\udc5b], the expected number of critical ties created in the window[\ud835\udc61, \ud835\udc61+\ud835\udc50 \u2032\ud835\udc5b] is \u03a9(\ud835\udc5b) . This sampling process captures each critical tie with probability at most(\ud835\udc50\u2032\ud835\udc5b)/(\ud835\udc5a\u2212 3\ud835\udc50\ud835\udc5b\u2212\ud835\udc50 \u2032\ud835\udc5b)=\ud835\udc42(\ud835\udc5b/\ud835\udc5a) . The total expected number of critical ties that get created over all insertions must therefore be at least\u03a9(\ud835\udc5b\ud835\udf07)=\u03a9(\ud835\udc5a).\u25a1 Finally, we can complete the proof of Proposition 3.3. Proof of Proposition 3.3. For \ud835\udc5a=\ud835\udc42(\ud835\udc5b)", "source": "loadbalancing.pdf"}, {"text": "number of critical ties that get created over all insertions must therefore be at least\u03a9(\ud835\udc5b\ud835\udf07)=\u03a9(\ud835\udc5a).\u25a1 Finally, we can complete the proof of Proposition 3.3. Proof of Proposition 3.3. For \ud835\udc5a=\ud835\udc42(\ud835\udc5b) , the proposition is trivial. For\ud835\udc5a=\ud835\udf14(\ud835\udc5b) , the proposition follows from Lemma A.2 and Corollary A.8.\u25a1 B McDiarmid\u2019s Inequality In several sections of this paper, it is helpful to make use of the following inequality due to McDiarmid [21]: Theorem B.1(McDiarmid\u2019s Inequality [21]).Say that a function \ud835\udc39(\ud835\udc4b 1, . . . , \ud835\udc4b\ud835\udc5b) is \ud835\udc3f-Lipschitz if changing the value of any single argument \ud835\udc4b\ud835\udc56 never changes \ud835\udc39 by more than \ud835\udc3f. Let \ud835\udc4b1, . . . , \ud835\udc4b\ud835\udc5b be independent random variables, and let\ud835\udc39be an\ud835\udc3f-Lipschitz function. Then, for all\ud835\udc58\u22651, Pr[\ud835\udc39(\ud835\udc4b", "source": "loadbalancing.pdf"}, {"text": "any single argument \ud835\udc4b\ud835\udc56 never changes \ud835\udc39 by more than \ud835\udc3f. Let \ud835\udc4b1, . . . , \ud835\udc4b\ud835\udc5b be independent random variables, and let\ud835\udc39be an\ud835\udc3f-Lipschitz function. Then, for all\ud835\udc58\u22651, Pr[\ud835\udc39(\ud835\udc4b 1, . . . , \ud835\udc4b\ud835\udc5b) \u2212E[\ud835\udc39(\ud835\udc4b 1, . . . , \ud835\udc4b\ud835\udc5b)] \u2265\ud835\udc58 \u221a\ud835\udc5b] \u2264\ud835\udc52 \u2212\u03a9(\ud835\udc58 2 ) . 37", "source": "loadbalancing.pdf"}, {"text": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics Christian Intern`o 1 2 * Jumpei Yamaguchi3 2 * Loren Amdahl-Culleton 4 Markus Olhofer 2\u2020 David Klindt 5\u2020 Barbara Hammer 1\u2020 Abstract Determining whether neural models internalize physical laws as world models, rather than ex- ploiting statistical shortcuts, remains challeng- ing, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent ca- pability via downstream adaptation (e.g., fine- tuning or high-capacity probes), but such inter- ventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We pro- pose a non-invasive evaluation protocol,PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by thelinear representation hypothesis(Nanda et al., 2023b). Across", "source": "observereffect.pdf"}, {"text": "self-supervised learning (SSL). We pro- pose a non-invasive evaluation protocol,PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by thelinear representation hypothesis(Nanda et al., 2023b). Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessi- ble.PhyIPrecovers internal energy and Newto- nian inverse-square scaling on OOD tests (e.g., \u03c1 >0.90 ). In contrast, adaptation-based evalua- tions can collapse this structure (\u03c1\u22480.05 ). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physi- cal world models. 1. Introduction AI for natural sciences has evolved from computational ac- celeration to the construction of \u201cWorld Models\u201d (Ha & Schmidhuber, 2018;", "source": "observereffect.pdf"}, {"text": "more accurate evaluation of physi- cal world models. 1. Introduction AI for natural sciences has evolved from computational ac- celeration to the construction of \u201cWorld Models\u201d (Ha & Schmidhuber, 2018; LeCun & Courant, 2022), aiming to synthesize vast observational data into representations that encode the governing physical laws of the system (Wang et al., 2023; Bommasani et al., 2022), rather than just statis- tical shortcuts (Geirhos et al., 2020). *Equal contribution . \u2020Co-advising. 1Bielefeld University 2Honda Research Institute EU 3Tokyo Institute of Technol- ogy 4Simplex, Astera Institute 5Cold Spring Harbor Laboratory. Correspondence to: Christian Intern `o <christian.interno@uni- bielefeld.de>. \u2642lightbulbHypothesis Physical System SSL (m\u03b8) Dynamics Recovered Linearly m\u03b8\u2032 Dynamics Lost \u03b8\u2192\u03b8\u2032 Downstream taskadaptation The aspiration is that a neural network, trained", "source": "observereffect.pdf"}, {"text": "Laboratory. Correspondence to: Christian Intern `o <christian.interno@uni- bielefeld.de>. \u2642lightbulbHypothesis Physical System SSL (m\u03b8) Dynamics Recovered Linearly m\u03b8\u2032 Dynamics Lost \u03b8\u2192\u03b8\u2032 Downstream taskadaptation The aspiration is that a neural network, trained on diverse physical regimes, will implicitly learn the corresponding physical laws, analogous to the historical transition from describing motion (kinematics) to uncovering the forces that drive it (dynamics) (Schmidt & Lipson, 2009b). However, despite high predictive fidelity within training dis- tributions, neural models often fail to capture universal phys- ical mechanisms (Coveney & Highfield, 2025; Motamed et al., 2025). If a model cannot distinguish universal laws from correlations, its scientific utility remains limited (Chen et al., 2022; Wang et al., 2023). The prevailing methodology for adapting these models to new", "source": "observereffect.pdf"}, {"text": "a model cannot distinguish universal laws from correlations, its scientific utility remains limited (Chen et al., 2022; Wang et al., 2023). The prevailing methodology for adapting these models to new downstream tasks is \u2019pretrain-then-fine-tune\u2019, where the backbone is updated alongside a randomly initialized head for a specific downstream task (Wortsman et al., 2021; Mai et al., 2024). Recently, this paradigm has been ex- tended to validate intrinsic knowledge. Invasive adaptation and high-capacity nonlinear probes are used to test whether inductive biases in models align with a postulated world model (Vafa et al., 2025; Belinkov, 2022a). However, the- oretical works on probing warn that such high-capacity in- terventions often \u201clearn the task\u201d themselves rather than extracting it (Hewitt & Liang, 2019b;", "source": "observereffect.pdf"}, {"text": "et al., 2025; Belinkov, 2022a). However, the- oretical works on probing warn that such high-capacity in- terventions often \u201clearn the task\u201d themselves rather than extracting it (Hewitt & Liang, 2019b; Ravichander et al., 2021), while feature distortion dynamics work suggests that, during adaptation, noisy gradients warp the backbone to fit random initializations rather than the task structure (Kumar et al., 2022; Trivedi et al., 2023). This degrades physical representations, challenging theLin- 1 arXiv:2602.12218v1 [cs.LG] 12 Feb 2026 The Observer Effect in World Models ear Representation Hypothesis(LRH) (Nanda et al., 2023c; DiCarlo & Cox, 2007), which posits that features are en- coded as linear directions in the activation space of capable models. Furthermore, when modeling continuous physical evolution (ranging from orbital", "source": "observereffect.pdf"}, {"text": "DiCarlo & Cox, 2007), which posits that features are en- coded as linear directions in the activation space of capable models. Furthermore, when modeling continuous physical evolution (ranging from orbital ordinary differential equa- tions (ODEs) to fluid partial differential equations (PDEs)), the structural equivalence between residual networks and Euler discretizations becomes critical (Chen et al., 2018b; Haber & Ruthotto, 2017). Since the model effectively learns to act as a numerical integrator for these dynamics (Chen et al., 2018a), invasive fine-tuning can disrupt the weights into non-physical regimes. Bridging the gap between predictive fidelity and the evalua- tion of physical understanding requires a shift towards inter- pretable probing frameworks (Bereska & Gavves, 2024) and rigorous experimental design and control (Hewitt &", "source": "observereffect.pdf"}, {"text": "between predictive fidelity and the evalua- tion of physical understanding requires a shift towards inter- pretable probing frameworks (Bereska & Gavves, 2024) and rigorous experimental design and control (Hewitt & Liang, 2019b; Belinkov, 2022b). We argue that apparent failures to encode physics are often not failures of learning but arti- facts of the measurement process itself (i.e., the downstream adaptation task), which can distort the underlying represen- tation (Santi et al., 2025; Belinkov, 2022a). We propose theNon-Invasive Physical Probe(PhyIP), which is a mechanistic evaluation framework that uses time- independent linear readouts on frozen SSL representations to extract latent conserved quantities. These quantities are then distilled into interpretable formulas via symbolic re- gression (SR) (Biggio et al., 2021) and validated against", "source": "observereffect.pdf"}, {"text": "readouts on frozen SSL representations to extract latent conserved quantities. These quantities are then distilled into interpretable formulas via symbolic re- gression (SR) (Biggio et al., 2021) and validated against strict control baselines for probes (Hewitt & Liang, 2019a). These controls are designed to rule out possible false posi- tives and false negatives under Out-Of-Distribution (OOD) settings. Consequently, we posit that adopting neural dynamics mod- els as afixed measurement instrument(Jaeger, 2001; Men- cattini et al., 2026) is vital for valid scientific inquiry. In classical experimental design, the measuring tool should remain invariant relative to the subject to avoid confounding the observation with the instrument\u2019s own adaptation (Mari et al., 2023). We validate this hypothesis on high-fidelity simulations from the TheWell", "source": "observereffect.pdf"}, {"text": "remain invariant relative to the subject to avoid confounding the observation with the instrument\u2019s own adaptation (Mari et al., 2023). We validate this hypothesis on high-fidelity simulations from the TheWell benchmark (Ohana et al., 2024): a2D Turbulent Radiative Layer(Stachenfeld et al., 2021), a3D Red Supergiant(Goldberg et al., 2022), and a3D Supernova Explosion(Hirashima et al., 2023). We find that low-error SSL models actively encode physical dynamics into linear subspaces across diverse regimes. In radiative turbulence, PhyIPrecovers the internal energy law ( E\u22481.5P ) with high precision. In the more complexRed Supergiantsetting, it spontaneously develops a correction term for convective kinetic energy (\u223c\u03c1v 2 r). Conversely, we show that invasive methods such as non- linear probes (Belinkov, 2022a), last-layer fine-tuning (LLFT) (Kirichenko", "source": "observereffect.pdf"}, {"text": "it spontaneously develops a correction term for convective kinetic energy (\u223c\u03c1v 2 r). Conversely, we show that invasive methods such as non- linear probes (Belinkov, 2022a), last-layer fine-tuning (LLFT) (Kirichenko et al., 2023a), and full fine-tuning via Inductive Bias Probes (IBP) (Vafa et al., 2025) can systemat- ically mislead. In the supernova simulation, these methods report high accuracy despite SSL failure. Furthermore, in replicating the orbital mechanics experiment of Vafa et al. (2025), we provide a mechanistic analysis of this destructive intervention. We observe a collapse in representational sim- ilarity (CKA) (Kornblith et al., 2019) in deep blocks, where the optimizer minimizes loss by discarding time-varying features (speed, radius) and relying instead on constants (mass). This shows that physical knowledge", "source": "observereffect.pdf"}, {"text": "(CKA) (Kornblith et al., 2019) in deep blocks, where the optimizer minimizes loss by discarding time-varying features (speed, radius) and relying instead on constants (mass). This shows that physical knowledge was latent in the SSL model but corrupted by the measurement process it- self, analogous to the\u201cobserver effect\u201d(Sassoli de Bianchi, 2013; Heisenberg, 1927). Our contributions: 1. Non-Invasive Physical Probe (PhyIP):We introduce a framework to extract latent physical quantities from frozen representations without inducing distortion. 2. Experimental Design:We derive a bound linking SSL error (\u03f5) and functional curvature (K\u03a6) to linear decod- ability, enabling strict experimental control. 3. Physics Recovery:We successfully recover fundamental laws, including internal energy ( E\u22481.5P ) and grav- itational force ( F\u221d1/r 2), across complex fluid and", "source": "observereffect.pdf"}, {"text": "decod- ability, enabling strict experimental control. 3. Physics Recovery:We successfully recover fundamental laws, including internal energy ( E\u22481.5P ) and grav- itational force ( F\u221d1/r 2), across complex fluid and orbital benchmarks where invasive methods fail. 4. Invasive Corruption Evidence:These results demon- strate that adaptation acts as a destructive intervention that overwrite internal world models. 2. Preliminaries & Framework To distinguish between a model that encodes physics and one that memorizes data, we must formalize the interaction between the physical dynamics, the neural architecture, and the probe as \u201cmeasurement instrument\u201d. Data Generating Process (DGP):We assume the physical system is a time-dependent field u(z, t) on a domain \u2126\u2286 RD evolving via PDEs: \u2202u \u2202t =F(u,\u2207u, . . .) . To", "source": "observereffect.pdf"}, {"text": "Data Generating Process (DGP):We assume the physical system is a time-dependent field u(z, t) on a domain \u2126\u2286 RD evolving via PDEs: \u2202u \u2202t =F(u,\u2207u, . . .) . To align with the discrete nature of computation, this continuous field is discretized into a finite-dimensional state vector x(t)\u2208 X \u2286R n (we also write xt with a subscript to denote the functional dependence on time). Consequently, the evolution follows an autonomous ODE: \u02d9x(t) =f(x(t)) where f:X \u2192R n is the Lipschitz continuous vector field approximating the continuous dynamics. Observational Data:The continuous DGP is observed at discrete time intervals \u2206t, yielding a dataset of trajec- tories T . A trajectory \u03c4\u2208 T is a sequence of states (x0, x1, . .", "source": "observereffect.pdf"}, {"text": "continuous DGP is observed at discrete time intervals \u2206t, yielding a dataset of trajec- tories T . A trajectory \u03c4\u2208 T is a sequence of states (x0, x1, . . . , xT ) where xt+1 =x t+ R t+\u2206t t f(x(s))ds . The learning task is to approximate this transition operator. Physical Observables & Curvature:Let \u03a6 :X \u2192R k be 2 The Observer Effect in World Models a target physical functional defining the quantity of interest s= \u03a6(x) . We assume \u03a6 is C2-smooth and define its Curvature K\u03a6 as the Lipschitz constant of the gradient \u2207\u03a6 with respect to the Euclidean norm: \u2225\u2207\u03a6(xa)\u2212 \u2207\u03a6(x b)\u22252 \u2264K \u03a6\u2225xa \u2212x b\u22252 (1) where xa, xb \u2208 X . K\u03a6 quantifies", "source": "observereffect.pdf"}, {"text": "as the Lipschitz constant of the gradient \u2207\u03a6 with respect to the Euclidean norm: \u2225\u2207\u03a6(xa)\u2212 \u2207\u03a6(x b)\u22252 \u2264K \u03a6\u2225xa \u2212x b\u22252 (1) where xa, xb \u2208 X . K\u03a6 quantifies non-linearity in the in- put space. By Taylor\u2019s theorem, the deviation of \u03a6 from its linear tangent is at most 1 2 K\u03a6\u2225\u2206x\u22252. For linear func- tionals (e.g., Linear Momentum), K\u03a6 = 0. For non-linear interactions (e.g., Gravitational Forces),K \u03a6 >0. Neural Dynamics Model:Let m\u03b8 be a model that maps an input trajectory x0:t to a latent representation ht and a next-step prediction \u02c6xt+1. The model is trained to minimize the self-supervised prediction error : \u03f5=E[L SSL(\u02c6xt+1, xt+1)](2) Where LSSL is typically the Mean Squared Error (MSE), quantifying the model\u2019s", "source": "observereffect.pdf"}, {"text": "a next-step prediction \u02c6xt+1. The model is trained to minimize the self-supervised prediction error : \u03f5=E[L SSL(\u02c6xt+1, xt+1)](2) Where LSSL is typically the Mean Squared Error (MSE), quantifying the model\u2019s ability to approximate the underly- ing physical transition operator. Probe:A probe is a diagnostic function PW :R d \u2192R k with learnable parameters W . It maps thefrozenlatent representation ht to the value of the target physical quantity: st+1 = \u03a6(xt).(3) 2.1. Why a Linear Probe for Physics? While theLinear Representation Hypothesis(DiCarlo & Cox, 2007) is well-documented for neural models trained on language data (Park et al., 2023; Nanda et al., 2023c), this property remains under-explored in models trained on physical systems.We hypothesize that the linear encoding of physical dynamics", "source": "observereffect.pdf"}, {"text": "trained on language data (Park et al., 2023; Nanda et al., 2023c), this property remains under-explored in models trained on physical systems.We hypothesize that the linear encoding of physical dynamics is an emergent capability of a successful self-supervised optimization task. Most effective scientific models (e.g., Transformers, U-Nets, and Fourier Neural Operators) effectively model continuous dynamics via an Euler-like discretization (Haber & Ruthotto, 2017; Chen et al., 2018a). We formalize thisincremental residual predictionproperty as follows: the model predicts a future state as an additive update: the model predicts a future state as an additive update \u02c6xt+1 =x t +g(h t), where g is a learned decoder representing the state displacement. Complementarily, anOrigin-Preserving Decoder: g(0) =0 ensures that a null latent state", "source": "observereffect.pdf"}, {"text": "as an additive update \u02c6xt+1 =x t +g(h t), where g is a learned decoder representing the state displacement. Complementarily, anOrigin-Preserving Decoder: g(0) =0 ensures that a null latent state ( ht =0 ) corresponds to the identity map (no physical update), a property actively encouraged by zero-initialization (Goyal et al., 2017) and weight decay (He et al., 2015). A model m\u03b8 with low error \u03f5 must maintain a representation h that is locally consistent with the physical update \u2206x. We define the optimal linear probe PW \u2217 as the best first- order approximation mapping this latent space to the target quantity update. The expected probe error in recovering the evolution of the physical target quantity\u2206s=s(t+1)\u2212s(t) is bounded by: E", "source": "observereffect.pdf"}, {"text": "best first- order approximation mapping this latent space to the target quantity update. The expected probe error in recovering the evolution of the physical target quantity\u2206s=s(t+1)\u2212s(t) is bounded by: E h |PW \u2217 ht \u2212\u2206s| 2 i | {z } Probe Error \u2264C 1 \u00b7\u03f5|{z} Modeling SSL Error +C 2 \u0002 K2 \u03a6 \u00b7Var(x) \u0003 | {z } Curvature Error +O(\u2206t 4)| {z } Discretization Error (4) See Section B for derivation. This inequality establishes the linear probe as a rigorous diagnostic tool. The error decomposes into two sources:1)Modeling SSL Error ( \u03f5): If the model fails to predict the dynamics (high \u03f5), the probe fails.2)Curvature ( K\u03a6): If the physical dynamics is highly non-linear, a linear approximation naturally suffers.", "source": "observereffect.pdf"}, {"text": "Error ( \u03f5): If the model fails to predict the dynamics (high \u03f5), the probe fails.2)Curvature ( K\u03a6): If the physical dynamics is highly non-linear, a linear approximation naturally suffers. Crucially, physical dynamics can be non-linear; for instance, in orbital mechanics, the force vector \u2207\u03a6(x), where x is a position trajectory, rotates as the planet moves. While a time- dependent probe (one that \u201crotates\u201d its weights to match local gradients) might achieve lower error, it risks approxi- mating the physics via its own capacity rather than measur- ing the representation (Hewitt & Liang, 2019b). Therefore, the probe optimal W \u2217 must be a single constant matrix across all time steps. A probe success under these con- straints serves as a", "source": "observereffect.pdf"}, {"text": "(Hewitt & Liang, 2019b). Therefore, the probe optimal W \u2217 must be a single constant matrix across all time steps. A probe success under these con- straints serves as a possiblelitmus testfor evaluation. Takeaway 1: Probes as Fixed Instruments The measurement instrument must be afixed, time- invariant linear operatortargeting the next state. Consequently, a successful readout under this strict condition serves as evidence that the SSL model has transformed the complex non-linear dynamics into a linearizable representation, consistent with the error bound derived in Eq. 4. 2.2.PhyIP: Non-Invasive Probe for Physics The objective of our framework is to probe the internal geometry of a pre-trained SSL model m\u03b8 on continuous physical trajectories without corrupting its learned represen- tation. This three-stage", "source": "observereffect.pdf"}, {"text": "Physics The objective of our framework is to probe the internal geometry of a pre-trained SSL model m\u03b8 on continuous physical trajectories without corrupting its learned represen- tation. This three-stage process is summarized in Figure 1. Feature Extraction and Linear Probing.To probe the geometry of the activation space, H \u2286R d, we train a linear probe, PW :H \u2192 Q , where Q \u2286R k is the space of the physical quantity. The target dimension k\u22651 depends on the quantity being probed; for example, k= 1 for a scalar (such as force magnitude) or k= 2 for a 2D vector (such as the force \u20d7F= (F x, Fy)). Here, h(t) is the internal activation vector (e.g., from the decoder block)", "source": "observereffect.pdf"}, {"text": "as force magnitude) or k= 2 for a 2D vector (such as the force \u20d7F= (F x, Fy)). Here, h(t) is the internal activation vector (e.g., from the decoder block) of the pre- trained SSL model, m\u03b8. This activation is the result of the model processing Section 2.1, h(t) represents the model\u2019s internal \u201cplan\u201d to execute the update. Following Section 2.1, 3 The Observer Effect in World Models Input Physical system Target A quantity derived from a physical law not explicitly seen in training Self-Supervised Pre-trained Model Output Hidden Futures Output Predicted Physical State Formula Discovered formula tested as model in OOD scenarios Input Physical state variable Evolutionary Algorithm Extract Frozen Features Probe Equation DiscoveryFeature Extraction \u2744 Figure 1.Overview ofPhyIPframework.", "source": "observereffect.pdf"}, {"text": "Output Predicted Physical State Formula Discovered formula tested as model in OOD scenarios Input Physical state variable Evolutionary Algorithm Extract Frozen Features Probe Equation DiscoveryFeature Extraction \u2744 Figure 1.Overview ofPhyIPframework. (1) Feature Extraction:We extract frozen activations, h(t) from a SSL model.(2) Linear Probing:A linear probe is trained to predict a new physical quantity, \u02c6s(t+ 1) =W h(t) +b. Its success on OOD tests indicates that \u02c6sis linearly encoded.(3) Equation Discovery & Validation:SR translates the probe\u2019s function into a symbolic formula, \u02c6\u03a6SR, and tests it on OOD data to confirm its physical plausibility. we probe \u2206st. The target is the physical state at the next time step,\u02c6s(t+ 1), via the transformation: \u02c6s(t+ 1) =W h(t;m\u03b8) +b(5) \u02c6s(t+ 1)\u2208 Q \u2286R k", "source": "observereffect.pdf"}, {"text": "physical plausibility. we probe \u2206st. The target is the physical state at the next time step,\u02c6s(t+ 1), via the transformation: \u02c6s(t+ 1) =W h(t;m\u03b8) +b(5) \u02c6s(t+ 1)\u2208 Q \u2286R k is the predicted quantity. The linear mapping matrix W\u2208R k\u00d7d and the bias vector b\u2208R k transform the high-dimensional representation h(t)\u2208 H \u2286 Rd into the low-dimensional spaceQ. The probe\u2019s optimal parameters, (W \u2217, b\u2217), are found by minimizing a loss function (e.g., MSE) on a training set Dtrain ={(h i, s)}N i=1. Here, the index i iterates over the N samples in the dataset and does not represent the time t. Each sample i is a pair created from a specific time step t in the simulation, such that", "source": "observereffect.pdf"}, {"text": "N samples in the dataset and does not represent the time t. Each sample i is a pair created from a specific time step t in the simulation, such that hi =h(t) (the model activation at time t) and si =s(t+1) (the ground-truth physical quantity at thenexttime step): (W \u2217, b\u2217) = arg min W,b 1 N NX i=1 \u2225si \u2212(W h i +b)\u2225 2 (6) This loss function minimizes the Euclidean distance be- tween the ground-truth quantitysi and the probe\u2019s linear pre- diction \u02c6si. The optimization process is constrained to learn only W and b (the linear map parameters), while the com- plex, nonlinear feature extraction of the underlying model m\u03b8 (which producesh i) remains frozen. The probe\u2019s success", "source": "observereffect.pdf"}, {"text": "to learn only W and b (the linear map parameters), while the com- plex, nonlinear feature extraction of the underlying model m\u03b8 (which producesh i) remains frozen. The probe\u2019s success is then quantified by its generalization performance on an (OOD) test set (DOOD). For our physical systems, DOOD consists of simulations where key generative parameters (e.g., central star mass, gravitational constant, initial velocity, or boundary conditions) are sampled outside the distribution used for the self-supervised pre-training. A success here confirms that the linear encoding is a robust physical invariant, not a memorized correlation. Equation Discovery.While an OOD-generalizing probe confirms a meaningful geometry, its learned mapping W remains opaque. To decode this geometry, we employ Sym- bolic Regression (SR). This step", "source": "observereffect.pdf"}, {"text": "a memorized correlation. Equation Discovery.While an OOD-generalizing probe confirms a meaningful geometry, its learned mapping W remains opaque. To decode this geometry, we employ Sym- bolic Regression (SR). This step treats the probe\u2019s output as the ground truth. The SR algorithm is given the physical state variables xi (e.g., position, pressure) as inputs and the probe\u2019s predictions \u02c6si as targets. SR searches a space of symbolic expressionsGfor an optimal formula \u02c6\u03a6\u2217 SR: \u02c6\u03a6\u2217 SR = arg min \u02c6\u03a6\u2208G 1 N NX i=1 \u2225\u02c6si \u2212 \u02c6\u03a6(xi)\u22252 (7) To ensure \u02c6\u03a6\u2217 SR represents a true physical principle, we treat it as a standalone physical hypothesis and evaluate its zero- shot generalization on DOOD. A low loss on unseen simula- tion parameters serves", "source": "observereffect.pdf"}, {"text": "represents a true physical principle, we treat it as a standalone physical hypothesis and evaluate its zero- shot generalization on DOOD. A low loss on unseen simula- tion parameters serves as robust validation that the original modelm \u03b8 successfully encoded the governing law. 3. Probing Complex Fluid Dynamics Systems Setup:To demonstrate the generality ofPhyIP, we applied it to three high-dimensional fluid dynamics simulations from the TheWell benchmark (Ohana et al., 2024): a2D Turbulent Radiative Layer(Stachenfeld et al., 2021), a3D Red Supergiant Convective Envelope(Goldberg et al., 2022), and a3D Supernova Explosion(Hirashima et al., 2023). The experimental setting satisfies the conditions of Section 2.1. We tested neural models\u2014including U-Net (Ronneberger et al., 2015), UNetConvNext (Liu et al., 2022), FNO (Li et al.,", "source": "observereffect.pdf"}, {"text": "et al., 2023). The experimental setting satisfies the conditions of Section 2.1. We tested neural models\u2014including U-Net (Ronneberger et al., 2015), UNetConvNext (Liu et al., 2022), FNO (Li et al., 2021b), and TFNO (Li et al., 2021a)\u2014trained solely on self-supervised next-state prediction. These models function as residual predictors, satisfying the setting discussed in Section 2. Using our non-invasive methodPhyIP, a linear probe was trained on frozen ac- tivations (e.g., U-Net neck). To predict the global total internal energy Eint, we implemented the probe as a 1\u00d71 Convolutional Layer (kernel size 1) followed by a global summation. This forces the probe to predict the lo- cal energy density contribution (\u03c1u)i,j at each voxel (i, j) using only the local latent vector", "source": "observereffect.pdf"}, {"text": "1) followed by a global summation. This forces the probe to predict the lo- cal energy density contribution (\u03c1u)i,j at each voxel (i, j) using only the local latent vector hi,j. The final prediction is the sum over the domain: \u02c6Eint =P i,j Probe(hi,j)\u2206V . To distill these probes into interpretable formulas, we re- stricted the PySR search space to basic arithmetic operators {+,\u2212,\u00d7, /} with strict dimensional consistency constraints 4 The Observer Effect in World Models Table 1.Comprehensive Probe Analysis on Fluid Dynamics.OOD evaluation of internal energy recovery. PhyIP (top) reliably extracts physical laws from frozen representations when the SSL error (\u03f5) is low, recovering the ideal gas law (E\u22481.5P ) and kinetic corrections (\u03c1v2 r). The SN-3D results", "source": "observereffect.pdf"}, {"text": "(top) reliably extracts physical laws from frozen representations when the SSL error (\u03f5) is low, recovering the ideal gas law (E\u22481.5P ) and kinetic corrections (\u03c1v2 r). The SN-3D results demonstrate that invasive probes (bottom) can hide collapse, whereas PhyIP correctly identifies it. 2D Turbulent Layer(N= 9OOD)3D Red Supergiant(N= 3OOD) 3D Supernova(N= 27OOD) Probe Task SSL Task Probe Task SSL Task Probe Task SSL Task Model / Method MAPE\u2193\u03c1\u2191\u03f5 OOD\u2193 MAPE\u2193\u03c1\u2191\u03f5 OOD\u2193 MAPE\u2193\u03c1\u2191\u03f5 OOD\u2193 I. PhyIP (Section 2.2) UNetConvNext 36.9\u00b11.20.83\u00b10.020.20\u00b10.01 18.2\u00b10.90.91\u00b10.010.02\u00b10.00 140.4\u00b112.10.15\u00b10.050.30\u00b10.02 UNetClassic 42.3\u00b12.10.71\u00b10.040.26\u00b10.02 25.6\u00b11.50.69\u00b10.030.09\u00b10.01 135.9\u00b110.50.08\u00b10.020.40\u00b10.03 FNO 76.0\u00b15.30.61\u00b10.060.49\u00b10.04 95.1\u00b14.20.22\u00b10.080.05\u00b10.01 95.3\u00b18.10.18\u00b10.040.36\u00b10.02 TFNO 89.7\u00b16.10.67\u00b10.050.58\u00b10.05 92.5\u00b15.00.25\u00b10.070.04\u00b10.01 92.1\u00b17.80.21\u00b10.030.36\u00b10.03 II. Baselines & Probes (i) Linear Probe onRaw Inputs 58.5\u00b14.10.32\u00b10.05- 88.2\u00b13.50.25\u00b10.04- 142.1\u00b115.00.22\u00b10.01- (ii)Time-DependentProbe 22.1\u00b11.00.88\u00b10.02- 15.4\u00b10.80.95\u00b10.01- 93.2\u00b111.50.64\u00b10.05- (iii)MLP Probe (Belinkov, 2022a)37.2\u00b11.50.72\u00b10.03- 19.1\u00b11.20.82\u00b10.02- 125.5\u00b114.20.48\u00b10.06- (iv)LL-FT (Kirichenko et al.,", "source": "observereffect.pdf"}, {"text": "TFNO 89.7\u00b16.10.67\u00b10.050.58\u00b10.05 92.5\u00b15.00.25\u00b10.070.04\u00b10.01 92.1\u00b17.80.21\u00b10.030.36\u00b10.03 II. Baselines & Probes (i) Linear Probe onRaw Inputs 58.5\u00b14.10.32\u00b10.05- 88.2\u00b13.50.25\u00b10.04- 142.1\u00b115.00.22\u00b10.01- (ii)Time-DependentProbe 22.1\u00b11.00.88\u00b10.02- 15.4\u00b10.80.95\u00b10.01- 93.2\u00b111.50.64\u00b10.05- (iii)MLP Probe (Belinkov, 2022a)37.2\u00b11.50.72\u00b10.03- 19.1\u00b11.20.82\u00b10.02- 125.5\u00b114.20.48\u00b10.06- (iv)LL-FT (Kirichenko et al., 2023b)32.5\u00b12.80.80\u00b10.04- 23.4\u00b14.10.80\u00b10.05- 65.0\u00b113.10.59\u00b10.04- (v)Full FT (IBP (Vafa et al., 2025))41.2\u00b13.50.81\u00b10.04- 21.1\u00b16.40.80\u00b10.03- 18.3\u00b118.50.71\u00b10.01- SSL Input Vars {\u03c1, P, vx, vy} {\u03c1, P, vr, v\u03b8, v\u03d5} {\u03c1, P, T, vx, vy, vz} Probe Target Eint =R 1.5P dV(\u03b3= 5/3) Eint =R \u03c1u dV Eint =R \u03c1u dV Discovered Law(\u02c6\u03a6SR) E\u22481.48\u00b7P E\u22481.45P+0.42\u03c1v2r E\u22480.35\u2212 h 0.06 (P+0.2) i (penalty10 12) to prioritize simplicity over curve fitting. Control Baselines & Invasive Probes.All probes are trained using supervised pairs derived exclusively from the in-Distribution (ID) SSL training set, ensuring strictly zero- shot evaluation on OOD regimes.", "source": "observereffect.pdf"}, {"text": "curve fitting. Control Baselines & Invasive Probes.All probes are trained using supervised pairs derived exclusively from the in-Distribution (ID) SSL training set, ensuring strictly zero- shot evaluation on OOD regimes. We compare against:i) Raw Input Baseline: A linear regression on flattened raw input fields xt \u2208R C\u00d7H\u00d7W .ii)Time-Dependent Probe: A linear probe {Wt} optimized per time-step removing the curvature penalty (K\u03a6) from our bound to quantify \u201ccur- vature mismatch.\u201diii)Non-Linear MLP Probe (Belinkov, 2022a): A 3-layer MLP (254\u219232) on frozen activations ht. Finally, we test aiv)Last-Layer Fine-Finetuning (LL- FT) (Kirichenko et al., 2023b) andv)Full Fine-tuning Adap- tation (all \u03b8) via the IBP (Vafa et al., 2025). Success here despite high SSL error ( \u03f5) confirms the probe islearning from scratchrather than", "source": "observereffect.pdf"}, {"text": "al., 2023b) andv)Full Fine-tuning Adap- tation (all \u03b8) via the IBP (Vafa et al., 2025). Success here despite high SSL error ( \u03f5) confirms the probe islearning from scratchrather than measuring the world model. OOD Evaluation.We validated on 39 Out-of-Distribution (OOD) test sets, 9 unseen cooling rates for TRL-2D, 3 dis- tinct stellar evolution phases for RSG-3D, and 27 novel environments varying ambient gas density and metallicity for Supernova. Results summarized in Table 1 reveal a di- vergence that provides empirical validation for Section 2.1. For the2D Turbulent Radiative Layer, the U-Net architec- tures minimized the SSL test error (\u03f5\u22480.19 ) significantly better than FNO models (\u03f5\u22480.50 ) with thePhyIPlinear encoding \u03c1= 0.83 and MAPE = 36.9.PhyIPsymbolic regression recovered the", "source": "observereffect.pdf"}, {"text": "the U-Net architec- tures minimized the SSL test error (\u03f5\u22480.19 ) significantly better than FNO models (\u03f5\u22480.50 ) with thePhyIPlinear encoding \u03c1= 0.83 and MAPE = 36.9.PhyIPsymbolic regression recovered the equation E\u22481.48\u00b7P , which matches the constant ( 1.5 for \u03b3= 5/3 ) with <2% er- ror. This success is visually confirmed in Figure 2 (top), where predictions form a linear cluster, and in Figure 2 (bot- tom), where the symbolic model (green) of UNetConvNext accurately forecasts the energy decay on unseen cooling rates. In contrast, higher-capacity or invasive baselines do not improve upon the linear readout: the MLP probe reaches \u03c1= 0.72 (MAPE = 37.2%), and both last-layer fine-tuning and full fine-tuning perform slightly worse thanPhyIPon OOD. The3D Red", "source": "observereffect.pdf"}, {"text": "do not improve upon the linear readout: the MLP probe reaches \u03c1= 0.72 (MAPE = 37.2%), and both last-layer fine-tuning and full fine-tuning perform slightly worse thanPhyIPon OOD. The3D Red Supergiantsimulation provides the strongest validation. The UNetConvNext achieved the lowest OOD prediction error ( \u03f5= 0.0201 ), outperforming the FNO (\u03f5= 0.0505 ). As expected, this SSL mastery enabled pre- cisePhyIP\u2019s linear decoding ( \u03c1= 0.91 ). Here,PhyIP shows that rather than just retrieving the static pressure law (1.5P ), the probe recovered a composite symbolic ex- pression E\u22481.45P+0.42\u03c1v 2 r . We validate this term via dimensional analysis, the quantity \u03c1v2 has the units of en- ergy density (J/m3), matching the units of Pressure ( P ). This confirms", "source": "observereffect.pdf"}, {"text": "r . We validate this term via dimensional analysis, the quantity \u03c1v2 has the units of en- ergy density (J/m3), matching the units of Pressure ( P ). This confirms that the term is a valid correction for kinetic energy. As shown in Figure 2 (bottom), this \u201cconvective proxy\u201d allows the symbolic formula of UNetConvNext to track the ground-truth energy evolution, while the scatter plot (Section 3, Center) confirms the precision of thePhyIP decoding.PhyIPprobe reduces MAPE error by nearly 5\u00d7 5 The Observer Effect in World Models 60000 65000 70000 True Value (T otal Energy) 60000 65000 70000Probe Predicted Value 2D T urbulent Layer (trl2D) 200000 300000 400000 500000 True Value (T otal Energy) 200000 300000 400000 500000Probe Predicted Value", "source": "observereffect.pdf"}, {"text": "True Value (T otal Energy) 60000 65000 70000Probe Predicted Value 2D T urbulent Layer (trl2D) 200000 300000 400000 500000 True Value (T otal Energy) 200000 300000 400000 500000Probe Predicted Value 3D Red Supergiant (cer3D) 20000 0 20000 40000 True Value (T otal Energy) 20000 0 20000 40000 Probe Predicted Value 3D Supernova (sn3D) UNetConvNext UNetClassic FNO TFNO y=x (Perfect Prediction) 0 25 75 10050 0 25 75 10050 72500 70000 67500 65000 62500 TFNOUNetConvNext 500000 400000 300000 200000 0 25 75 10050 0 25 75 10050 Turbulent Layer (trl2D) Red Supergiant (cer3D) Supernova (sn3D) Internal Energy T ime step TFNOUNetConvNext 25 75 10050 0 25 75 10050 40000 20000 0 0 TFNOUNetConvNext T ime step T ime step M e", "source": "observereffect.pdf"}, {"text": "Supergiant (cer3D) Supernova (sn3D) Internal Energy T ime step TFNOUNetConvNext 25 75 10050 0 25 75 10050 40000 20000 0 0 TFNOUNetConvNext T ime step T ime step M e a n T r u e M e a n P r o b e M e a n S R Figure 2.Probing Latent Physical Laws. (Top)The Non-Invasive Probe successfully extracts Total Internal Energy from frozen SSL representations in TRL-2D and RSG-3D (linear alignment), but correctly identifies representational collapse in the SN-3D experiment. (Bottom)Zero-shot generalization. Discovered symbolic formulas (Mean SR) accurately predict energy dynamics on unseen simulation parameters for the successful models compared to the Raw Input Baseline (\u03c1= 0.25 and MAPE = 88.2%), proving high-level abstraction. The MLP Probe", "source": "observereffect.pdf"}, {"text": "accurately predict energy dynamics on unseen simulation parameters for the successful models compared to the Raw Input Baseline (\u03c1= 0.25 and MAPE = 88.2%), proving high-level abstraction. The MLP Probe (\u03c1= 0.82 and MAPE = 19.1%) together with LL-FT and IBP fails to improve uponPhyIP, verifying that the relevant geometry is linearly encoded. Finally, by using the Time- Dependent Probe we gain better performance (\u03c1= 0.95 and MAPE= 15.4%) as expected. Finally, the3D Supernovatest proved challenging for all architectures, with SSL prediction error at a high \u03f5\u2248 0.30\u22120.37. Empirically, no model when probed withPhyIP achieved a correlation above \u03c1= 0.2 . Furthermore,PhyIP symbolic regression of the best performer model TFNO failed to find any physical law, fitting instead a spurious", "source": "observereffect.pdf"}, {"text": "model when probed withPhyIP achieved a correlation above \u03c1= 0.2 . Furthermore,PhyIP symbolic regression of the best performer model TFNO failed to find any physical law, fitting instead a spurious ra- tional function E\u22480.35\u2212[ 0.06 (P+0.2)] that implies unphys- ical inverse scaling. This failure is visualized in Figure 2, where both the probe and symbolic formula completely fail to track the energy decay.This failure exposes the danger of invasive probing:whilePhyIPprobe correctly diagnoses this (MAPE = 140.4%), the full fine-tuning via IBP (Table 1, row v) achieves a deceptively low MAPE = 18.3% and \u03c1= 0.71 . This massive discrepancy ( 140%\u219218% for MAPE and 0.21\u21920.71 ) confirms that the invasive probes did not measure the model\u2019s knowledge but rather overwrote", "source": "observereffect.pdf"}, {"text": "= 18.3% and \u03c1= 0.71 . This massive discrepancy ( 140%\u219218% for MAPE and 0.21\u21920.71 ) confirms that the invasive probes did not measure the model\u2019s knowledge but rather overwrote it, hallucinating competence where there was none. Takeaway 2: Strict Experimental Control In complex fluids, faithful evaluation requiresstrict experimental control. PhyIP succeeds only when SSL models the dynamics (low OOD \u03f5), while inva- sive adaptation canmaskbackbone failure (SN-3D: MAPE140%\u219218% ) by learning the probe task itself and distorting otherwise valid linear structure (as seen in TRL-2D and RSG-3D). 4. The Confounding Nature of Invasive Probes Motivation.While the Fluid Dynamics experiment (Sec- tion 3) demonstrate scalability, it is difficult to isolate the exact mechanism of invasive corruption. To provide a con-", "source": "observereffect.pdf"}, {"text": "Confounding Nature of Invasive Probes Motivation.While the Fluid Dynamics experiment (Sec- tion 3) demonstrate scalability, it is difficult to isolate the exact mechanism of invasive corruption. To provide a con- trollable test, we replicate the Orbital Mechanics experiment of Vafa et al. (2025) for theinductive bias probe. This setting allows us to:(1)directly comparePhyIPagainst invasive methods in a highly nonlinear task (force vector) and(2) perform a mechanistic analysis to point exactly when and how invasive probes overwrite knowledge. Setup:A 109M parameter Transformer ( m\u03b8), pre-trained (SSL) on orbital trajectories, is subjected to full-parameter fine-tuning on a small datasets where the output is the force vector at each point in the trajectory. The weights of the entire architecture \u03b8 are updated to", "source": "observereffect.pdf"}, {"text": "to full-parameter fine-tuning on a small datasets where the output is the force vector at each point in the trajectory. The weights of the entire architecture \u03b8 are updated to \u03b8\u2032 i by minimizing the 6 The Observer Effect in World Models Physical System Components whose dynamics are governed by physical law SSL Model Manifold linearly encodes accurate vector forces Fine-tuned model Manifold collapse destroys vector force encoding SSL (e.g, next state prediction) Adaptation to new task (e.g, vector force prediction) 1 6 11 16 21 26 31 36 41 46 1 0 1 p Pearson Correlation ( is Better) 1 6 11 16 21 26 31 36 41 46 10 50 100 150% Error Magnitude MAPE (%) ( is", "source": "observereffect.pdf"}, {"text": "46 1 0 1 p Pearson Correlation ( is Better) 1 6 11 16 21 26 31 36 41 46 10 50 100 150% Error Magnitude MAPE (%) ( is Better) Test set Our linear probe Fine-Tuning Figure 3.The Failure of Invasive Probing. Top:The orderly geometry of the SSL model (center) is destroyed by invasive fine-tuning (right), dropping correlation from \u03c1= 0.94\u2192 \u22120.03 .Bottom:Quantitative Impact. This geometric destruction causes the invasive probe (Green) to fail erratically on OOD task, whereas our non-invasive probe (Blue) remains robust (\u03c1 >0.8). task loss over a fine-tuning dataset Dtask,i. For each pair (x, s), x represents the input trajectory and s the ground- truth physical target variable, the force vector \u03b8\u2032 i = arg", "source": "observereffect.pdf"}, {"text": "over a fine-tuning dataset Dtask,i. For each pair (x, s), x represents the input trajectory and s the ground- truth physical target variable, the force vector \u03b8\u2032 i = arg min \u03b8 X (x,s)\u2208Dtask,i Ltask(m\u03b8(x), s)(8) The subsequent inductive bias analysis is performed on the fine-tuned modelm \u03b8\u2032 i . Representation Collapse:We investigate whether inva- sive fine-tuning acts as adestructive interventionthat de- grades the SSL model\u2019s ( m\u03b8) latent geometry. To estab- lish a robust baseline, we employ the non-invasive probe PhyIP(Section 2.2) on the frozen activations of the decoder blocks.9. We comparePhyIPagainst the fully adapted fine-tuned model via IBP. As shown in Figure 3, our probe maintains high performance ( \u03c1 >0.85 , MAPE <30% ) across 50 OOD", "source": "observereffect.pdf"}, {"text": "blocks.9. We comparePhyIPagainst the fully adapted fine-tuned model via IBP. As shown in Figure 3, our probe maintains high performance ( \u03c1 >0.85 , MAPE <30% ) across 50 OOD tests, while the fine-tuned model exhibits erratic failure with MAPE spiking between 50% and 150% (see Appendix E for solar system validation results). Table 2 confirms the non-triviality of the task: the Raw Input Probe fails (MAPE 65.2%). The Time-Dependent probe attains low error (MAPE 12.1% and \u03c1= 0.96 ) by adapting to local gradients, effectively bypassing the curvature term K\u03a6 in Equation (4). Introducing limited adaptation with LL-FT partially mitigates the degradation observed with the IBP (MAPE 45.3%, \u03c1= 0.65 ), but it still performs substantially worse thanPhyIP(MAPE24.5%,\u03c1= 0.91).", "source": "observereffect.pdf"}, {"text": "K\u03a6 in Equation (4). Introducing limited adaptation with LL-FT partially mitigates the degradation observed with the IBP (MAPE 45.3%, \u03c1= 0.65 ), but it still performs substantially worse thanPhyIP(MAPE24.5%,\u03c1= 0.91). We selected blocks.9 via a systematic analysis per block on the 50 OOD test set (Figure 4). While physical infor- mation is initially entangled, requiring the non-linear MLP (d\u2192254\u21921 ) probe to extract it, the representation \u2019lin- earizes\u2019 with depth, reaching maximal disentanglement at blocks.9(\u03c1\u22480.91and MAPE\u22480.25). Table 2.Orbital Mechanics Baseline Analysis.Comparison of our Non-Invasive Probe against baselines on the 50 OOD test set (force vector task). OOD test sets Method MAPE\u2193\u03c1(Pearson)\u2191 I. Non-Invasive Probe (PhyIP) 24.5\u00b14.2 0.91\u00b10.02 II. Baselines & Controls (i) Linear Probe onRaw Inputs 65.2\u00b14.50.45\u00b10.03 (ii)Time-DependentProbe 12.1\u00b11.20.96\u00b10.01 (iii)MLP", "source": "observereffect.pdf"}, {"text": "OOD test set (force vector task). OOD test sets Method MAPE\u2193\u03c1(Pearson)\u2191 I. Non-Invasive Probe (PhyIP) 24.5\u00b14.2 0.91\u00b10.02 II. Baselines & Controls (i) Linear Probe onRaw Inputs 65.2\u00b14.50.45\u00b10.03 (ii)Time-DependentProbe 12.1\u00b11.20.96\u00b10.01 (iii)MLP Probe 22.5\u00b13.00.88\u00b10.04 (iv)LL-FT 45.3\u00b15.10.65\u00b10.06 (v)Full FT (IBP) 81.5\u00b112.40.05\u00b10.35 Mechanistic Analysis:Figure 3 visualizes the mechanism of this failure. Using 2D PaCMAP projections of activation manifolds (methodology in Section D), we observe that the SSL model ( m\u03b8) encodes a gradient of ground-truth force magnitudes. In contrast, the fine-tuned model ( m\u03b8\u2032) exhibits a modified manifold, confirming the modification of geometric structure. This collapse is driven by parameter shifts in the decoder\u2019s attention and MLP layers (Section F). To quantify this, we analyze layer-wise activations h(l) \u2208 RB\u00d7T\u00d7d for OOD test trajectories", "source": "observereffect.pdf"}, {"text": "This collapse is driven by parameter shifts in the decoder\u2019s attention and MLP layers (Section F). To quantify this, we analyze layer-wise activations h(l) \u2208 RB\u00d7T\u00d7d for OOD test trajectories X , where B is batch size, Tsequence length, andd= 768. We measureParameter Modifications, quantified by the rel- ative Frobenius norm of the weight change for each block l : \u03b4(l) = \u2225\u03b8\u2032(l)\u2212\u03b8(l)\u2225F \u2225\u03b8(l)\u2225F as Guo et al. (2020) shows. Second, we assessRepresentational Driftusing Linear Centered Kernel Alignment (CKA) (Kornblith et al., 2019), which computes the s(l) CKA. Finally, we identify the specific phys- ical concepts erased by this drift. We isolate the subset of neurons S(l) exhibiting the top 20% of parameter modifica- tions \u03b4j =\u2225w \u2032 j \u2212w", "source": "observereffect.pdf"}, {"text": "identify the specific phys- ical concepts erased by this drift. We isolate the subset of neurons S(l) exhibiting the top 20% of parameter modifica- tions \u03b4j =\u2225w \u2032 j \u2212w j\u22252 within the MLP projection layer. 7 The Observer Effect in World Models 0.2 0.4 0.6 0.8 1.0 Pearson ( ) Our Linear Probe MLP Probe 1 2 3 4 5 6 7 8 9 10 Transformer Block 0.00 0.25 0.50 0.75 1.00 MAPE Figure 4.Probe Analysis per BlockLayer-wise performance of Non-Invasive (PhyIP, Blue) vs. Non-Linear (MLP, Green) probes. While early layers MLP perform better than linear, the representation spontaneouslylinearizesin deep layers, peaking at Block 9 (\u03c1\u22480.91). We then compute the shift in the maximum Pearson correla- tion \u2206\u03c1k between", "source": "observereffect.pdf"}, {"text": "early layers MLP perform better than linear, the representation spontaneouslylinearizesin deep layers, peaking at Block 9 (\u03c1\u22480.91). We then compute the shift in the maximum Pearson correla- tion \u2206\u03c1k between the flattened activation history hj \u2208R N and ground-truth physical vectors\u03d5 k \u2208R N : \u2206\u03c1k = max j\u2208S (l) corr(h\u2032 j,\u03d5 k) \u2212max j\u2208S (l) |corr(hj,\u03d5 k)|(9) Applying these metrics reveals a distinct chain of corruption (Figure 5). Structural modifications are highly localized, while the global average is low, the attention and MLP layers of deep blocks (B5\u2013B10) undergo significant shifts (\u03b4(l) \u2248 0.10). This structural modification directly drives functional collapse: while early layers remain stable ( sCKA \u22481.0 ), the representations in these modified deep blocks diverge successfully", "source": "observereffect.pdf"}, {"text": "significant shifts (\u03b4(l) \u2248 0.10). This structural modification directly drives functional collapse: while early layers remain stable ( sCKA \u22481.0 ), the representations in these modified deep blocks diverge successfully (sCKA <0.2). The results in Figure 5 show a targeted erasure of dynamic invariants: encoding forSpeedandRadiusdrops signifi- cantly (\u2206\u03c1\u2248 \u22120.15 ), while static variables likeMassre- main untouched. This confirms that fine-tuning specifically modifies parameters responsible for Hamiltonian dynamics (K \u221dv 2,1/r ) to minimize error on the narrow distribution. Fine-tuning Dataset Analysis:The effectiveness of fine- tuning is heavily reliant on the dataset representativeness used for adaptation (Kumar et al., 2022). By replicating the dataset as in (Vafa et al., 2025), Figure 6 shows that the data for the Star Mass", "source": "observereffect.pdf"}, {"text": "the dataset representativeness used for adaptation (Kumar et al., 2022). By replicating the dataset as in (Vafa et al., 2025), Figure 6 shows that the data for the Star Mass (m2) is a single spike at 1.0 (Green). These discrepancies indicate that the fine-tuning dataset does not provide sufficient diversity to maintain the model\u2019s general understanding of physics. This lack of support directly drives the optimizer to erase the now-unnecessary dynamic quantities (speed, radius) in favor of static shortcuts suitable for the narrow distribution (See to Section G for Force VectorsandForce Magnitudedistribution analysis). Symbolic Validation of Discovered Physics:We apply SR to distill the non-intrusive probe and IBP output into interpretable formulas as in Figure 1. When evaluated them Speed Radius", "source": "observereffect.pdf"}, {"text": "VectorsandForce Magnitudedistribution analysis). Symbolic Validation of Discovered Physics:We apply SR to distill the non-intrusive probe and IBP output into interpretable formulas as in Figure 1. When evaluated them Speed Radius Force S. Mass Mass Momntm 0.2 0.1 0.0 0.1 0.2 (Erasure) B0 B1 B2 B3 B4 B5 B6 B7 B8 B9 B10 B11 Transformer Blocks 0.0 0.5 1.0CKA CKA Param 0 5 10 Param (%) Figure 5.Mechanistic Origins of Erasure. Top:Heatmap of linear decodability change (\u2206\u03c1). Fine-tuning selectively erases dynamic variables (Speed, Radius) while preserving static one (Mass).Bottom:This collapse is driven by a parameter change causing a drop in representational similarity (CKA). 0.0 0.5 1.0 1.5 2.0 2.5 Mass (m2) 0 1 Density Star Mass Distribution (m2) Pre-train (SSL) Fine-tune", "source": "observereffect.pdf"}, {"text": "driven by a parameter change causing a drop in representational similarity (CKA). 0.0 0.5 1.0 1.5 2.0 2.5 Mass (m2) 0 1 Density Star Mass Distribution (m2) Pre-train (SSL) Fine-tune Figure 6.Narrow Data.Comparison of Star Mass ( m2) distri- butions. While the SSL pre-training data (Grey) covers a diverse physical range, the fine-tuning dataset (Green dashed) is a single point mass atm 2 = 1.0. as a physical model on the 50 OOD orbital test sets (force vector task). Figure 7 shows non-invasive probe formula tracks the ground truth with high precision, whereas the formula extracted from the IBP remains erratic. The truth is the Newton Law (Newton, 1687): F\u221d m1m2 r2 The formula discovered by the IBP (Vafa et", "source": "observereffect.pdf"}, {"text": "high precision, whereas the formula extracted from the IBP remains erratic. The truth is the Newton Law (Newton, 1687): F\u221d m1m2 r2 The formula discovered by the IBP (Vafa et al., 2025) is dominated by artifacts:F\u221d h sin \u0010 1 sin(r\u22120.2) \u0011 + 1.5 i | {z } Hallucinated Artifacts \u00b7 1 r\u22121 +m 2| {z } Distorted Decay . In contrast, the Non-Invasive Probe recovers the signal: F\u2248P(r, m 2)| {z } Residual Noise + 1 r2 |{z} Recovered Law . Although the non-invasive probe formula is still an approxi- mation, it discovers a distinct additive term where the resid- ual P(r, m2) vanishes, recovering the inverse-square law F\u22481/r 2. The full list of discovered formulas is available in", "source": "observereffect.pdf"}, {"text": "approxi- mation, it discovers a distinct additive term where the resid- ual P(r, m2) vanishes, recovering the inverse-square law F\u22481/r 2. The full list of discovered formulas is available in Appendix H (Table 4). 8 The Observer Effect in World Models 0 2 4 6 8 Residual (Fpred \u2212 Ftrue) 101 102 103 104 Count (Log Scale) Fine-Tuning (SR) Linear Probe (SR) Zero Error Figure 7.Symbolic Validation.Distribution of prediction errors on OOD data. Our non-invasive probe (Blue) achieves high preci- sion compared to the erratic invasive baseline (Green). Mechanistic Confirmation.The successful recovery of the 1/r2 term provides functional validation for the mechanistic analysis in Section 4. As visualized in the erasure heatmap (Figure 5), the neurons encoding Radius (r) were", "source": "observereffect.pdf"}, {"text": "successful recovery of the 1/r2 term provides functional validation for the mechanistic analysis in Section 4. As visualized in the erasure heatmap (Figure 5), the neurons encoding Radius (r) were preserved in the SSL model butspecifically erasedduring fine-tuning. Because the fine-tuned model lost the internal representation of r, it physicallycould notexpress the correct inverse- square law, defaulting to heuristics. Takeaway 3: Erasure of Encoded Dynamics Invasive probes do not merely measure inductive bias\u2014they overwrite it: fine-tuning induces repre- sentational drift that selectivelyerases dynamic state variables(e.g., speed, radius) to fit narrow downstream data. Reliable world-model evaluation must preserve the backbone. 5. Conclusion AI for scientific discovery has reached a critical stage (Wang et al., 2023). As models scale, the challenge shifts", "source": "observereffect.pdf"}, {"text": "downstream data. Reliable world-model evaluation must preserve the backbone. 5. Conclusion AI for scientific discovery has reached a critical stage (Wang et al., 2023). As models scale, the challenge shifts from training to correctly interpreting the latent knowledge they have internalized (Vafa et al., 2024; Mencattini et al., 2026). Distinguishing whether neural dynamics models internal- ize physical laws as world models (Vafa et al., 2024) or merely rely on statistical shortcuts (Geirhos et al., 2020) is computationally difficult, as standard invasive protocols often act as interventions that distort the underlying repre- sentation. To address this, we introduced the non-invasive PhyIPframework to evaluate the intrinsic physics of Self- Supervised Learning (SSL) models without inducing feature distortion (Kumar et al., 2022). Empirically,", "source": "observereffect.pdf"}, {"text": "repre- sentation. To address this, we introduced the non-invasive PhyIPframework to evaluate the intrinsic physics of Self- Supervised Learning (SSL) models without inducing feature distortion (Kumar et al., 2022). Empirically, our non-invasive approach reveals physical structures that standard invasive methods miss. On com- plex benchmarks from \u201cTheWell\u201d (Ohana et al., 2024), we precisely recovered the internal energy law (E\u22481.5P ) in radiative turbulence (Stachenfeld et al., 2021) and identi- fied emergent correction terms for convective kinetic energy (\u223c\u03c1v 2 r) in stellar simulations (\u03c1 >0.90 ) (Goldberg et al., 2022). Furthermore, by replicating orbital mechanics ex- periments (Vafa et al., 2025), we successfully recovered the inverse-square law with high fidelity (\u03c1\u22480.91 ), whereas invasive adaptation probes reported near-zero correlation (\u03c1\u22480.05).", "source": "observereffect.pdf"}, {"text": "Furthermore, by replicating orbital mechanics ex- periments (Vafa et al., 2025), we successfully recovered the inverse-square law with high fidelity (\u03c1\u22480.91 ), whereas invasive adaptation probes reported near-zero correlation (\u03c1\u22480.05). Conversely, we show that invasive probes\u2014including non- linear MLP probes (Belinkov, 2022a), Last-Layer Fine- Tuning (LLFT) (Kirichenko et al., 2023a), and Inductive Bias Probes (IBP) (Vafa et al., 2025) can act as destruc- tive interventions. Rather than passively measuring latent knowledge, they overwrite the representation (Kumar et al., 2022). Our mechanistic analysis confirms that these optimiz- ers systematically suppress complex time-varying features (e.g., speed, radius) to exploit simple constant identifiers (e.g., mass), effectively \u201challucinating\u201d competence or eras- ing physical laws to fit narrow data (Mukhoti et al., 2024; Geirhos et", "source": "observereffect.pdf"}, {"text": "time-varying features (e.g., speed, radius) to exploit simple constant identifiers (e.g., mass), effectively \u201challucinating\u201d competence or eras- ing physical laws to fit narrow data (Mukhoti et al., 2024; Geirhos et al., 2020). In concurrent work, Liu et al. (2026) identify key induc- tive biases; specifically spatial smoothness (continuous re- gression), stability (noise injection), and temporal locality (context restriction); that enable Transformers to learn New- tonian physics with perfect fidelity (R2 \u22481 ). While they demonstrate that explicitly enforcing these constraints guar- antees the acquisition of exact physical models, our work offers a complementary perspective focused on evaluation. We find that even without these additional inductive biases, standard SSL approximately encodes physical dynamics into linear subspaces. Although these latent representa- tions", "source": "observereffect.pdf"}, {"text": "offers a complementary perspective focused on evaluation. We find that even without these additional inductive biases, standard SSL approximately encodes physical dynamics into linear subspaces. Although these latent representa- tions may not always reach the perfect precision of con- strained models, their linear extractability confirms that the core physical laws are already emerging. This validates the promise of general-purpose foundation models (Bommasani et al., 2022): that broad physical understanding can emerge implicitly from data scale and diversity. We hope this work encourages the adoption of neural models asfixed measurement instruments(Peters et al., 2016) and the use of non-invasive protocols to distinguish true machine learning failures from artifacts of adaptation. Ultimately, our findings suggest thatScientific AI requires not just better models,", "source": "observereffect.pdf"}, {"text": "et al., 2016) and the use of non-invasive protocols to distinguish true machine learning failures from artifacts of adaptation. Ultimately, our findings suggest thatScientific AI requires not just better models, but better instruments to measure them. Limitations and Future Work:We identify three primary constraints. Linear probes prevent the probe from solv- ing the physics independently but limit performance on highly non-linear entangled representations, where phys- ical quantities may be encoded in more complex, non- linear geometries. Future research should investigate subspace-constrained or weight-preserving adaptation proto- cols. These methods would aim to acquire new task-specific capabilities while strictly protecting the linear physical in- variants\u2014such as conservation laws\u2014already internalized by the model. 9 The Observer Effect in World Models References Belinkov, Y", "source": "observereffect.pdf"}, {"text": "to acquire new task-specific capabilities while strictly protecting the linear physical in- variants\u2014such as conservation laws\u2014already internalized by the model. 9 The Observer Effect in World Models References Belinkov, Y . Probing classifiers: Promises, shortcomings, and advances.Computational Linguistics, 48(1):207\u2013219, 2022a. Belinkov, Y . Probing classifiers: Promises, shortcomings, and advances.Computational Linguistics, 48(1):207\u2013219, March 2022b. doi: 10.1162/coli a 00422. URL https: //aclanthology.org/2022.cl-1.7/. Bereska, L. and Gavves, E. Mechanistic interpretability for ai safety \u2013 a review, 2024. URL https://arxiv. org/abs/2404.14082. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Paras- candolo, G. Neural symbolic regression that scales. In International Conference on Machine Learning (ICML), 2021. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg,", "source": "observereffect.pdf"}, {"text": "symbolic regression that scales. In International Conference on Machine Learning (ICML), 2021. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse- lut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna,", "source": "observereffect.pdf"}, {"text": "D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchan- dani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadim- itriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y ., Ruiz, C., Ryan, J., R \u00b4e, C., Sadigh, D., Sagawa, S., Santhanam,", "source": "observereffect.pdf"}, {"text": "S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y ., Ruiz, C., Ryan, J., R \u00b4e, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tram`er, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y ., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y ., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2022. URL https://arxiv.org/abs/2108.07258. Chen, B., Huang, K., Raghupathi, S., Chandratreya, I., Du, Q., and Lipson, H. Automated discovery of fundamental variables hidden in experimental data.Nature Computa- tional Science, 2(7):433\u2013442, 2022. Chen,", "source": "observereffect.pdf"}, {"text": "URL https://arxiv.org/abs/2108.07258. Chen, B., Huang, K., Raghupathi, S., Chandratreya, I., Du, Q., and Lipson, H. Automated discovery of fundamental variables hidden in experimental data.Nature Computa- tional Science, 2(7):433\u2013442, 2022. Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. InAdvances in Neural Information Processing Systems (NeurIPS), 2018a. Chen, R. T. Q., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equa- tions. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.),Advances in Neural Information Process- ing Systems, volume 31. Curran Associates, Inc., 2018b. URL https://proceedings.neurips. cc/paper_files/paper/2018/file/ 69386f6bb1dfed68692a24c8686939b9-Paper. pdf. Coveney, P. and Highfield, R. Ai needs physics more than physics needs ai, 2025. URL https://arxiv.org/ abs/2512.16344.", "source": "observereffect.pdf"}, {"text": "ing Systems, volume 31. Curran Associates, Inc., 2018b. URL https://proceedings.neurips. cc/paper_files/paper/2018/file/ 69386f6bb1dfed68692a24c8686939b9-Paper. pdf. Coveney, P. and Highfield, R. Ai needs physics more than physics needs ai, 2025. URL https://arxiv.org/ abs/2512.16344. Cranmer, M., Greydanus, S., Hoyer, S., Battaglia, P., Spergel, D., and Ho, S. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020. DiCarlo, J. J. and Cox, D. D. Untangling invari- ant object recognition.Trends in Cognitive Sci- ences, 11(8):333\u2013341, 2007. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2007.06.010. URL https://www.sciencedirect.com/ science/article/pii/S1364661307001593. Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Short- cut learning in deep neural networks.Nature Ma- chine Intelligence, 2(11):665\u2013673, 2020. doi: 10.1038/ s42256-020-00257-z. Goldberg, J. A., Jiang, Y .-F., and Bildsten, L. Numerical simulations of convective three-dimensional", "source": "observereffect.pdf"}, {"text": "A. Short- cut learning in deep neural networks.Nature Ma- chine Intelligence, 2(11):665\u2013673, 2020. doi: 10.1038/ s42256-020-00257-z. Goldberg, J. A., Jiang, Y .-F., and Bildsten, L. Numerical simulations of convective three-dimensional red super- giant envelopes.The Astrophysical Journal, 929(2):156, apr 2022. doi: 10.3847/1538-4357/ac5ab3. URL https: //doi.org/10.3847/1538-4357/ac5ab3. Goyal, P. et al. Accurate, large minibatch sgd: Training imagenet in 1 hour.arXiv preprint arXiv:1706.02677, 2017. Greydanus, S., Dzamba, M., and Yosinski, J. Hamiltonian neural networks. InAdvances in Neural Information Processing Systems (NeurIPS), volume 32, 2019. Guo, Y ., Codella, N. C., Karlinsky, L., Codella, J. V ., Smith, J. R., Saenko, K., Rosing, T., and Feris, R. A broader study of cross-domain few-shot learning. InEuropean Conference on Computer Vision, pp. 124\u2013141. Springer, 2020. Ha, D.", "source": "observereffect.pdf"}, {"text": "V ., Smith, J. R., Saenko, K., Rosing, T., and Feris, R. A broader study of cross-domain few-shot learning. InEuropean Conference on Computer Vision, pp. 124\u2013141. Springer, 2020. Ha, D. and Schmidhuber, J. World models. 2018. doi: 10.5281/ZENODO.1207631. URL https://zenodo. org/record/1207631. 10 The Observer Effect in World Models Haber, E. and Ruthotto, L. Stable architectures for deep neural networks.Inverse Problems, 34(1):014004, dec 2017. doi: 10.1088/1361-6420/aa9a90. URL https: //doi.org/10.1088/1361-6420/aa9a90. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition, 2015. URL https:// arxiv.org/abs/1512.03385. Heisenberg, W. \u00a8Uber den anschaulichen inhalt der quan- tentheoretischen kinematik und mechanik.Zeitschrift f \u00a8ur Physik, 43(3-4):172\u2013198, 1927. Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Inui, K., Jiang,", "source": "observereffect.pdf"}, {"text": "den anschaulichen inhalt der quan- tentheoretischen kinematik und mechanik.Zeitschrift f \u00a8ur Physik, 43(3-4):172\u2013198, 1927. Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Inui, K., Jiang, J., Ng, V ., and Wan, X. (eds.),Proceedings of the 2019 Conference on Empir- ical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733\u20132743, Hong Kong, China, November 2019a. Association for Compu- tational Linguistics. doi: 10.18653/v1/D19-1275. URL https://aclanthology.org/D19-1275/. Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. InConference on Empirical Methods in Natural Language Processing (EMNLP), 2019b. Hirashima, K., Moriwaki, K., Fujii, M. S., Hirai, Y ., Saitoh, T. R., Makino, J., and Ho, S. Surrogate modeling for", "source": "observereffect.pdf"}, {"text": "on Empirical Methods in Natural Language Processing (EMNLP), 2019b. Hirashima, K., Moriwaki, K., Fujii, M. S., Hirai, Y ., Saitoh, T. R., Makino, J., and Ho, S. Surrogate modeling for computationally expensive simulations of supernovae in high-resolution galaxy simulations.arXiv preprint arXiv:2311.08460, 2023. Intern`o, C., Geirhos, R., Olhofer, M., Liu, S., Hammer, B., and Klindt, D. AI-generated video detection via perceptual straightening. InThe Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum? id=LsmUgStXby. Jaeger, H. The\u201decho state\u201dapproach to analysing and training recurrent neural networks. 2001. URL https: //api.semanticscholar.org/CorpusID: 15467150. Kamienny, P.-A., d\u2019Ascoli, S., Lample, G., and Charton, F. End-to-end symbolic regression with transformers. InAdvances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 10269\u201310281, 2022. Karniadakis, G. E., Kevrekidis,", "source": "observereffect.pdf"}, {"text": "Kamienny, P.-A., d\u2019Ascoli, S., Lample, G., and Charton, F. End-to-end symbolic regression with transformers. InAdvances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 10269\u201310281, 2022. Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang, L. Physics-informed machine learning.Nature Reviews Physics, 3(6):422\u2013440, 2021. Kirichenko, P., Izmailov, P., and Wilson, A. G. Last layer re-training is sufficient for robustness to spurious cor- relations. InThe Eleventh International Conference on Learning Representations, 2023a. URL https: //openreview.net/forum?id=Zb6c8A-Fghk. Kirichenko, P., Izmailov, P., and Wilson, A. G. Last layer re-training is sufficient for robustness to spurious cor- relations, 2023b. URL https://arxiv.org/abs/ 2204.02937. Klindt, D., O\u2019Neill, C., Reizinger, P., Maurer, H., and Mi- olane, N. From superposition to sparse codes: inter- pretable", "source": "observereffect.pdf"}, {"text": "sufficient for robustness to spurious cor- relations, 2023b. URL https://arxiv.org/abs/ 2204.02937. Klindt, D., O\u2019Neill, C., Reizinger, P., Maurer, H., and Mi- olane, N. From superposition to sparse codes: inter- pretable representations in neural networks, 2025. URL https://arxiv.org/abs/2503.01824. Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. Similar- ity of neural network representations revisited. InInterna- tional Conference on Machine Learning, pp. 3519\u20133529. PMLR, 2019. K\u00a8ugelgen, J. V ., Sharma, Y ., Gresele, L., Brendel, W., Sch\u00a8olkopf, B., Besserve, M., and Locatello, F. Self- supervised learning with data augmentations provably isolates content from style. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.),Advances in Neu- ral Information Processing Systems, 2021. URL https: //openreview.net/forum?id=4pf_pOo0Dt. Kumar, A., Raghunathan, A., Jones,", "source": "observereffect.pdf"}, {"text": "from style. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.),Advances in Neu- ral Information Processing Systems, 2021. URL https: //openreview.net/forum?id=4pf_pOo0Dt. Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pretrained fea- tures and underperform out-of-distribution. InIn- ternational Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=UYneFzXSJWh. LeCun, Y . and Courant. A path towards au- tonomous machine intelligence version 0.9.2, 2022-06- 27. 2022. URL https://api.semanticscholar. org/CorpusID:251881108. Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat- tacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equa- tions, 2021a. URL https://arxiv.org/abs/ 2010.08895. Li, Z., Kovachki, N. B., Azizzadenesheli, K., liu, B., Bhat- tacharya, K., Stuart, A., and", "source": "observereffect.pdf"}, {"text": "Anandkumar, A. Fourier neural operator for parametric partial differential equa- tions, 2021a. URL https://arxiv.org/abs/ 2010.08895. Li, Z., Kovachki, N. B., Azizzadenesheli, K., liu, B., Bhat- tacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equa- tions. InInternational Conference on Learning Represen- tations, 2021b. URL https://openreview.net/ forum?id=c8P9NQVtmnO. Liu, Z., Mao, H., Wu, C., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s.CoRR, abs/2201.03545, 2022. URL https://arxiv.org/ abs/2201.03545. Liu, Z., Sanborn, S., Ganguli, S., and Tolias, A. From kepler to newton: Inductive biases guide learned world models in transformers, 2026. URL https://arxiv.org/ abs/2602.06923. 11 The Observer Effect in World Models Mai, Z., Chowdhury, A., Zhang, P., Tu, C.-H., Chen, H.- Y ., Pahuja, V", "source": "observereffect.pdf"}, {"text": "learned world models in transformers, 2026. URL https://arxiv.org/ abs/2602.06923. 11 The Observer Effect in World Models Mai, Z., Chowdhury, A., Zhang, P., Tu, C.-H., Chen, H.- Y ., Pahuja, V ., Berger-Wolf, T., Gao, S., Stewart, C., Su, Y ., and Chao, W.-L. Fine-tuning is fine, if cali- brated. InThe Thirty-eighth Annual Conference on Neu- ral Information Processing Systems, 2024. URL https: //openreview.net/forum?id=XRJXKBeeTD. Mari, L., Wilson, M., and Maul, A.Philosophical Perspec- tives on Measurement, pp. 81\u2013121. Springer International Publishing, Cham, 2023. ISBN 978-3-031-22448-5. doi: 10.1007/978-3-031-22448-5 4. URL https://doi. org/10.1007/978-3-031-22448-5_4. Mencattini, T., Cadei, R., and Locatello, F. Exploratory causal inference in saence, 2026. URL https:// arxiv.org/abs/2510.14073. Motamed, S., Culp, L., Swersky, K., Jaini, P., and Geirhos, R. Do generative video models", "source": "observereffect.pdf"}, {"text": "T., Cadei, R., and Locatello, F. Exploratory causal inference in saence, 2026. URL https:// arxiv.org/abs/2510.14073. Motamed, S., Culp, L., Swersky, K., Jaini, P., and Geirhos, R. Do generative video models understand physical principles?, 2025. URL https://arxiv.org/abs/ 2501.09038. Mukhoti, J., Gal, Y ., Torr, P., and Dokania, P. K. Fine-tuning can cripple foundation models; preserving features may be the solution, 2024. URL https://openreview. net/forum?id=VQ7Q6qdp0P. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Stein- hardt, J. Progress measures for grokking via mechanistic interpretability, 2023a. URL https://arxiv.org/ abs/2301.05217. Nanda, N., Lee, A., and Wattenberg, M. Emergent linear representations in world models of self-supervised se- quence models, 2023b. URL https://arxiv.org/ abs/2309.00941. Nanda, N. et al. Emergent linear representations in world models of self-supervised", "source": "observereffect.pdf"}, {"text": "and Wattenberg, M. Emergent linear representations in world models of self-supervised se- quence models, 2023b. URL https://arxiv.org/ abs/2309.00941. Nanda, N. et al. Emergent linear representations in world models of self-supervised learning.arXiv preprint arXiv:2309.00941, 2023c. Newton, I.Philosophi\u00e6 naturalis principia mathematica. Jussu Societatis Regi\u00e6 ac Typis Josephi Streater, Lon- don, 1687. Annotated 1st Edition, Cambridge Digital Library. https://cudl.lib.cam.ac.uk/view/ PR-ADV-B-00039-00001/1. Ohana, R., McCabe, M., Meyer, L., Morel, R., Agocs, F., Beneitez, M., Berger, M., Burkhart, B., Dalziel, S., Field- ing, D., et al. The well: a large-scale collection of diverse physics simulations for machine learning, 2024. Park, K., Choe, Y . J., and Veitch, V . The linear represen- tation hypothesis and the geometry of large language models. InCausal Representation Learning Workshop at", "source": "observereffect.pdf"}, {"text": "machine learning, 2024. Park, K., Choe, Y . J., and Veitch, V . The linear represen- tation hypothesis and the geometry of large language models. InCausal Representation Learning Workshop at NeurIPS 2023, 2023. URL https://openreview. net/forum?id=T0PoOJg8cK. Peters, J., B \u00a8uhlmann, P., and Meinshausen, N. Causal in- ference by using invariant prediction: Identification and confidence intervals.Journal of the Royal Statistical So- ciety Series B: Statistical Methodology, 78(5):947\u20131012, 10 2016. ISSN 1369-7412. doi: 10.1111/rssb.12167. URL https://doi.org/10.1111/rssb.12167. Ravichander, A., Belinkov, Y ., and Hovy, E. Probing the probing paradigm: Does probing accuracy reveal con- tent or correlation? InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 2021. Reizinger, P., Balestriero, R., Klindt, D., and Brendel, W. Position: An empirically grounded", "source": "observereffect.pdf"}, {"text": "con- tent or correlation? InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 2021. Reizinger, P., Balestriero, R., Klindt, D., and Brendel, W. Position: An empirically grounded identifiability theory will accelerate self supervised learning research. InForty-second International Conference on Machine Learning Position Paper Track, 2025. URL https: //openreview.net/forum?id=ET6qJpllEi. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolu- tional networks for biomedical image segmentation. InIn- ternational Conference on Medical image computing and computer-assisted intervention, pp. 234\u2013241. Springer, 2015. Santi, R. D., Vlastelica, M., Hsieh, Y .-P., Shen, Z., He, N., and Krause, A. Flow density control: Genera- tive optimization beyond entropy-regularized fine-tuning. InThe Thirty-ninth Annual Conference on Neural In- formation Processing Systems, 2025. URL https: //openreview.net/forum?id=JzCjNJlSxI. Sassoli de", "source": "observereffect.pdf"}, {"text": "He, N., and Krause, A. Flow density control: Genera- tive optimization beyond entropy-regularized fine-tuning. InThe Thirty-ninth Annual Conference on Neural In- formation Processing Systems, 2025. URL https: //openreview.net/forum?id=JzCjNJlSxI. Sassoli de Bianchi, M. The observer effect.Founda- tions of Science, 18(2):213\u2013243, 2013. doi: 10.1007/ s10699-012-9298-3. URL https://doi.org/10. 1007/s10699-012-9298-3. Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data.Science, 324(5923):81\u201385, 2009a. Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data.Science, 324(5923):81\u201385, 2009b. doi: 10.1126/science. 1165893. URL https://www.science.org/ doi/abs/10.1126/science.1165893. Spies, A. F., Edwards, W., Ivanitskiy, M. I., Skapars, A., R \u00a8auker, T., Inoue, K., Russo, A., and Shanahan, M. Transformers use causal world models in maze- solving tasks, 2025. URL https://arxiv.org/ abs/2412.11867. Stachenfeld, K., Brandstetter, J., Pfaff, T., Hoi,", "source": "observereffect.pdf"}, {"text": "R \u00a8auker, T., Inoue, K., Russo, A., and Shanahan, M. Transformers use causal world models in maze- solving tasks, 2025. URL https://arxiv.org/ abs/2412.11867. Stachenfeld, K., Brandstetter, J., Pfaff, T., Hoi, S. C., Battaglia, P., and Kim, B. Learned coarse models for efficient turbulence simulation.arXiv preprint arXiv:2112.15275, 2021. 12 The Observer Effect in World Models Teoh, J., Tomar, M., Ahn, K., Hu, E. S., Sharma, P., Islam, R., Lamb, A., and Langford, J. Next-latent prediction transformers learn compact world models, 2025. URL https://arxiv.org/abs/2511.05963. Trivedi, P., Koutra, D., and Thiagarajan, J. J. A closer look at model adaptation using feature distortion and simplicity bias. InThe Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=wkg_b4-IwTZ. Vafa, K., Chen, J. Y ., Rambachan,", "source": "observereffect.pdf"}, {"text": "A closer look at model adaptation using feature distortion and simplicity bias. InThe Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=wkg_b4-IwTZ. Vafa, K., Chen, J. Y ., Rambachan, A., Kleinberg, J., and Mullainathan, S. Evaluating the world model implicit in a generative model. InThe Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=aVK4JFpegy. Vafa, K., Chang, P. G., Rambachan, A., and Mullainathan, S. What has a foundation model found? using inductive bias to probe for world models, 2025. URL https: //arxiv.org/abs/2507.06952. Wang, H., Fu, T., Du, Y ., et al. Scientific discovery in the age of artificial intelligence.Nature, 620(7972):47\u201360, 2023. doi: 10.1038/s41586-023-06221-2. URL https: //doi.org/10.1038/s41586-023-06221-2. Wang, Y ., Huang, H., Rudin, C., and Shaposhnik, Y", "source": "observereffect.pdf"}, {"text": "Du, Y ., et al. Scientific discovery in the age of artificial intelligence.Nature, 620(7972):47\u201360, 2023. doi: 10.1038/s41586-023-06221-2. URL https: //doi.org/10.1038/s41586-023-06221-2. Wang, Y ., Huang, H., Rudin, C., and Shaposhnik, Y . Un- derstanding how dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization.Journal of Machine Learning Research, 22(201):1\u201373, 2021. URL http: //jmlr.org/papers/v22/20-1061.html. Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Gontijo-Lopes, R., Hajishirzi, H., Farhadi, A., Namkoong, H., and Schmidt, L. Robust fine-tuning of zero-shot models.arXiv preprint arXiv:2109.01903, 2021.https://arxiv.org/abs/2109.01903. Zimmermann, R. S., Sharma, Y ., Schneider, S., Bethge, M., and Brendel, W. Contrastive learning inverts the data generating process. In Meila, M. and Zhang, T. (eds.),Proceedings of", "source": "observereffect.pdf"}, {"text": "arXiv:2109.01903, 2021.https://arxiv.org/abs/2109.01903. Zimmermann, R. S., Sharma, Y ., Schneider, S., Bethge, M., and Brendel, W. Contrastive learning inverts the data generating process. In Meila, M. and Zhang, T. (eds.),Proceedings of the 38th International Conference on Machine Learning, volume 139 ofProceedings of Machine Learning Research, pp. 12979\u201312990. PMLR, 18\u201324 Jul 2021. URLhttps://proceedings.mlr. press/v139/zimmermann21a.html. 13 The Observer Effect in World Models Appendix Contents AAdditional Related Work 14 BFormal Derivation of Equation (2) 15 CFluid Dynamics Experiment Setting 17 DManifold Visualization Methodology 18 EAnalysis of Solar System Replication 19 FParameter Change Analysis 20 GFull Fine-tuning Data Distribution Analysis 20 HSymbolic Formula Comparison 21 A. Additional Related Work We situate our work at the intersection of mechanistic interpretability and the dynamics of transfer learning", "source": "observereffect.pdf"}, {"text": "GFull Fine-tuning Data Distribution Analysis 20 HSymbolic Formula Comparison 21 A. Additional Related Work We situate our work at the intersection of mechanistic interpretability and the dynamics of transfer learning for AI-driven scientific discovery. AI for Physics.The discovery of physical laws from data typically relies on two paradigms: enforcing laws via architectural priors, such as Hamiltonian/Lagrangian Neural Networks (HNNs/LNNs) (Greydanus et al., 2019; Cranmer et al., 2020; Karniadakis et al., 2021), or post-hoc Symbolic Regression (SR) (Schmidt & Lipson, 2009a). While SR methods attempt to learn symbolic expressions directly from high-dimensional inputs (Biggio et al., 2021; Kamienny et al., 2022), they often struggle with the curse of dimensionality. However, recent findings suggest that standard SSL suffices to identify dynamics without", "source": "observereffect.pdf"}, {"text": "high-dimensional inputs (Biggio et al., 2021; Kamienny et al., 2022), they often struggle with the curse of dimensionality. However, recent findings suggest that standard SSL suffices to identify dynamics without physics-specific constraints (Chen et al., 2022; Reizinger et al., 2025). Intern `o et al. (2025) observe that physically consistent dynamics from natural video emerge as linear \u201cstraight\u201d trajectories in pre-trained latent spaces, whereas AI-generated video creates curved latent trajectories due to physical artifact implausibility. Inductive Biases and Causal Discovery.The mechanism by which generic foundation models capture physical laws remains a subject of intense debate. Vafa et al. (2025) utilized \u201cInductive Bias Probes\u201d to audit models for physical compliance, concluding that standard Transformers achieve high predictive accuracy but fail to internalize", "source": "observereffect.pdf"}, {"text": "subject of intense debate. Vafa et al. (2025) utilized \u201cInductive Bias Probes\u201d to audit models for physical compliance, concluding that standard Transformers achieve high predictive accuracy but fail to internalize fundamental forces. Responding to this, Liu et al. (2026) argue that this failure stems from a lack of architectural constraints; they distinguish between \u201cKeplerian\u201d world models (curve-fitting) and \u201cNewtonian\u201d models (causal forces), demonstrating that the latter only emerge when specific biases are enforced. However, recent mechanistic interpretability studies suggest that causal structures may emerge naturally without such constraints (Nanda et al., 2023b). Spies et al. (2025) identified latent \u201cWorld Models\u201d in maze-solving Transformers, while Mencattini et al. (2026) demonstrated that causal effects can be explicitly recovered from frozen foundation models", "source": "observereffect.pdf"}, {"text": "al., 2023b). Spies et al. (2025) identified latent \u201cWorld Models\u201d in maze-solving Transformers, while Mencattini et al. (2026) demonstrated that causal effects can be explicitly recovered from frozen foundation models using Sparse Autoencoders (Klindt et al., 2025), effectively disentangling the \u201ctreatment\u201d variables from unstructured representations. Self-Supervised Learning and Emergent World Models.The capability of next-token prediction to induce compact belief states remains a subject of active debate. Teoh et al. (2025) argue that in generic discrete domains, this objective is theoretically insufficient without auxiliary losses. This view is challenged by empirical findings in game playing (Nanda et al., 2023b) and mechanistic analysis (Nanda et al., 2023a), which demonstrate that Transformers develop linear representations 14 The Observer Effect in World Models of", "source": "observereffect.pdf"}, {"text": "findings in game playing (Nanda et al., 2023b) and mechanistic analysis (Nanda et al., 2023a), which demonstrate that Transformers develop linear representations 14 The Observer Effect in World Models of \u201cworld models\u201d\u2014solely from the predictive objective. From a theoretical standpoint, work on identifiability in SSL (Zimmermann et al., 2021; K\u00a8ugelgen et al., 2021) suggests that it can provably recover ground-truth latent factors given sufficient data diversity. In the context of continuous physics dynamics, we align with the view that residual networks function as discretizations of Ordinary Differential Equations (Haber & Ruthotto, 2017; Chen et al., 2018a). B. Formal Derivation of Equation (2) \u2206xt (T rue) J ht (Model) \u2207\u03a6(xt) \u00b5\u03a6 (Probe) Modeling Error (\u03f5) Curvature (K\u03a6) Figure 8.Geometry of the", "source": "observereffect.pdf"}, {"text": "Ruthotto, 2017; Chen et al., 2018a). B. Formal Derivation of Equation (2) \u2206xt (T rue) J ht (Model) \u2207\u03a6(xt) \u00b5\u03a6 (Probe) Modeling Error (\u03f5) Curvature (K\u03a6) Figure 8.Geometry of the Linear Probe Error Bound.The total error stems from two mismatches:1) Modeling Error:The deviation of the model\u2019s linearized update (J ht) from the true physical update (\u2206xt).2) Curvature Error:The angular deviation between the local physical gradient (\u2207\u03a6t) and the global mean gradient (\u00b5\u03a6) used by the fixed linear probe. High curvature (K\u03a6) increases this angle. We derive the error bound for the linear probe, explicitly handling the approximation errors induced by the non-linearity of the physical functional and the neural decoder. B.1. Proof Strategy: The Double Linearization Our goal is to", "source": "observereffect.pdf"}, {"text": "the linear probe, explicitly handling the approximation errors induced by the non-linearity of the physical functional and the neural decoder. B.1. Proof Strategy: The Double Linearization Our goal is to bound the error of a linear probe PW \u2217 mapping the latent representation ht directly to the physical quantity update\u2206s t. To do this, we decompose the true non-linear transition into two approximating linear steps: 1. Physics Linearization:We approximate the curved physical law \u2206st = \u03a6(x t+1)\u2212\u03a6(x t) via its local gradient \u2207\u03a6(xt). The error in this step depends on thePhysical CurvatureK \u03a6. 2. Model Linearization:We approximate the non-linear residual decoder g(ht) via its Jacobian J. The error in this step is bounded by theSSL Prediction Error\u03f5. The total probe", "source": "observereffect.pdf"}, {"text": "thePhysical CurvatureK \u03a6. 2. Model Linearization:We approximate the non-linear residual decoder g(ht) via its Jacobian J. The error in this step is bounded by theSSL Prediction Error\u03f5. The total probe error is derived by bounding the mismatch between the fixed linear probe W \u2217 (which must average over the state space) and these local linearizations. B.2. Setup and Definitions Let the state space be X \u2282R n. The physical functional is \u03a6 :X \u2192R k, such that st = \u03a6(x t). The target update is \u2206st =s t+1 \u2212s t. The model predicts\u02c6xt+1 =x t +g(h t), whereg(0) =0. \u2022Physics Dynamics:x t+1 =x t + \u2206xt, where\u2206x t \u2248\u2206t\u00b7f(x t). \u2022SSL Objective:E[\u2225\u02c6x t+1 \u2212x t+1\u22252]\u2264\u03f5. \u2022Linear Probe:P W \u2217(ht) =W", "source": "observereffect.pdf"}, {"text": "The model predicts\u02c6xt+1 =x t +g(h t), whereg(0) =0. \u2022Physics Dynamics:x t+1 =x t + \u2206xt, where\u2206x t \u2248\u2206t\u00b7f(x t). \u2022SSL Objective:E[\u2225\u02c6x t+1 \u2212x t+1\u22252]\u2264\u03f5. \u2022Linear Probe:P W \u2217(ht) =W \u2217ht, whereW \u2217 \u2208R k\u00d7d. 15 The Observer Effect in World Models B.3. Step-by-Step Derivation Step 1: Linearizing the Physical Law (\u03a6).Assuming \u03a6 is C2-smooth, we expand the physical update around the current statex t: \u2206st =\u2207\u03a6(x t)\u22a4\u2206xt +R \u03a6(xt,\u2206t)(10) where the remainder is bounded by the curvature constantK \u03a6 (the Lipschitz constant of\u2207\u03a6): \u2225R\u03a6\u2225 \u2264 1 2 K\u03a6\u2225\u2206xt\u22252 \u2208 O(\u2206t2)(11) Step 2: Linearizing the Model Decoder (g).Since the decoder is origin-preserving, we linearize around the null latent ht =0: g(ht) =J h t +R g(ht)(12) where J=\u2207g(0)\u2208R n\u00d7d is", "source": "observereffect.pdf"}, {"text": "O(\u2206t2)(11) Step 2: Linearizing the Model Decoder (g).Since the decoder is origin-preserving, we linearize around the null latent ht =0: g(ht) =J h t +R g(ht)(12) where J=\u2207g(0)\u2208R n\u00d7d is the Jacobian. The SSL error constraintE[\u2225g(ht)\u2212\u2206x t\u22252]\u2264\u03f5 implies that the linear termJ ht approximates the true physical update\u2206xt up to the training error and higher-order terms. Specifically,E[\u2225J ht\u2212\u2206xt\u22252]\u2248\u03f5 . Step 3: Defining the Optimal Fixed Probe.A linear probe W \u2217 must correspond to a single, time-independent matrix. The optimal choice is the projection of theexpected global gradientonto the decoder\u2019s tangent space: W \u2217 =\u00b5 \u22a4 \u03a6 J(13) where\u00b5 \u03a6 =E x[\u2207\u03a6(x)]\u2208R n\u00d7k is the mean gradient of the functional over the state distribution. Step 4: Error Decomposition.We analyze the squared", "source": "observereffect.pdf"}, {"text": "space: W \u2217 =\u00b5 \u22a4 \u03a6 J(13) where\u00b5 \u03a6 =E x[\u2207\u03a6(x)]\u2208R n\u00d7k is the mean gradient of the functional over the state distribution. Step 4: Error Decomposition.We analyze the squared error of the probe prediction against the true update: Lprobe =E[\u2225W \u2217ht \u2212\u2206s t\u22252](14) Substituting the linearizations from Steps 1 and 2, and adding/subtracting the term \u2207\u03a6(xt)\u22a4J ht (the local linear approxi- mation): \u2225W \u2217ht \u2212\u2206s t\u2225=\u2225\u00b5 \u22a4 \u03a6 J ht \u2212(\u2207\u03a6(x t)\u22a4\u2206xt +R \u03a6)\u2225 =\u2225(\u00b5 \u03a6 \u2212 \u2207\u03a6(xt))\u22a4J ht| {z } Term A: Curvature Mismatch +\u2207\u03a6(x t)\u22a4(J ht \u2212\u2206x t)| {z } Term B: Modeling Error \u2212R\u03a6\u2225(15) Using the inequality(a+b+c) 2 \u22643(a 2 +b 2 +c 2), we bound the expectation: Analyzing Term A (Curvature Mismatch):This term measures the", "source": "observereffect.pdf"}, {"text": "t)| {z } Term B: Modeling Error \u2212R\u03a6\u2225(15) Using the inequality(a+b+c) 2 \u22643(a 2 +b 2 +c 2), we bound the expectation: Analyzing Term A (Curvature Mismatch):This term measures the error of using theaveragegradient \u00b5\u03a6 instead of the localgradient\u2207\u03a6(x t). Applying Cauchy-Schwarz: E[Term A2]\u2264E[\u2225\u00b5 \u03a6 \u2212 \u2207\u03a6(xt)\u22252\u2225J ht\u22252](16) We bound the step size \u2225J ht\u22252 \u2264C step. The remaining term is strictly the variance of the gradient, Var(\u2207\u03a6(x)). Using the Lipschitz property of the gradient (CurvatureK \u03a6): Var(\u2207\u03a6(x))\u2264K 2 \u03a6 \u00b7Var(x)(17) Thus,E[Term A 2]\u2264C stepK2 \u03a6 Var(x). Analyzing Term B (Modeling Error):This term measures the failure of the model to produce the correct state update. E[Term B2]\u2264sup x \u2225\u2207\u03a6(x)\u22252 \u00b7E[\u2225J h t \u2212\u2206x t\u22252]\u2264C grad \u00b7\u03f5(18) Final Bound.Combining terms, we", "source": "observereffect.pdf"}, {"text": "(Modeling Error):This term measures the failure of the model to produce the correct state update. E[Term B2]\u2264sup x \u2225\u2207\u03a6(x)\u22252 \u00b7E[\u2225J h t \u2212\u2206x t\u22252]\u2264C grad \u00b7\u03f5(18) Final Bound.Combining terms, we obtain the final inequality: E \u0002 \u2225PW \u2217 ht \u2212\u2206s t\u22252\u0003 \u2264C 1 \u00b7\u03f5+C 2 \u0002 K2 \u03a6 \u00b7Var(x) \u0003 +O(\u2206t 4)(19) 16 The Observer Effect in World Models C. Fluid Dynamics Experiment Settings To validate the generality of our non-invasive probing framework, we applied it to three complex, high-dimensional fluid dynamics simulations fromTheWellbenchmark (Ohana et al., 2024). We tested multiple neural simulator architectures\u2014U- Net (Ronneberger et al., 2015), UNetConvNext (Liu et al., 2022), FNO (Li et al., 2021b), and TFNO (Li et al., 2021a)\u2014all trained on a self-supervised next-step", "source": "observereffect.pdf"}, {"text": "multiple neural simulator architectures\u2014U- Net (Ronneberger et al., 2015), UNetConvNext (Liu et al., 2022), FNO (Li et al., 2021b), and TFNO (Li et al., 2021a)\u2014all trained on a self-supervised next-step prediction task. Our objective was to determine if these models implicitly learned the conservation of total internal energy (Eint) purely from observing state transitions. We extracted frozen activations h(t) from the bottleneck (U-Nets) or the final spectral block (FNOs) and trained a linear probe to predictE int(t+ 1). C.1. 2D Turbulent Radiative Layer (TRL-2D) This simulation models a 2D slice of a stellar atmosphere or accretion disk, governed by compressible magnetohydro- dynamics (MHD) with radiative transfer. It captures the interplay between magnetic turbulence and radiative cooling. Governing Equations.The system evolves", "source": "observereffect.pdf"}, {"text": "of a stellar atmosphere or accretion disk, governed by compressible magnetohydro- dynamics (MHD) with radiative transfer. It captures the interplay between magnetic turbulence and radiative cooling. Governing Equations.The system evolves according to: \u2202\u03c1 \u2202t +\u2207 \u00b7(\u03c1v) = 0(20) \u2202(\u03c1v) \u2202t +\u2207 \u00b7(\u03c1vv+P) = 0(21) \u2202E \u2202t +\u2207 \u00b7((E+P)v) =\u2212 E tcool (22) where the internal energy is defined by the ideal gas law: E=P/(\u03b3\u22121) with \u03b3= 5/3. Figure 9.TRL-2D Simulation (Stachenfeld et al., 2021) Task & Probe Configuration. \u2022SSL Input:{\u03c1, v x, vy, P}at timet. Resolution:128\u00d7128. \u2022Probe Target:The total internal energyE int = R \u2126 P \u03b3\u22121 dV. C.2. 3D Red Supergiant Convective Envelope (RSG-3D) This dataset simulates the outer convective envelope of a red supergiant star, governed by compressible", "source": "observereffect.pdf"}, {"text": "energyE int = R \u2126 P \u03b3\u22121 dV. C.2. 3D Red Supergiant Convective Envelope (RSG-3D) This dataset simulates the outer convective envelope of a red supergiant star, governed by compressible hy- drodynamics with radiative transfer. It features strong convective upflows and buoyancy-driven turbulence. Governing Equations. d\u03c1 dt =\u2212\u03c1\u2207 \u00b7V(23) d2r dt2 =\u2212 \u2207P \u03c1 +a visc \u2212 \u2207\u03a6grav (24) du dt =\u2212 P \u03c1 \u2207 \u00b7V+ \u0393\u2212\u039b \u03c1 (25) Here, u is specific internal energy, \u03a6grav is gravitational potential, and \u0393,\u039b represent radiative heating/cooling. Figure 10.RSG-3D Simulation (Goldberg et al., 2022) Task & Probe Configuration. \u2022SSL Input:{\u03c1, P, v r, v\u03b8, v\u03d5}at timet. Resolution:64\u00d764\u00d764. \u2022 Probe Target:Total internal energy Eint = R \u2126 \u03c1u dV . Note that specific energy", "source": "observereffect.pdf"}, {"text": "2022) Task & Probe Configuration. \u2022SSL Input:{\u03c1, P, v r, v\u03b8, v\u03d5}at timet. Resolution:64\u00d764\u00d764. \u2022 Probe Target:Total internal energy Eint = R \u2126 \u03c1u dV . Note that specific energy u is not an input; the probe must implicitly deriveufromPand\u03c1via the equation of state. C.3. 3D Supernova Explosion (SN-3D) A simulation of a core-collapse supernova, involving extreme relativistic velocities, shock waves, and a simplified nuclear burning network. 17 The Observer Effect in World Models Governing Equations.The system includes the standard conservation of mass and momentum, but energy is dominated by nuclear terms: \u2202E \u2202t +\u2207 \u00b7. . .=\u2212cG 0 r \u2212\u03c1V\u00b7 \u2207\u03a6(26) \u2202I \u2202t +cn\u00b7 \u2207I=S(I,n)(27) where S is the source term from the nuclear burning network, creating extreme non-linearities.", "source": "observereffect.pdf"}, {"text": "nuclear terms: \u2202E \u2202t +\u2207 \u00b7. . .=\u2212cG 0 r \u2212\u03c1V\u00b7 \u2207\u03a6(26) \u2202I \u2202t +cn\u00b7 \u2207I=S(I,n)(27) where S is the source term from the nuclear burning network, creating extreme non-linearities. Figure 11.SN-3D Simulation (Hirashima et al., 2023) Task & Probe Configuration. \u2022SSL Input:{\u03c1, P gas, T, vx, vy, vz}. Resolution:64\u00d764\u00d764. \u2022 Probe Target:Internal energy density \u03f5=\u03c1u , derived from a lookup table of nuclear equations of state (EOS), not a simple ideal gas law. C.4. Data Partitioning To ensure the validity of OOD generalization claims, we enforced a strict separation of datasets following setting descibed in (Ohana et al., 2024). 1.SSL Training Set (D train):Usedonlyfor pre-training the backbone modelm \u03b8. 2. Probe Training Set (Dprobe):A strictly In-Distribution (ID) subset held", "source": "observereffect.pdf"}, {"text": "of datasets following setting descibed in (Ohana et al., 2024). 1.SSL Training Set (D train):Usedonlyfor pre-training the backbone modelm \u03b8. 2. Probe Training Set (Dprobe):A strictly In-Distribution (ID) subset held out from Dtrain. The probe learns the mapping Won this data.Crucially, the probe never sees OOD data during training. 3. OOD Test Sets (DOOD):Completely distinct simulations with physical parameters (e.g., cooling rates, stellar mass) unseen in eitherD train orD probe. C.5. Implementation and Reproducibility Details We detail the training hyper-parameters for the SSL backbone, the non-invasive probes, and the invasive baselines in Table 3. SSL Training.All models were trained using the AdamW optimizer with a cosine annealing schedule. Training was performed on 2\u00d7NVIDIA H100 GPUs. Probe Training.The linear probes were", "source": "observereffect.pdf"}, {"text": "baselines in Table 3. SSL Training.All models were trained using the AdamW optimizer with a cosine annealing schedule. Training was performed on 2\u00d7NVIDIA H100 GPUs. Probe Training.The linear probes were trained using Ridge Regression (L2 regularization) on the frozen representations. Invasive Baselines. \u2022 MLP Probe:A non-linear probe composed of two dense layers ( d\u2192254\u21921 ) with ReLU activation. Trained using Adam on the frozen representation. \u2022 Full Fine-tuning (IBP):The entire backbone m\u03b8 is unfrozen and updated to predict the physical target exactly as in (Vafa et al., 2025). D. Manifold Visualization Methodology The manifold visualizations in Figure 3 (Top) are generated to contrast the internal representation geometry of the pre-trained SSL model (m\u03b8) and the fine-tuned model (m\u03b8\u2032). The process,", "source": "observereffect.pdf"}, {"text": "Visualization Methodology The manifold visualizations in Figure 3 (Top) are generated to contrast the internal representation geometry of the pre-trained SSL model (m\u03b8) and the fine-tuned model (m\u03b8\u2032). The process, as implemented in the provided analysis script, is as follows: \u2022 1. Data Sampling:We sample 50,000 data points from the validation set. For each point, we compute and store its corresponding ground-truth force magnitude (\u2225 \u20d7F\u2225= q F 2x +F 2y ), which is used to color the final plot. 18 The Observer Effect in World Models Table 3.Dataset & Hyperparameter Specifications.Comparison of the Newtonian Two-Body task (Vafa et al., 2025) and The Well benchmarks (Ohana et al., 2024). Feature Two-Body (Newton) TRL-2D RSG-3D SN-3D Spatial Resolution2(x, ycoords)384\u00d7128 256\u00d7128\u00d7256 64", "source": "observereffect.pdf"}, {"text": "Hyperparameter Specifications.Comparison of the Newtonian Two-Body task (Vafa et al., 2025) and The Well benchmarks (Ohana et al., 2024). Feature Two-Body (Newton) TRL-2D RSG-3D SN-3D Spatial Resolution2(x, ycoords)384\u00d7128 256\u00d7128\u00d7256 64 3 Total Trajectories10\u00d710 6 90 29 740 Data Partitioning (Train / Probe / OOD) SSL Train (N SSL )10\u00d710 6 72 23 592 Probe Train (N P robe ) 10,000 9 3 74 OOD Test Set 5 Galaxies 9 Cooling Rates 3 Phases 27 Env. Vars Probe & Adaptation Hyperparameters Optimizer AdamW AdamW AdamW AdamW Batch Size 64 32 8 8 Ridge Reg. (\u03b1) 1.0 1.0 1.0 1.0 MLP Hidden Dim 254 254 254 254 Adaptation LR1e-3 1e-3 1e-3 1e-3 \u2022 2. Activation Extraction:We process these 50,000 inputs through both", "source": "observereffect.pdf"}, {"text": "8 Ridge Reg. (\u03b1) 1.0 1.0 1.0 1.0 MLP Hidden Dim 254 254 254 254 Adaptation LR1e-3 1e-3 1e-3 1e-3 \u2022 2. Activation Extraction:We process these 50,000 inputs through both the frozen pre-trained model and the fine-tuned model. Using a PyTorch forward hook, we extract the high-dimensional activation vectors (embedding dimension d= 768 ) from theinputto the 10th decoder block ( \u2019blocks.9\u2019). This layer is chosen to match the layer used for linear probing. This step yields two distinct sets of high-dimensional representations: hssl \u2208R 50000\u00d7768 from the SSL model andh f t \u2208R 50000\u00d7768 from the fine-tuned model. \u2022 3. Dimensionality Reduction:To visualize these 768-dimensional manifolds in 2D, we apply the PaCMAP dimensionality reduction algorithm (Wang et al., 2021).", "source": "observereffect.pdf"}, {"text": "andh f t \u2208R 50000\u00d7768 from the fine-tuned model. \u2022 3. Dimensionality Reduction:To visualize these 768-dimensional manifolds in 2D, we apply the PaCMAP dimensionality reduction algorithm (Wang et al., 2021). Crucially, we fit the PaCMAP algorithmindependentlyto each set of activations. This generates two 2D projections, Zssl =PaCMAP(h ssl) and zf t =PaCMAP(h f t). While other algorithms like t-SNE and UMAP were considered, we selected PaCMAP for its superior ability to preserve bothlocalandglobaldata structure. E. Solar System Experiment We replicated the true solar system setup from Vafa et al. (2025) to evaluate OOD generalization on real orbital dynamics. As shown in Figure 12, our non-invasive probe (blue) consistently outperforms the invasive fine-tuning baseline (green), which exhibits erratic behavior across the", "source": "observereffect.pdf"}, {"text": "to evaluate OOD generalization on real orbital dynamics. As shown in Figure 12, our non-invasive probe (blue) consistently outperforms the invasive fine-tuning baseline (green), which exhibits erratic behavior across the planetary suite. Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune 1.0 0.5 0.0 0.5 1.0 p Pearson Correlation ( is Better) Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune 10 50 100% Error Magnitude MAPE (%) ( is Better) Our Probe Fine-Tuning Figure 12.Solar System Generalization Analysis.Comparison of force vector prediction performance. The invasive probe (Green) fails systematically, exhibiting high variance and negative correlations. The non-invasive probe (Blue) remains robust, though it highlights specific OOD challenges for Venus and Uranus. Outlier Analysis (Venus & Uranus).While the non-invasive probe generally succeeds,", "source": "observereffect.pdf"}, {"text": "variance and negative correlations. The non-invasive probe (Blue) remains robust, though it highlights specific OOD challenges for Venus and Uranus. Outlier Analysis (Venus & Uranus).While the non-invasive probe generally succeeds, we identify two distinct failure modes. ForVenus, the probe\u2019s Pearson correlation drops to \u03c1\u22480.4 with a corresponding spike in error (MAPE >50% ). 19 The Observer Effect in World Models Conversely, forUranus, the probe maintains high linearity ( \u03c1\u22480.8 ) but suffers from calibration error (MAPE \u224870% ). This suggests that while the model correctly encodes thedirectionalityof the force at Uranus\u2019s distance, the magnitude scaling at the outer solar system boundary drifts from the training distribution. Systemic Failure of Invasive Probing.In contrast, the failure of the invasive fine-tuning probe is", "source": "observereffect.pdf"}, {"text": "Uranus\u2019s distance, the magnitude scaling at the outer solar system boundary drifts from the training distribution. Systemic Failure of Invasive Probing.In contrast, the failure of the invasive fine-tuning probe is not merely an issue of precision, but of fundamental physical correctness. For Jupiter, the invasive probe exhibits a strongnegativePearson correlation (\u03c1\u2248 \u22120.9 ). This indicates that the fine-tuning process has inverted the vector field, effectively predicting a repulsive force rather than an attractive one. For the inner planets, the invasive probe shows near-zero correlation (\u03c1\u22480.0 ), confirming the mechanistic finding in Section 4 that dynamic invariants (like the distance-force relationship) are erased during adaptation. Our non-invasive probe maintains high performance (\u03c1 >0.85 ) on these same planets, confirming that the", "source": "observereffect.pdf"}, {"text": "in Section 4 that dynamic invariants (like the distance-force relationship) are erased during adaptation. Our non-invasive probe maintains high performance (\u03c1 >0.85 ) on these same planets, confirming that the correct physical world model exists in the backbone but is destroyed by the invasive measurement. F. Parameter Change Analysis To quantify the invasiveness of the fine-tuning process, we analyze the magnitude of weight modifications across the transformer architecture. We compute the layer-wise relative change using the Frobenius norm: \u03b4(l) = \u2225\u03b8\u2032(l) \u2212\u03b8 (l)\u2225F \u2225\u03b8(l)\u2225F (28) where \u03b8(l) represents the weights of layer l in the pre-trained SSL model, and \u03b8\u2032(l) represents the weights after fine-tuning on the inductive bias task. EmbComb Pos Emb B0 B1 B2 B3 B4 B5 B6", "source": "observereffect.pdf"}, {"text": "of layer l in the pre-trained SSL model, and \u03b8\u2032(l) represents the weights after fine-tuning on the inductive bias task. EmbComb Pos Emb B0 B1 B2 B3 B4 B5 B6 B7 B8 B9 B10 B11 Ln F 0.0 2.5 5.0 7.5 10.0 Relative Change (%) Figure 13.Global Invasiveness:Parameter heatmap showing modification concentrated in deep layers. As illustrated in Figure 13, the modification is highly non-uniform. \u2022 Early Layers (Blocks 0\u20134):Exhibit high stability ( \u03b4(l) <0.02 ), indicating that the basic feature extraction for orbital trajectories remains largely intact. \u2022 Deep Layers (Blocks 5\u201310):Show a sharp spike in parameter modification ( \u03b4(l) >0.10 ), particularly in the MLP projections and Self-Attention output matrices. This concentrated modification in the deeper blocks corroborates", "source": "observereffect.pdf"}, {"text": "Layers (Blocks 5\u201310):Show a sharp spike in parameter modification ( \u03b4(l) >0.10 ), particularly in the MLP projections and Self-Attention output matrices. This concentrated modification in the deeper blocks corroborates our \u201cErasure\u201d hypothesis. In hierarchical models, deep layers typically encode high-level semantic variables and dynamic rules (?). The fact that the optimizer selectively targets these layers suggests it is overwriting the model\u2019s high-level physics engine (the \u201cWorld Model\u201d) to replace it with the shallow heuristics required by the narrow fine-tuning distribution. G. Full Fine-tuning Data Distribution Analysis We analyze the distributional shift between the self-supervised pre-training dataset (DSSL) and the downstream fine-tuning dataset (Dtask). As hypothesized in Section 4, invasive adaptation on narrow distributions encourages the model to discard complex", "source": "observereffect.pdf"}, {"text": "distributional shift between the self-supervised pre-training dataset (DSSL) and the downstream fine-tuning dataset (Dtask). As hypothesized in Section 4, invasive adaptation on narrow distributions encourages the model to discard complex physical dependencies in favor of statistical shortcuts. Figure 14 illustrates this discrepancy across key variables: 20 The Observer Effect in World Models Train (Pre-training) Finetune (Diagnostic) Test (OOD) 1.0 0.5 0.01.5 1.0 0.5 0.0 Vector Forces 0.5 1.0 1.5 Density Vector Forces 6 4 2 0 0.25 0.50 0.75 1.00 1.25 1.50 Force Magnitude Force Magnitude 60 Star Mass 10 5 0 1 1 20 Mass Mass 40 20 0 0.50 0.75 1.00 1.25 1.50 1.75 Mass 2 Figure 14.Data Distribution Shift.Comparison of physical variables between SSL (Grey) and Fine-tuning", "source": "observereffect.pdf"}, {"text": "5 0 1 1 20 Mass Mass 40 20 0 0.50 0.75 1.00 1.25 1.50 1.75 Mass 2 Figure 14.Data Distribution Shift.Comparison of physical variables between SSL (Grey) and Fine-tuning (Green). The fine-tuning data collapses to narrow regimes (e.g., single star mass), inducing simplicity bias. \u2022 Star Mass (m2):While the SSL pre-training data covers a continuous range of stellar masses, the fine-tuning dataset collapses to a single point mass at m2 = 1.0. This lack of variance removes the incentive for the model to maintain m2 as an active variable, leading to the \u201cerasure\u201d observed in our mechanistic analysis. \u2022 Force Magnitude (|| \u20d7F|| ):The fine-tuning dataset exhibits a highly peaked distribution centered around || \u20d7F|| \u22481.0 , failing to", "source": "observereffect.pdf"}, {"text": "leading to the \u201cerasure\u201d observed in our mechanistic analysis. \u2022 Force Magnitude (|| \u20d7F|| ):The fine-tuning dataset exhibits a highly peaked distribution centered around || \u20d7F|| \u22481.0 , failing to cover the heavy tails of high-force interactions or low-force interactions. This restricts the optimizer\u2019s ability to learn the full inverse-square law (1/r2), as gradients are dominated by a specific force regime. \u2022 Force Vectors (\u20d7F ):The distribution of vector components in the fine-tuning set differs significantly from the isotropic distribution seen during pre-training, potentially overfitting the model to specific orbital orientations rather than learning rotation-invariant physics. H. Symbolic Formula Comparison Table 4 presents the symbolic equations discovered for the gravitational force magnitude (F ) via Symbolic Regression (SR). We compare", "source": "observereffect.pdf"}, {"text": "orientations rather than learning rotation-invariant physics. H. Symbolic Formula Comparison Table 4 presents the symbolic equations discovered for the gravitational force magnitude (F ) via Symbolic Regression (SR). We compare the equations extracted from the Invasive Fine-Tuning baseline (IBP) (Vafa et al., 2025) against those recovered byPhyIP. The SR algorithm (PySR) attempts to fit the scalar force magnitude \u2225 \u20d7F\u2225 using the state variables r (distance), m1 (planet mass), and m2 (star mass). The search was constrained to standard arithmetic and trigonometric (+, -, *, /, sin, cos) Table 4.Symbolic Equation Discovery.Comparison of discovered laws. The Invasive Probe fits spurious correlations (nested sines), while the Non-Invasive Probe recovers the structural1/r 2 dependence. Source Complexity Discovered Symbolic Equation Ground Truth \u2013", "source": "observereffect.pdf"}, {"text": "Equation Discovery.Comparison of discovered laws. The Invasive Probe fits spurious correlations (nested sines), while the Non-Invasive Probe recovers the structural1/r 2 dependence. Source Complexity Discovered Symbolic Equation Ground Truth \u2013 \u2225 \u20d7F\u2225 \u221d m1m2 r2 Invasive FT (IBP)High\u2225 \u20d7F\u2225 \u221d \u0014 sin \u0012 1 sin(r\u22120.24) \u0013 + 1.45 \u0015 \u00b7 1 1/r+m 2 (Baseline) (Fails to isolate1/r 2; relies on high-freq artifacts) Non-Invasive (Ours)Low (Rank 1)\u2225 \u20d7F\u2225 \u2248 1 1/r+ 1.16 Non-Invasive (Ours)Med (Rank 2)\u2225 \u20d7F\u2225 \u2248sin(sin(sin(r\u00b70.07) + 0.48)) Non-Invasive (Ours) Best (Rank 3)\u2225 \u20d7F\u2225 \u2248 0.1 r2 |{z} Recovered Physics + sin(. . .)| {z } Residual Noise To ensure reproducibility, we generated 7 candidate equations using PySR\u2019s simulated annealing. The \u201cBest\u201d equation reported in Table 4 was selected", "source": "observereffect.pdf"}, {"text": "Physics + sin(. . .)| {z } Residual Noise To ensure reproducibility, we generated 7 candidate equations using PySR\u2019s simulated annealing. The \u201cBest\u201d equation reported in Table 4 was selected using the Pareto frontier of the score metric (minimizing MSE while penalizing complexity). 21", "source": "observereffect.pdf"}, {"text": "Progress in Particle Physics and Modern Cosmology A. D. Dolgov This is the translation into English of my paper published in \u201dEinshteinovskij Sbornik\u201d 1980-1981 [1]. The content of the book is presented in the Appendix. This is the first paper where the dynamical mechanism of \u201dphoenix universe\u201d was worked out. The notion of phoenix universe was first mentioned in the paper by Lemaitre in 1933 [2], where he assumed that a repetition of successive phases of expansion and contraction was possible. Lemaitre called such model of the universe, that is born, dies and is reborn, the phoenix model, named after the mythical bird able to reborn from the ashes. According to the author, there could be an infinite number of", "source": "particlephysics.pdf"}, {"text": "born, dies and is reborn, the phoenix model, named after the mythical bird able to reborn from the ashes. According to the author, there could be an infinite number of such cycles in the past and future. In this model, however, a possible solution to the fundamental question was unclear, what was the mechanism for the \u201crebirth\u201d of the universe? This mechanism was worked out in ref. [1], published in 1980 in Russian. Moreover, in our recent paper [3] a mechanism of dynamical cancellation of vacuum energy was proposed, that permits to eliminate vacuum energy locally down to zero and permits universe to jump to a lower hot level, leading to rebirth of a hot universe. Introduction Modern cosmology was", "source": "particlephysics.pdf"}, {"text": "permits to eliminate vacuum energy locally down to zero and permits universe to jump to a lower hot level, leading to rebirth of a hot universe. Introduction Modern cosmology was born as a result of Einstein\u2019s formulation [4] of the general the- ory of relativity and Friedmann\u2019s discovery [5] of non-stationary solutions of Einstein\u2019s equations. Einstein, when he first expressed the idea of applying the equations he had discovered to cosmology was, however, discouraged by the fact that these equations do not have stationary solutions in a cosmological situation. To eliminate this \u201ddeficiency,\u201d Einstein proposed generalizing the equations of general relativity by adding the so-called cosmological term [6], which could stabilize the Universe. However, it soon became clear that the", "source": "particlephysics.pdf"}, {"text": "eliminate this \u201ddeficiency,\u201d Einstein proposed generalizing the equations of general relativity by adding the so-called cosmological term [6], which could stabilize the Universe. However, it soon became clear that the Universe is, after all, expanding in full accordance with Friedmann\u2019s predictions. This discovery was made by Hubble [7], who saw that distant astronomical objects are moving away from us at a speed proportional to distance to them. The natural next step was the formulation of the hot model of the Universe by Gamow [8], based on the theory of synthesis of elements, which became the generally accepted cosmological model after the discovery of the relic electromagnetic radiation by Penzias and Wilson [9]. The sta- tus of the hot model was", "source": "particlephysics.pdf"}, {"text": "of elements, which became the generally accepted cosmological model after the discovery of the relic electromagnetic radiation by Penzias and Wilson [9]. The sta- tus of the hot model was further strengthened after detailed calculations of abundances of light elements produced in a hot Universe that were performed by Wagoner, Fowler, and Hoyle [10]. The results of these calculations were in excellent agreement with astro- 1 arXiv:2602.00106v1 [astro-ph.CO] 25 Jan 2026 nomical observations. Particularly successful was the coincidence with observations of the calculated 4Heabundance, which had not been achieved in other models. The impressive achievements of Friedmann\u2019s cosmology further highlight the fact that the assumptions underlying it are truly mysterious. Of course, we are not talking about the theoretical basement", "source": "particlephysics.pdf"}, {"text": "other models. The impressive achievements of Friedmann\u2019s cosmology further highlight the fact that the assumptions underlying it are truly mysterious. Of course, we are not talking about the theoretical basement which is very simple and beautiful; the question concerns the choice of model parameters, the initial conditions that determine the development of the universe. It is precisely the realization of the maximally symmetrical, homogeneous, isotropic initial state, very similar in its properties to vacuum. This requires a precise matching of the model parameters with precision, which has no analogues in physics. Otherwise, the world would be completely different, unsuitable for life, at least in its current form. One might think that the Creator took special care to prepare comfortable conditions", "source": "particlephysics.pdf"}, {"text": "physics. Otherwise, the world would be completely different, unsuitable for life, at least in its current form. One might think that the Creator took special care to prepare comfortable conditions for us, taking care of each one from 10 80 particle of the visible part of the world to make it suitable for our life. This circumstance is the basis for the so-called anthropic principle in its strong formu- lation: the fact of our existence is the answer to the question of why the universe is the way it is, and why we are in the universe, since in the universe not adapted for life such a question cannot be asked, because there simply would be no one to ask", "source": "particlephysics.pdf"}, {"text": "and why we are in the universe, since in the universe not adapted for life such a question cannot be asked, because there simply would be no one to ask it. In such a situa- tion, physicists have nothing to do. I would therefore like to find some kind of explanation for these \u201dfundamental\u201d problems of cosmology, to construct a model in which the universe developed to its current state more or less independently of the initial conditions, adhering to the fundamental laws of physics. Later, thanks to revolutionary advances in elementary particle theory, it became possible to do this. In this sense, cosmology is entering a new level: fundamental cosmological parameters, considered as given values, as a result of", "source": "particlephysics.pdf"}, {"text": "in elementary particle theory, it became possible to do this. In this sense, cosmology is entering a new level: fundamental cosmological parameters, considered as given values, as a result of the choice of initial conditions, may turn out to be calculable quantities. However, it should not be assumed that all these cosmological problems have already been solved; there are still many difficulties ahead, and, possibly, the final answer will differ significantly from the variants currently under consideration, but in any case, there is a fundamental possi- bility of answering the question about the selection of initial conditions in the \u201dbest of all possible worlds\u201d scenario. The order of presentation of the material in the article is the following: in section", "source": "particlephysics.pdf"}, {"text": "question about the selection of initial conditions in the \u201dbest of all possible worlds\u201d scenario. The order of presentation of the material in the article is the following: in section 1, the basic observational facts about the universe, that will be relevant for the future content, are briefly discussed . Section 2 examines important cosmological problems and indicates possible ways of solving them within the framework of the so-called model of an expanding universe, the inflationary model. Section 3 provides a more detailed description of the inflationary model, its difficulties, and possible ways to overcome them. The conclusion summarizes the results. Universe today (observational data) 1. The fact of the expansion of the universe is not disputed by anyone. Apparently,", "source": "particlephysics.pdf"}, {"text": "and possible ways to overcome them. The conclusion summarizes the results. Universe today (observational data) 1. The fact of the expansion of the universe is not disputed by anyone. Apparently, it does not cause any objections to the Hubble\u2019s law of the proportionality of the speed of an object to the distance of that object v=Hr(1) 2 this is true of course on the average, with exclusion of the chaotic motion of individual galaxies in their clusters. So far, however, there is no consensus on the value of the coefficient of proportionality, known as the Hubble constantH. Most astronomers currently give values ofHclose to 100km/sec/Mps [11] but there are works [12] in which a twice smaller value is presented. The", "source": "particlephysics.pdf"}, {"text": "of proportionality, known as the Hubble constantH. Most astronomers currently give values ofHclose to 100km/sec/Mps [11] but there are works [12] in which a twice smaller value is presented. The criticism of the latter paper by the supporters of the large value of H seems convincing but the data on the universe age (see section 6 below) force us to conclude thatH= 100 km/sec/Mpc remains a viable possibility. 2. Whether the expansion will stop or continue indefinitely is determined by the ratio of the average energy density in the universe to the so-called critical density 1 \u2126 =\u03f1/\u03f1 c, \u03f1 c = 3H 2/(8\u03c0G) = 1.86\u00b710 \u221229h2 100 g/cm3 (2) whereGis the gravitational constant:G\u2261m \u22122 P = (1.22\u00b710 19GeV)\u22121 and", "source": "particlephysics.pdf"}, {"text": "to the so-called critical density 1 \u2126 =\u03f1/\u03f1 c, \u03f1 c = 3H 2/(8\u03c0G) = 1.86\u00b710 \u221229h2 100 g/cm3 (2) whereGis the gravitational constant:G\u2261m \u22122 P = (1.22\u00b710 19GeV)\u22121 and h100 =H/(100 km/sec/Mpc).(3) If \u2126>1, then the universe is closed and a period of contraction will eventually take place; if \u2126\u22641 , then expansion will continue indefinitely. The case \u2126 = 1 corresponds to the spatially flat, Euclidean universe, Modern estimates of the average energy density [13], based on measurements of its gravitational effects, are close to \u2126 = 0.3, which supports the open universe model. It is interesting that the determination of the matter density by direct estimate of the amount of matter contained in visible objects and interstellar", "source": "particlephysics.pdf"}, {"text": "which supports the open universe model. It is interesting that the determination of the matter density by direct estimate of the amount of matter contained in visible objects and interstellar space, gives the result approximately an order of magnitude smaller, \u2126 B \u223c0.03. The sub-indexBindicates that this quantity is the usual proton-neutron or baryonic matter. The discrepancy between \u2126 found from dynamics of galaxies and \u2126 B creates the problem of dark matter of the Universe [14].2 One might think that by some not yet known reasons we do not see a significant part of the usual proton-neutron-electron matter. However, given the abundance of deuterium and helium-4 in the Universe, on the one hand, and the theory of galaxy formation,", "source": "particlephysics.pdf"}, {"text": "see a significant part of the usual proton-neutron-electron matter. However, given the abundance of deuterium and helium-4 in the Universe, on the one hand, and the theory of galaxy formation, on the other, this possibility is unlikely. The most popular view is that the invisible matter is either massive neutrinos or some kind of weakly interacting particles, such as axions or photinos. A discussion of these issues in the literature can be found in ref. [16]. A modifications of gravitational interaction cannot be ruled out as well. In principle, an invisible matter could lift \u2126 up to unity but only if it is distributed homogeneously throughout all the space [17], though is seems quite unlikely, but strictly speaking not excluded,", "source": "particlephysics.pdf"}, {"text": "invisible matter could lift \u2126 up to unity but only if it is distributed homogeneously throughout all the space [17], though is seems quite unlikely, but strictly speaking not excluded, This may be of interest for the model of inflationary universe cosmology discussed below. 3. As is well known, the General Relativity equations allow for a generalization by introducing the so-called cosmological term, \u039b [6]: R\u00b5\u03bd \u2212(1/2)g \u00b5\u03bdR= 8\u03c0GT \u00b5\u03bd \u2212\u039bg \u00b5\u03bd,(4) 1We use here the system of units used in particle physics, where speed of light, Boltzmann constant, and reduced Planck constant are all equal to unity:C=k= \u00afh= 1. For example the proton mass is equal tom p = 940M eV= 10 13K= 5\u00b710 13cm\u22121 = 1.5\u00b710 24sec\u22121. 2Fritz", "source": "particlephysics.pdf"}, {"text": "reduced Planck constant are all equal to unity:C=k= \u00afh= 1. For example the proton mass is equal tom p = 940M eV= 10 13K= 5\u00b710 13cm\u22121 = 1.5\u00b710 24sec\u22121. 2Fritz Zwicky who discovered dark matter in the 30th and was badly criticized by the community. referred contemptuously to \u201cthe useless trash in the bulging astronomical journals\u201d, saying \u201cAstronomers are spherical bastards. No matter how you look at them they are just bastards.\u201d 3 whereT \u00b5\u03bd is the energy-momentum tensor of matter, and the term proportional to \u039b describes gravitating vacuum and can be written in the form with\u03f1 vac being the vacuum energy density: \u039bg\u00b5\u03bd =\u22128\u03c0GT vac \u00b5\u03bd =\u22128\u03c0G\u03f1 vacg\u00b5\u03bd.(5) In the standard scenario of the universe evolution it is", "source": "particlephysics.pdf"}, {"text": "and can be written in the form with\u03f1 vac being the vacuum energy density: \u039bg\u00b5\u03bd =\u22128\u03c0GT vac \u00b5\u03bd =\u22128\u03c0G\u03f1 vacg\u00b5\u03bd.(5) In the standard scenario of the universe evolution it is assumed that \u039b = 0, though the observational bounds evaluated in units of the critical energy density are not very strong: |\u03f1vac|<5\u00b710 \u221247m4 N \u224810 \u221229g/cm3 \u223c\u03f1 c.(6) On the other hand, the vacuum energy is very small compared to the characteristic values of\u03f1at the time of the universe creation and in view of that it looks natural to assume that\u03f1 vac identically vanishes. We will return to this issue below. 4. The averaged over large cosmological scales distribution of matter in the universe is highly uniform. Of course, the", "source": "particlephysics.pdf"}, {"text": "that\u03f1 vac identically vanishes. We will return to this issue below. 4. The averaged over large cosmological scales distribution of matter in the universe is highly uniform. Of course, the inhomogeneities are huge at the galactic scales but at distances greater than about 100 Mpc, with the variations of density over directions are quite small \u2206\u03f1/\u03f1 <10 \u22123.(7) The homogeneity and isotropy of the universe is also supported by the observations of cosmic microwave background radiation which directional vaariations do not exceed 10 \u22124 5. There are compelling observational reasons to believe that antimatter is practically absent in the universe, i.e. positrons, antiprotons, antineutrons, but it follows from theory that the amount of antimatter is most likely not insignificant. Strictly", "source": "particlephysics.pdf"}, {"text": "to believe that antimatter is practically absent in the universe, i.e. positrons, antiprotons, antineutrons, but it follows from theory that the amount of antimatter is most likely not insignificant. Strictly speaking, it cannot be ruled out that distant galaxies consist of antimatter, but in all known cases of colliding galaxies or galaxies surrounded by common clouds of interstellar gas, it is clear that these regions contain matter of the same type. This circumstance, as well as the fact that there are few antiprotons in cosmic rays, makes the hypothesis of the existence of antimatter in significant amount highly unlikely. The presence only of matter in the surrounding universe is called the charge or baryon asymmetry of the universe. An important", "source": "particlephysics.pdf"}, {"text": "the existence of antimatter in significant amount highly unlikely. The presence only of matter in the surrounding universe is called the charge or baryon asymmetry of the universe. An important constant in cosmology is the ratio of the average density of baryonsN B to the density of relic photonsN \u03b3: N\u03b3 = 550(T /3K)3cm\u22123.(8) According to current data, this ratio is in the range \u03b2=N B/N\u03b3 = 10\u22129 \u221210 \u221210.(9) 6. The time elapsed from the hot singularity to the present time, is called the age of the Universet U. The value oft U is surely greater than the estimated age of the Earth: 5\u00b710 9 years. Nuclear chronology, as well as the theory of stellar evolution, together with the", "source": "particlephysics.pdf"}, {"text": "The value oft U is surely greater than the estimated age of the Earth: 5\u00b710 9 years. Nuclear chronology, as well as the theory of stellar evolution, together with the fact of the observations of old star clusters, leads to a much larger value [18] tu \u224815\u00b710 9 years.(10) Theory permits to express the age of the universe through the current values of Hubble\u2019s constant and parameter \u2126. Since, according to the standard scenario, the universe spent 4 most of its life in a state of dominance of non-relativistic matter the following expression is valid: tu = 10.8\u00b710 9 years [h100(1 + \u221a \u2126/2)]\u22121 (11) (under assumption that the cosmological constant vanishes). So we have to choose between the following", "source": "particlephysics.pdf"}, {"text": "the following expression is valid: tu = 10.8\u00b710 9 years [h100(1 + \u221a \u2126/2)]\u22121 (11) (under assumption that the cosmological constant vanishes). So we have to choose between the following possibilities: a) the Hubble parameter is smaller than that presented in majority of papers,h 100 <0.6; b) nuclear chronometry and stellar evolution theory suggest a significantly larger value: tu \u224815\u00b710 9 years; c) the cosmological constant is non-zero and close to its upper limit on\u03f1 vac (6), permitted by astronomical observations. Let us note, anticipating what is written below, that in the inflationary universe model these contradictions become even more profound, since in this model the value \u2126 = 1 is predicted and the universe aget u should be smaller", "source": "particlephysics.pdf"}, {"text": "in the inflationary universe model these contradictions become even more profound, since in this model the value \u2126 = 1 is predicted and the universe aget u should be smaller than that in the case \u2126 = 0.3. Fundamental Cosmological Problems. The Einstein equations which make the basis of the modern cosmology have the following very simple form for homogeneous and isotropic distribution of matter: \u00a8a= 4\u03c0G 3 a(3p+\u03f1),(12) \u02d9a2 = 8\u03c0G 3 \u03f1a2 \u2212k.(13) where dots denote differentiation over time,\u03f1andPare respectively energy density and pressure of matter (with possible inclusion of the vacuum term);ais the scale factor, the value of which is not determined. Ifk\u0338= 0, the value ofacan be normalized by the conditionsk= +1 ork=\u22121. Equation (13) can", "source": "particlephysics.pdf"}, {"text": "inclusion of the vacuum term);ais the scale factor, the value of which is not determined. Ifk\u0338= 0, the value ofacan be normalized by the conditionsk= +1 ork=\u22121. Equation (13) can be conveniently rewritten as \u03f1=\u03f1 c \u2212 k a2 , \u03f1 c = \u02d9a2 8\u03c0Ga2 ,(14) from which it follows thatk <0 corresponds to a closed universe, andk >0 corresponds to an open one. There are different expansion regimes depending on the equation of state. Assuming k= 0 we find the following. Relativistic gas:\u03f1\u223ca \u22124, a\u223ct 1/2: p=\u03f1/3, \u03f1(a)\u223ca \u22124, a\u223ct 1/2; (15) Non-relativistic gas:\u03f1\u223ca \u22123, a\u223ct 2/3: p= 0, \u03f1(a)\u223ca \u22123, a\u223ct 2/3; (16) Cosmic strings: p=\u2212\u03f1/3, \u03f1(a)\u223ca \u22122, a\u223ct; (17) 5 Domain walls: p=\u22122\u03f1/3, \u03f1(a)\u223ca \u22121, a\u223ct 2;", "source": "particlephysics.pdf"}, {"text": "a\u223ct 1/2; (15) Non-relativistic gas:\u03f1\u223ca \u22123, a\u223ct 2/3: p= 0, \u03f1(a)\u223ca \u22123, a\u223ct 2/3; (16) Cosmic strings: p=\u2212\u03f1/3, \u03f1(a)\u223ca \u22122, a\u223ct; (17) 5 Domain walls: p=\u22122\u03f1/3, \u03f1(a)\u223ca \u22121, a\u223ct 2; (18) Gravitating vacuum: p=\u2212\u03f1, \u03f1(a) =const, a\u223cexp \"r 8\u03c0G\u03f1 3 t # .(19) At the present time the universe is dominated by nonrelativistic matter and the ex- pansion regime is close to (16), if\u03f1is not too much different from\u03f1 c. The transition from relativistic regime to non-relativistic one takes place at the redshiftz= 4\u00b710 4h2 100. It is unknown if there were regimes dominated by cosmic strings or domain walls, but it is often assumed that there was period of cosmolgical term dominance, The cosmological impact of domain walls which", "source": "particlephysics.pdf"}, {"text": "there were regimes dominated by cosmic strings or domain walls, but it is often assumed that there was period of cosmolgical term dominance, The cosmological impact of domain walls which appears at spontaneous discrete sym- metry breaking was considered in ref. [19], where it was pointed out that these walls, if they existed, would destroy the homogeneity of the universe. The role of cosmic strings was studied in papers [20], where it was shown that the inhomogeneities created by strings are safely small and, moreover, strings that appear in unified theories of strong and electroweak interactions with characteristic energy scale 1014 \u221210 15 GeV could explain the observed the large scale structure of the universe in the forms of galaxies", "source": "particlephysics.pdf"}, {"text": "theories of strong and electroweak interactions with characteristic energy scale 1014 \u221210 15 GeV could explain the observed the large scale structure of the universe in the forms of galaxies or their clusters. How far can we travel backward in time depends upon our knowledge of the particle interactions at high energies and densities. One may be sure that the simple relativistic expansion regime (15) was realized up to energies of several tens or even hundreds MeV. Somewhere close to these energies the equation of state of the primeval plasma might be changed because of the QCD phase transition from free quarks and gluons down to hadrons. Prior to this phase transition the equation of state of the primeval plasma", "source": "particlephysics.pdf"}, {"text": "might be changed because of the QCD phase transition from free quarks and gluons down to hadrons. Prior to this phase transition the equation of state of the primeval plasma is also close to that of the ideal gas. According to our present day knowledge, confirmed by laboratory experiments, this is possibly true starting down from the temperatures about 100 GeV. Advancing into the region of higher temperatures is not so reliable, since experimental data on the particle properties atE >100 GeV are practically absent. On the other hand, there is a theory that describes strong and electroweak interactions of elementary particles in a unified way and has a number of other attractive features, which asserts that nothing revolutionary happens", "source": "particlephysics.pdf"}, {"text": "is a theory that describes strong and electroweak interactions of elementary particles in a unified way and has a number of other attractive features, which asserts that nothing revolutionary happens up to the Planck energyE\u224810 19 GeV when, apparently, the effects of quantum gravity become significant. It is not yet clear how to deal with this. Within the framework of such theories, it can be concluded that the dynamics of the universe is in principle known up energiesE P L = 10 19 GeV. If we assume that all phase transitions occurring during the cooling of the Universe are phase transitions of of the second type or weakly delayed transitions of type I, then the influence of these transitions on", "source": "particlephysics.pdf"}, {"text": "transitions occurring during the cooling of the Universe are phase transitions of of the second type or weakly delayed transitions of type I, then the influence of these transitions on the nature of expansion will not be particularly noticeable and the expansion regime (15) would be approximately valid up to the Planck energy. The statement about phase transition in theories with spontaneously broken symmetry was first made in ref. [21] and since then it has been used in cosmological models. 6 Returning to equation (12), we rewrite it in the form: \u2126\u22121 2 \u22121 = \u0000 \u2126\u22121 1 \u22121 \u0001 \u03f11a2 1 \u03f12a2 2 .(20) where the indices 1 and 2 refer to the values of the quantities at timest1", "source": "particlephysics.pdf"}, {"text": "form: \u2126\u22121 2 \u22121 = \u0000 \u2126\u22121 1 \u22121 \u0001 \u03f11a2 1 \u03f12a2 2 .(20) where the indices 1 and 2 refer to the values of the quantities at timest1 andt 2. Takingt 1 as the present moment andt 2 as the moment of transition from relativistic to non-relativistic expansion law we find: \u2126\u22121 2 \u22121 = \u0000 \u2126\u22121 1 \u22121 \u0001 z\u22121 2 \u224810 \u22124.(21) Choosingt 2 as the beginning of the primordial nucleosynthesis:t 2 = 1 sec andT 2 = 1 MeV, and in this state the universe surely was, as is proven by the abundance of light elements, we find \u2126\u22121 2 \u22121\u224810 \u221216. If we move further deeper to the beginning up toT 2 =T P", "source": "particlephysics.pdf"}, {"text": "surely was, as is proven by the abundance of light elements, we find \u2126\u22121 2 \u22121\u224810 \u221216. If we move further deeper to the beginning up toT 2 =T P l = 1019 GeV, we find that|\u2126 2 \u22121|= 10 \u221259. In other words, for the Universe to reach the present day state the initial state should be fiine-tuned with fantastic accuracy|\u2126 2 \u22121| \u224810 \u221259. The result looks natural enough since for|\u2126 2 \u22121| \u223c1, the characteristic time when the universe expansion for a closed universe would turn into contraction during time close to the Planck one, while an open universe would expand so fast that neither galaxies nor stars or planets would be formed. Thus our existence demonstrates", "source": "particlephysics.pdf"}, {"text": "contraction during time close to the Planck one, while an open universe would expand so fast that neither galaxies nor stars or planets would be formed. Thus our existence demonstrates the extremely fine-tuned initial state of the universe close to the flat 3D one (k= 0). This mysterious fact is the subject of one of the most pressing problems in cosmology: how did such favorable extremely fined tuned initial conditions could arise? This problem is calledthe problem of initial conditions(the first problem). There are several other cosmological problems, without solving which we cannot be sure that we understand how our world was created. The second problem: Isotropy and homogeneityof the universe also imply very specific initial conditions, noticeable deviations from", "source": "particlephysics.pdf"}, {"text": "which we cannot be sure that we understand how our world was created. The second problem: Isotropy and homogeneityof the universe also imply very specific initial conditions, noticeable deviations from which are not allowed even by the anthropic principle. This makes it all the more desirable to find a natural explanation for this fact. The isotropy of the cosmic background radiationpresents a new problem (the third), which is the so called the horizon problem. It is related to the fact that the scale factor in the regimes (15,16) rises slower than the size of the causally connected regionr h \u2248t. Relic radiation became free, i.e., it ceased to interact with anything at the hydrogen recombination temperatureT rec \u22483000 K. This", "source": "particlephysics.pdf"}, {"text": "than the size of the causally connected regionr h \u2248t. Relic radiation became free, i.e., it ceased to interact with anything at the hydrogen recombination temperatureT rec \u22483000 K. This corresponds to the change of the scale factor by about the factor of 1000. For the standard model of the universe evolution this corresponds to the horizon size at this moment equal to\u223c10 13 sec. Correspondingly the size of the region of the physical processes which could create background radiation cannot exceed this value. The attempt to explain this phenomenon, based on the hypothesis of the existence of a maximum density of energy, was made in [22], but the question of the existence of a maximum density of energy was", "source": "particlephysics.pdf"}, {"text": "based on the hypothesis of the existence of a maximum density of energy, was made in [22], but the question of the existence of a maximum density of energy was not resolved. An attempt of this kind, based on the hypothesis of the existence of maximum energy density, was made in ref. [22], but the question of entropy growth in this model remains unclear. The problem of charge asymmetry in the universe(i.e. the presence in the universe only of particles (i.e., protons, neutrinos, electrons) and practically absence of 7 antiparticles, which has been a problem for cosmologists for many years, now can be considered as solved according to ref. [23]. Within the frameworks of the model, there is not only", "source": "particlephysics.pdf"}, {"text": "which has been a problem for cosmologists for many years, now can be considered as solved according to ref. [23]. Within the frameworks of the model, there is not only qualitative but also quantitative agreement with astronomical data for the value ofN b/N\u03b3. To avoid returning to this question later, let us briefly review main features of the mechanism of generation of the excess of particles over antiparticles in the universe. A more detailed discussion can be found in review [24] and in the popular paper [25]. The basic hypothesis is that the baryon charge B is not conserved. Such conservation is indeed predicted by models of grand unification. In these models, there are superheavy particles with masses of the", "source": "particlephysics.pdf"}, {"text": "is that the baryon charge B is not conserved. Such conservation is indeed predicted by models of grand unification. In these models, there are superheavy particles with masses of the order of 1014 \u221210 15 GeV decaying into the states with different values of the baryonic number. Two other essential ingredients of the model are the violation of charge invariance (i.e., the difference in interactions between particles and antiparticles) and deviation from thermodynamic equilibrium in the expanding universe. It can be shown that under such conditions, the decay of the aforementioned superheavy particles (X-, or H-bosons) wiill lead to an excess of particles over antiparticles which because of the violation of thermal equilibrium will not be compensated for by other", "source": "particlephysics.pdf"}, {"text": "aforementioned superheavy particles (X-, or H-bosons) wiill lead to an excess of particles over antiparticles which because of the violation of thermal equilibrium will not be compensated for by other processes. Although the magnitude of this excess cannot be precisely calculated, since the details of the interaction between X-f-bosons are unknown, order-of-magnitude estimates give an answer that is reasonable and consistent with observations. For a number of the models the result does not depend on the initial conditions, i.e. on whether there was initiallyy an excess of baryons or antibaryons, or whether the plasma was charge neutral. An essential feature of this scheme in its classical version is the presence of a large number of X- or Z-bosons in the", "source": "particlephysics.pdf"}, {"text": "or whether the plasma was charge neutral. An essential feature of this scheme in its classical version is the presence of a large number of X- or Z-bosons in the initial state of the plasma, for which it is necessary to reach the temperatures aboutT\u224810 15 GeV. The seventh problem, the problem of magnetic monopoles,is somewhat dif- ferent from those listed above, since it is not a specifically cosmological problem, but is related to the prediction of the existence of magnetic monopoles in models of grand uni- fication. Born in the phase transition from symmetric to asymmetric states during the cooling of the universe, these monopoles would survive until the present day, and their current concentration, calculated within the framework", "source": "particlephysics.pdf"}, {"text": "the phase transition from symmetric to asymmetric states during the cooling of the universe, these monopoles would survive until the present day, and their current concentration, calculated within the framework of the standard model, turns out to be unacceptably large [26]. Problems 1-3 and 7 are uniquely and beautifully solved by theinflationary universe model [27, 28], but at the same time, problem 4 only gets worse. The main idea of the inflationary universe model is that at some stage the energy-momentum spectrum was dominated by a vacuum energy term,T (vac) \u00b5\u03bd =\u03f1 vacg\u00b5\u03bd. According to eq. (19) the scale factor rises exponentially and the energy density quickly approaches the critical one, i.e. \u2126\u21921, see eq. (20). In this model, the", "source": "particlephysics.pdf"}, {"text": "\u00b5\u03bd =\u03f1 vacg\u00b5\u03bd. According to eq. (19) the scale factor rises exponentially and the energy density quickly approaches the critical one, i.e. \u2126\u21921, see eq. (20). In this model, the universe at a given time moment looks as an expanding empty space (density of the usual matter exponentially tends to zero). At the same time, all initial conditions are \u201dforgotten\u201d and the world becomes as uniform as emptiness can be. Then the vacuum \u201dmatter\u201d explodes, giving birth to elementary particles [29], which are thermalized and and the regime of expansion turns into the Friedman one. The required duration of the exponential (De Sitter) period\u03c4depends on the temperature of the created particles. To ensure \u2126\u223c1 at the present time it is", "source": "particlephysics.pdf"}, {"text": "turns into the Friedman one. The required duration of the exponential (De Sitter) period\u03c4depends on the temperature of the created particles. To ensure \u2126\u223c1 at the present time it is necessary that exp(H\u03c4)>10 30(T /MP l), 8 that is H\u03c4 >70\u2212ln(M pl/T) (22) This does not look unreasonably large. If the de Sitter stage actually took place, then at the \u201dzero\u201d moment, parameter \u2126 could have had practically any value. If the universe is open and \u2126\u22641, there no problem arose as the density of matter decreases, the vacuum energy\u03f1 vac begins to dominate, which does not change in the course of expansion. If however, \u2126>1 the universe might start to contract when still\u03f1 m > \u03f1 vac and the", "source": "particlephysics.pdf"}, {"text": "vac begins to dominate, which does not change in the course of expansion. If however, \u2126>1 the universe might start to contract when still\u03f1 m > \u03f1 vac and the exponential stage would not be there. However, (this comment belongs to L.B. Okun) it is known that in the oscillating universe the amplitude of its oscillations should increase due to rise of entropy and respectively\u03f1 m would be diminished at the the point of maximum expansion. Hence sooner or later a closed universe should come to the exponential expansion, of course, if\u03f1 vac >0. Thus, the problem of proximity of\u03f1to\u03f1 c at the present time can be solved without hypothesis of the fine tuning of the \u201dinitial\u201d parameters. However, if", "source": "particlephysics.pdf"}, {"text": "course, if\u03f1 vac >0. Thus, the problem of proximity of\u03f1to\u03f1 c at the present time can be solved without hypothesis of the fine tuning of the \u201dinitial\u201d parameters. However, if we take such point of view, then for the explanation of the proximity\u03f1to\u03f1 c, inflationary scenario is unnecessary since the swinging universe will gradually come to then observed present day state and we just happened to be there when the conditions for life became suitable. The problem of the horizon in inflationary universe is also naturally solved because the scale factora\u223cexpHtgrows faster than horizon. Note that inflationary model, parameter \u2126 should be extremely close to unity because even a very small excess of the exponential period duration over its necessary", "source": "particlephysics.pdf"}, {"text": "factora\u223cexpHtgrows faster than horizon. Note that inflationary model, parameter \u2126 should be extremely close to unity because even a very small excess of the exponential period duration over its necessary value (22) and deviation of \u2126 from unity is actually determined by the fluctuations of density in this model. Astronomical data, however, rather contradicts this, giving a value of \u2126 close to 0.3, unless the universe is filled with homogeneously distributed massive particles, as for example, neutrinos [30]. In this regard, it is very important to clarify the value of the Hubble\u2019s parameter, since for \u2126 = 1:h 100 = 7.2\u00d710 9 years/tu. This may be a realistic way to test inflationary models. The problem of monopole can be also", "source": "particlephysics.pdf"}, {"text": "the Hubble\u2019s parameter, since for \u2126 = 1:h 100 = 7.2\u00d710 9 years/tu. This may be a realistic way to test inflationary models. The problem of monopole can be also solved in this scenario if, after the phase transition which leads to monopole creation, there is still a noticeable exponential expansion. Note that this is required to solve the problem of homogeneity (see above). In this case, there would be no more than one monopole [31] in the visible universe, and a discovery of a second one would require a strong modification of the standard model. Let us note right away that in supersymmetric inflation models [32] the problem of the universe revives, and there are possibilities when the number", "source": "particlephysics.pdf"}, {"text": "strong modification of the standard model. Let us note right away that in supersymmetric inflation models [32] the problem of the universe revives, and there are possibilities when the number of monopoles in the universe is non-vanishing but still does not contradict observations Return now to the problem of homogeneity. In the first proposed model [27] it was assumed that the exponential expansion occurred in a symmetric state before transitioning to a non-symmetric phase. Subsequently, the bubbles of the new phase were formed that filled al the space. Generally in such a model the inhomogeneities created by the bubble walls should be very large (in ref. [34] a mechanism of vacuum burning is discussed for which it is probably not", "source": "particlephysics.pdf"}, {"text": "such a model the inhomogeneities created by the bubble walls should be very large (in ref. [34] a mechanism of vacuum burning is discussed for which it is probably not so). Much smaller inhomogeneities arise in the new inflationary scenario [28], according to which considerable exponential expansion took place not only before but also after phase transition, since in such models the vacuum average of the 9 scalar field (order parameter) very slowly in comparison with the universe expansion rate tended to its limiting value. The problem of inhomogeneities was studied in a series of papers [35], where it was shown that in the standard approach based on Coleman-WeinbergSU(5) model (see be- low) the inhomogeneities would be acceptably small only", "source": "particlephysics.pdf"}, {"text": "studied in a series of papers [35], where it was shown that in the standard approach based on Coleman-WeinbergSU(5) model (see be- low) the inhomogeneities would be acceptably small only for a very unnatural choice of model parameters. This forced us to turn to supersymmetric theories [36], in which such a disadvantage could be overcome. However, in the supersymmetric approach, other diffi- culties may arise, related to magnetic monopoles or to particle production, and generation of the baryon asymmetry, which should be treated with more complicated models Summarizing, it can be said that, in principle, the inflationary scenario offers a beautiful way to solve a number of cosmological problems, but there is hardly a natural concrete model that is completely", "source": "particlephysics.pdf"}, {"text": "be said that, in principle, the inflationary scenario offers a beautiful way to solve a number of cosmological problems, but there is hardly a natural concrete model that is completely free from all shortcomings. Concluding this section, I would like to note that, without solving the problems of cosmological inflationary models, in which exponential growth is caused by the nonzero vacuum energy, at least psychologically, does not seem entirely satisfactory, since it is one thing to assume that\u03f1 vac is always zero, while it is completely different to assume that at the \u201dbeginning\u201d there was\u03f1 vac =\u2212\u03b4\u03f1where\u03b4\u03f1is the change of vacuum energy at the phase transition 3. (However, defenders of these models may reasonably argue that the cosmological vacuum energy", "source": "particlephysics.pdf"}, {"text": "at the \u201dbeginning\u201d there was\u03f1 vac =\u2212\u03b4\u03f1where\u03b4\u03f1is the change of vacuum energy at the phase transition 3. (However, defenders of these models may reasonably argue that the cosmological vacuum energy problem exists independently of inflation.) An exception is presented by the model suggested in ref. [37], where inflation occurs in the pre-Planck era and is caused by non-linear quantum corrections from vacuum polariza- tion to the Einstein equations. It is also possible that the inflationary model is not the only way to solve the cosmological problems under discussion; in fact, there is an alternative attempt [38] based on the hypothesis of a large number of phase transitions in the early universe with a huge increase of entropy. However, one way", "source": "particlephysics.pdf"}, {"text": "there is an alternative attempt [38] based on the hypothesis of a large number of phase transitions in the early universe with a huge increase of entropy. However, one way or another, inflationary model is the first cosmological model in which a natural solution of several \u201deternal\u201d cosmological problems have been is realized which were previously considered as peculiarities of specific initial conditions. In more details, with some technicalities and analysis of the difficulties encountered, these issues are discussed in the following section. Returning to the problem of cosmological constant, we note that, as is mentioned above, that the data ont u,H, and \u2126 strongly indicate that\u03f1 vac \u0338= 0. If inflationary model is valid, and \u2126 = 1 then", "source": "particlephysics.pdf"}, {"text": "we note that, as is mentioned above, that the data ont u,H, and \u2126 strongly indicate that\u03f1 vac \u0338= 0. If inflationary model is valid, and \u2126 = 1 then forh 100 = 1, andt u = 15\u00b710 9 years\u03f1 vac should be positive and contribute today approximately 0.95\u03f1 c. Since\u03f1 c varies with time (roughly speaking, asm 2 P l/t2), and\u03f1 vac.=constit means that only at the present stage is the role of\u03f1 vac is noticeable, while at earlier it could be neglected, which is also one of the mysterious coincidences. Nowadays there is no satisfactory model explaining the smallness of the cosmological constant, but if the presented above values ofHandt u are confirmed, it would be natural to", "source": "particlephysics.pdf"}, {"text": "mysterious coincidences. Nowadays there is no satisfactory model explaining the smallness of the cosmological constant, but if the presented above values ofHandt u are confirmed, it would be natural to demand not a complete compensation of\u03f1vac but only down to the terms of orderm2 P l/t2. This, in turn, means that the non-compensated part of\u03f1 vac, which, strictly speaking, is not proportional tog \u00b5\u03bd could be noticeable during all the history of the universe and to 3Recently A.D. Linde suggested a model of exponential expansion that occurred without strong first order phase transition. It is discussed in more detail in the next section. 10 make an impact on the big bang nucleosynthesis, galaxy formation, and to give a contbution to", "source": "particlephysics.pdf"}, {"text": "order phase transition. It is discussed in more detail in the next section. 10 make an impact on the big bang nucleosynthesis, galaxy formation, and to give a contbution to the hidden mass of the universe. In ref. [39]. a model is suggested in which vacuum energy is cancelled down by the condensate of a scalar field with non-minimal coupling to gravity. The magnitude of the condensate increases by the impact of the cosmological term and this negative back reaction effectively eliminates vacuum energy. The vacuum energy decreases rather slowly, with the remaining value always being of the order of\u03f1 c(t)0. This is achieved due to the very strict and unnatural requirements imposed by the quantum field theory used in", "source": "particlephysics.pdf"}, {"text": "with the remaining value always being of the order of\u03f1 c(t)0. This is achieved due to the very strict and unnatural requirements imposed by the quantum field theory used in the model. It can be said that the problem of cosmological constant is the central problem of cosmology, without whose solution no cosmological model can be considered satisfactory. As for the problems of singularity and the \u201dcreation of the world,\u201d there are several \u201dcrazy\u201d ideas on this subject in the literature. I would like to add one more to them. Suppose there is, say, a scalar field\u03d5with an effective potential that has an infinite number of local minima, separated from each other by potential barriers, with each minimum becoming deeper", "source": "particlephysics.pdf"}, {"text": "Suppose there is, say, a scalar field\u03d5with an effective potential that has an infinite number of local minima, separated from each other by potential barriers, with each minimum becoming deeper and deeper as\u03d5increases. As simplest examples of such potentials one can takem 2\u03d52 cos(\u03d5/\u03c3) orm\u03d5 3[1\u2212\u03f5cos(\u03d5/\u03c3)]. The universe once created, maybe infinitely long ago, stuck in one of those local minima for some time, generally quite long. Then after quantum tunneling this universe or a part of it it undergoes into another vacuum state with lower energy an so on, so fourth infinitely many times. The energy released during this phase transition ends up in the form of elementary particles, which, as a result of the expansion of the universe,", "source": "particlephysics.pdf"}, {"text": "so fourth infinitely many times. The energy released during this phase transition ends up in the form of elementary particles, which, as a result of the expansion of the universe, especially in exponential period, \u201ddissolves\u201d in the universe. The cosmological constant (or, rather, cosmological constants) inherent in this model can be compensated for by a mechanism of the type described in ref. [39]. Such a model describes the eternally expanding universe, infinitely many times cooling down practically to a state of vacuum and exploding again. A suitable name for such a universe would be \u201dPhoenix-Universe.\u201d Unfortunately, or perhaps fortunately, this concept cannot be verified, since in the event of another Big Bang, a possible observer will disappear before noticing anything.", "source": "particlephysics.pdf"}, {"text": "a universe would be \u201dPhoenix-Universe.\u201d Unfortunately, or perhaps fortunately, this concept cannot be verified, since in the event of another Big Bang, a possible observer will disappear before noticing anything. Models of inflating universe. Most models of inflating universe are based on the assumption are based on the assumption that the phase transition from a symmetric to an asymmetric phase is a strongly delayed first order phase transition. In the symmetric phase the scalar field condensate (order parameter) is absent,\u27e8\u03d5\u27e9= 0, while vacuum energy is non-zero: T vac \u00b5\u03bd =g \u00b5\u03bdV(\u03c3),(23) whereVis the effective potential of the scalar field and\u03c3is its vacuum average value after the end of the phase transition. Assumption (23) does not have any natural physical basis", "source": "particlephysics.pdf"}, {"text": "\u00b5\u03bdV(\u03c3),(23) whereVis the effective potential of the scalar field and\u03c3is its vacuum average value after the end of the phase transition. Assumption (23) does not have any natural physical basis but it is only a demand that cosmological term disappeared after the phase transition in agreement with observations. Initially in the universe there could be some matter with energy density\u03f1m but\u03f1 m \u21920, while\u03f1 vac =const. If\u03f1 vac would be larger than\u03f1 m earlier than the phase transition took 11 place, then the universe would exponentially expand for a while: a\u223cexp(Ht), H= \u0014 8\u03c0 3m2 P l V(\u03c3 \u0015 (24) Typical value ofV(\u03c3) in Grand Unification Models is about (10 15GeV)4, and henceH\u2248 1011 GeV. For description of the phase", "source": "particlephysics.pdf"}, {"text": "a\u223cexp(Ht), H= \u0014 8\u03c0 3m2 P l V(\u03c3 \u0015 (24) Typical value ofV(\u03c3) in Grand Unification Models is about (10 15GeV)4, and henceH\u2248 1011 GeV. For description of the phase transition let us consider a simple scalar field model with the Lagrangian L= 1 2 (\u2202\u00b5\u03d5)2 \u2212 1 2 m2 0\u03d52 \u2212 \u03bb 4 \u03d54 +...(25) where multidots stand for the terms describing interactions with other fields: gauge bosons, fermions, and other scalar fields. This Lagrangian is symmetric with respect to the trans- formation\u03d5\u2192 \u2212\u03d5and possibly some other higher symmetry, if e.g\u03d5is a milti-component field. In the standard scheme of spontaneous symmetry violation it is assumed that the mass squared of the scalar field is negative,m 2 0 <0 and", "source": "particlephysics.pdf"}, {"text": "if e.g\u03d5is a milti-component field. In the standard scheme of spontaneous symmetry violation it is assumed that the mass squared of the scalar field is negative,m 2 0 <0 and hence the point of the stable potential extremum is\u03d5 2 0 =\u2212m 2 0 >0. At non-zero temperature the potential acquires an additional term\u03b1\u03d5 2T 2 which could shift the equilibrium point to\u03d5= 0 [40]. So it is clear how field\u03d5would behave in the course of the universe cooling down it this simplest model. At hight temperature the average value of\u03d5vanishes,\u27e8\u03d5\u27e9= 0. This corresponds to the state with unbroken symmetry. With decreasing temperature the summ 2 0 +\u03b1T 2 becomes negative and\u27e8\u03d5 2\u27e9=\u2212(m 2 0+\u03b1T 2)\u0338= 0 and the particles", "source": "particlephysics.pdf"}, {"text": "of\u03d5vanishes,\u27e8\u03d5\u27e9= 0. This corresponds to the state with unbroken symmetry. With decreasing temperature the summ 2 0 +\u03b1T 2 becomes negative and\u27e8\u03d5 2\u27e9=\u2212(m 2 0+\u03b1T 2)\u0338= 0 and the particles interacting with\u03d5acquire non-zero masses proportional to\u27e8\u03d5\u27e9. It easy to see that the phase transition in this model is the second order phase transition, which is not what we need. However, in more complex models, for example, those based on theSU(5) group, quantum corrections could lead to the first order phase transition. For the details and refences to the original literature one can address reviews [41,42]. One can show that in certain class of theories the effective potential calculated in one loop approximation, has indeed the desired form [40,43]: V(\u03d5, T)", "source": "particlephysics.pdf"}, {"text": "one can address reviews [41,42]. One can show that in certain class of theories the effective potential calculated in one loop approximation, has indeed the desired form [40,43]: V(\u03d5, T) = 1 2 \u0000 m2 0 +\u03b1T 2\u0001 \u03d52 + \u03bb 4 ln \u03d52 \u03c32\u221ae +aT 4 Z \u221e 0 dxx2 ln 1\u2212e \u221a x2+b\u03d52/T 2 1\u2212e \u2212x ! .(26) where\u03c3= 10 14 \u221210 15 GeV,\u03b1,\u03bb, and\u03b2are some numbers which depend upon the coupling constant; the value ofadoes not depend upon the interaction; the contribution of the corresponding term at\u03d5= 0 gives simply thermal energy of particles. In the limit of low temperature we may neglect the last term. This does not change qualitatively our conclusion. Potential (26) has a", "source": "particlephysics.pdf"}, {"text": "0 gives simply thermal energy of particles. In the limit of low temperature we may neglect the last term. This does not change qualitatively our conclusion. Potential (26) has a minimum at\u03d5= 0 ifm 2 0 +\u03b1T 2 >0. Ifm 2 0 +\u03b1T 2 < \u03bb\u03c3 2/e, then there is another minimum at\u03d5\u2248\u03c3. This minimum will be deeper than the first one and will therefore be stable stable ifm 2 0 +\u03b1T 2 < \u03bb\u03c3 2/4. Thus the stable at thigh temperatures minimum ofV(\u03d5, T) at\u03d5= 0 transforms at lower temperatures into a quasistable one (the true vacuum of this theory) separated from the original stable minimum by some potential barrier. 12 As a rule, the probability of passage through", "source": "particlephysics.pdf"}, {"text": "temperatures into a quasistable one (the true vacuum of this theory) separated from the original stable minimum by some potential barrier. 12 As a rule, the probability of passage through the potential barrier is exponentially suppressed, so the system can remain in a quasi-stable state for a very long time. Tunneling in quantum theory was first considered in paper [44]. A more elegant method was proposed in [45] According to the results of these works, the probability of tunneling is determined by the value of the action calculated on the solution of the classical equation of motion in imaginary time. An approximate answer can be obtained in a simpler way by finding the extremum of the action using the variational", "source": "particlephysics.pdf"}, {"text": "solution of the classical equation of motion in imaginary time. An approximate answer can be obtained in a simpler way by finding the extremum of the action using the variational method. In particular, the probability of tunneling per unit volume per unit time in potential (26) at zero temperature is equal to dW dV dt \u2248M 4 exp \" \u22128\u03c02 3\u03bb \u0012 ln \u03bb\u03c32 m2 \u0013\u22121# ,(27) whereMis some unknown factor with dimension of mass. Most likely, M is of the order of the inverse size of the bubble of the new phase, i.e.M\u2248m. Since\u03bb <1, the tunneling time is monstrously long and the universe will indeed have time to expand so much that no traces of matter, that was", "source": "particlephysics.pdf"}, {"text": "the new phase, i.e.M\u2248m. Since\u03bb <1, the tunneling time is monstrously long and the universe will indeed have time to expand so much that no traces of matter, that was originally in it, would remain. In reality, however, one should not think that the temperature is equal to zero, because due to the presence of the horizon, the temperature in the de Sitter universe cannot be lower thanH/(2\u03c0) [46] (in comoving coordinates). Since the state with\u27e8\u03d5\u27e9= 0 is unstable, then, sooner or later the transition to the new phase with\u27e8\u03d5\u27e9 \u0338= 0 should take place. Immediately after formation of the bubble of the new phase, the magnitude of the classical field inside it should be of the orderm/ \u221a \u03bb.", "source": "particlephysics.pdf"}, {"text": "\u0338= 0 should take place. Immediately after formation of the bubble of the new phase, the magnitude of the classical field inside it should be of the orderm/ \u221a \u03bb. This can be seen from the form of the dependence of the effective effective potential (26) on \u03d5. For natural values of parameters of the scalar field potential the value of\u03d5quickly, in comparison withH, tends to its limiting value\u03c3. Indeed after the quantum jump creating the phase transition the evolution of\u03d5is governed by the classical equation of motion: \u00a8\u03d5+ 3H \u02d9\u03d5=\u2212\u2202V /\u2202\u03d5(28) with the initial conditions\u03d5(0) =m/ \u221a \u03bband \u02d9\u03d5(0) = 0. Here we neglected the term containing spatial derivatives since their contribution is divided by the scale factor and", "source": "particlephysics.pdf"}, {"text": "\u02d9\u03d5=\u2212\u2202V /\u2202\u03d5(28) with the initial conditions\u03d5(0) =m/ \u221a \u03bband \u02d9\u03d5(0) = 0. Here we neglected the term containing spatial derivatives since their contribution is divided by the scale factor and quickly decreases with time. The value of\u2202V /\u2202\u03d5at\u03d5=m/ \u221a \u03bbcan be estimated using Eq. (26) asconst\u00b7m 2\u03d5, where is natural to expect thatm\u226bH. Hence\u03d5rises as exp(mt) and after the bubble formation the phase transition proceeds in time much shorter than H \u22121 and thus an exponential bloating of the bubble does not take place. In this case, there would be many bubbles in the visible today part of the universe, which in turn would lead to too large density inhomegeneities. More detailed discussion and references to the relevant literature can", "source": "particlephysics.pdf"}, {"text": "many bubbles in the visible today part of the universe, which in turn would lead to too large density inhomegeneities. More detailed discussion and references to the relevant literature can be found, for example, in ref [41]. In the modified version of the inflationary model [33] a strong condition is imposed on potentialV(\u03d5), namelym 2T\u2261\u2202 2V \u2202\u03d5 2 \u226aH 2 which does not naturally follow from the theory. This condition was specially invented to ensure a slow growth of the classical field \u03d5(t) after the phase transition,\u03d5(t)\u223cexp(m 2t/3H). For a solution of standing in front of us problems it is sufficient thatm 2 < H 2/25. In this case, the bubble\u2019s walls quickly disappear at infinity, and the entire universe", "source": "particlephysics.pdf"}, {"text": "solution of standing in front of us problems it is sufficient thatm 2 < H 2/25. In this case, the bubble\u2019s walls quickly disappear at infinity, and the entire universe is contained inside the interior of a single bubble, the characteristic size of which at the moment of formation was of the order of 13 m\u22121 and which, as a result of the universe expansion, reached the size up to r\u2273 1 r exp (3H2/m2)\u00b7(T /(3K).(29) Here, T is the temperature of the primary plasma after the phase transition. The total universe expansion in this version of the model is by far larger and differs from that given by eq. (29) by the exponential factor presented in eq. (27). Naturally", "source": "particlephysics.pdf"}, {"text": "total universe expansion in this version of the model is by far larger and differs from that given by eq. (29) by the exponential factor presented in eq. (27). Naturally in this variant of the model, the inhomogeneities will be significantly smaller, but as shown by calculations in the frameworks of the standardSU(5) theory, their magnitude happened to be larger than in reality approximately by two orders of magnitude [47]. Let us note that in contrast to the previous case the inhomogeneities are not connected with the bubble walls, but with rising quantum fluctuations in de Sitter universe. It is interesting that in the standard scenario of the universe evolution, i.e. without de Sitter stage, an estimate of inhomogeneities generated", "source": "particlephysics.pdf"}, {"text": "with rising quantum fluctuations in de Sitter universe. It is interesting that in the standard scenario of the universe evolution, i.e. without de Sitter stage, an estimate of inhomogeneities generated by quantum fluctuations leads to the result approximately two orders of magnitude smaller than it is necessary for galaxy formation. The condition of small inhomogeneities demands extremely slow variation of the effec- tive potentialV(\u03d5) in rather large interval of variation of\u03d5, including the region of small\u03d5. However, since the rate of particle production by external field\u03d5(t) is proportional to the speed of the field variation, then in such a model, the particles that subsequently should fill the expanding void, forming out world, are created too slowly and and their density", "source": "particlephysics.pdf"}, {"text": "speed of the field variation, then in such a model, the particles that subsequently should fill the expanding void, forming out world, are created too slowly and and their density and temperature, if they manage to thermalize, turn out to be too small and it would be impossible to explain the observed baryon asymmetry of the universe. Detailed discussion of these problem and the gravitational and temperature effects is presented in paper [42]. We note only that usually the role of gravity at the energies much smaller than the Planck one is not essential. However in the model under consideration where a new hierarchy of masses is introduced, namelym 2 \u226aH 2 orm 2 \u226aR, whereRis the four dimensional curvature", "source": "particlephysics.pdf"}, {"text": "one is not essential. However in the model under consideration where a new hierarchy of masses is introduced, namelym 2 \u226aH 2 orm 2 \u226aR, whereRis the four dimensional curvature of space-time. Just because of that the role of gravitational corrections to the effective potential happens to be non-negligible. In particular scalar field theory allows an addition to the Lagrangian the term\u03be\u03d5 2Rthat essentially changes the effective mass in de Sitter space, The smallness of the effective massm 2 in comparison withH 2 mentioned above implies smallness or cancellation of several terms each making a contribution to the coefficient in front of\u03d5 2/2 in the effective Lagrangian: m2 =m 2 0 +\u03b1T 2 +\u03beR+\u03bb\u27e8\u03d5 2\u27e9+...(30) Herem 0 it the field", "source": "particlephysics.pdf"}, {"text": "several terms each making a contribution to the coefficient in front of\u03d5 2/2 in the effective Lagrangian: m2 =m 2 0 +\u03b1T 2 +\u03beR+\u03bb\u27e8\u03d5 2\u27e9+...(30) Herem 0 it the field mass in symmetric state in flat space-time with zero temperature. The natural value ofm 0 in Grand Unification models is 10 14 GeV. An exception is presented by the Coleman-Weinberg model [48] which is actually defined by the conditionm 0 = 0, imposed onV(\u03d5). Perhaps there is some beauty in that, but strictly speaking, we do not have any basis for this assumption, as e.g. symmetry arguments, to imposem 0 = 0. In addition, it should be noted that the Coleman-Weinberg model has been formulated in the flat space, where", "source": "particlephysics.pdf"}, {"text": "for this assumption, as e.g. symmetry arguments, to imposem 0 = 0. In addition, it should be noted that the Coleman-Weinberg model has been formulated in the flat space, where the conditionm 0 = 0 means \u22022V \u22022\u03d5 |\u03d5=0,R=0 = 0,(31) 14 whereRis the curvature of space-time. However, at\u03d5= 0 the conditionR= 0 in infla- tionary model is not fulfilled but instead of (32) the relationR=\u22128\u03c0GT \u00b5 \u00b5 = 32\u03c0GV(\u03c3), was suggested in ref. [42] to change it to \u22022V \u22022\u03d5 |\u03d5=0,R=32\u03c0GV(\u03c3) = 0,(32) Since we do not understand the origin of the cosmological term, the justification of the above condition looks mysterious. In particular in the model of ref [39] the relationR= 32\u03c0GV(\u03c3) generally speaking is not fulfilled. Hence", "source": "particlephysics.pdf"}, {"text": "origin of the cosmological term, the justification of the above condition looks mysterious. In particular in the model of ref [39] the relationR= 32\u03c0GV(\u03c3) generally speaking is not fulfilled. Hence the self-consistent formulation of the Coleman-Weinberg condition should be modified. The second term in eq. (30) arises due to interaction with thermal bath in which field\u03d5 is situated. Then constant\u03b1is expected to be of the order ofN g 2 whereg\u223c0.5 is the gauge coupling constant andnis the number of vector fields in the underlying symmetry group. Due to existence of the lower limit on the temperature in De Sitter worldTH =H/(2\u03c0) [46] this term itself may break the necessary conditionm 2 < H 2/25. To prevent this from happening, in", "source": "particlephysics.pdf"}, {"text": "the lower limit on the temperature in De Sitter worldTH =H/(2\u03c0) [46] this term itself may break the necessary conditionm 2 < H 2/25. To prevent this from happening, in works [36] where super-symmetric inflation was considered, it was assumed that\u03d5is a gauge singlet and hence it does not interact with vector fields. Interactions with other fields may be made arbitrarily weak and so constant\u03b1may be very small. This field which only role is to ensure inflation is called the inflaton. If we impose the condition of the conformal invariance at zero temperature we have to take\u03be= 1/6. In this case the third term in eq. (30) gives too large contribution\u03beR= 12\u03beH 2 = 2H 2. However we know the", "source": "particlephysics.pdf"}, {"text": "at zero temperature we have to take\u03be= 1/6. In this case the third term in eq. (30) gives too large contribution\u03beR= 12\u03beH 2 = 2H 2. However we know the conformal invariance is, as a rule, broken, hence there is no necessity in this condition. In particular it can be shown that for Goldstone bosons\u03be= 0 [49]. The last term in eq. (30) is induced by quantum fluctuations in curved space-time [50]. It may be not essential if the self-interaction constant\u03bbof field\u03d5is taken sufficiently small. So, leaving aside questions about the naturalness we see that it is possible to ensure sufficient duration of inflation after the phase transition. It is somewhat more difficult to solve the problem of inhomogeneities and", "source": "particlephysics.pdf"}, {"text": "the naturalness we see that it is possible to ensure sufficient duration of inflation after the phase transition. It is somewhat more difficult to solve the problem of inhomogeneities and generation of the baryon asymmetry. For that to be true the effective potential should be very smooth at\u03d5 < H, so thatV \u2032\u2032(\u03d5)\u226aH 2 but very abruptly falling down for a large\u03d5. Models leading to potentials of such a type exist but it is still premature to say that the final version of the mechanism of exponential expansion is indeed found. Let us briefly dwell on particle production in inflationary model. Immediately after phase transition the universe was formless and void, and darkness was upon it. There was no matter", "source": "particlephysics.pdf"}, {"text": "found. Let us briefly dwell on particle production in inflationary model. Immediately after phase transition the universe was formless and void, and darkness was upon it. There was no matter in the form of elementary particles. The amplitude of\u03d5rose in accordance with eq. (28). The energy-momentum tensor in the r.h.s. of the equations of the General Relativity was given by the expressions: \u03f1=\u03f1 vac +V(\u03d5) + \u02d9\u03d52/2;p=\u2212\u03f1 vac \u2212V(\u03d5) + \u02d9\u03d52/2 (33) withV(0) = 0 andV(\u03c3) satisfying equation (23), when\u03d5reaches the value\u03c3which corresponds to the stable minimum of the potential. In the new inflationary model [33] the tunneling, as it was already noted, goes to small \u03d5=\u03d5 0, such thatV(\u03d5 0)\u226a\u03f1 vac. It is also assumed the field varies very", "source": "particlephysics.pdf"}, {"text": "In the new inflationary model [33] the tunneling, as it was already noted, goes to small \u03d5=\u03d5 0, such thatV(\u03d5 0)\u226a\u03f1 vac. It is also assumed the field varies very slowly, so that \u02d9\u03d5/\u03d5\u226a, H. Hence at the first stage the character of expansion practically would not be 15 changed. Particle production at this stage practically would not take place because of slow variation of\u03d5. Later when\u03d5reaches sufficiently large valuesV(\u03d5becomes steeper and damped oscilations of\u03d5around equilibrium point\u03c3begin. Damping of oscillations is induced by the two reasons: firstly by the universe expansion which is described by the friction term 3H \u02d9\u03d5in eq. (28) and secondly by the particle production which is not explicitely taken into account in (28). Since at large\u03d5the", "source": "particlephysics.pdf"}, {"text": "expansion which is described by the friction term 3H \u02d9\u03d5in eq. (28) and secondly by the particle production which is not explicitely taken into account in (28). Since at large\u03d5the oscillation frequency is quite high,\u03c9=m(\u03d5=\u03c3) = (10 14 \u221210 15) GeV, particle production at this period becomes quite essential. The expansion regime is drastically changed going from the exponential (19) to the power law one (15). Indeed for harmonic oscillations the pressure given by eqs. (33) is zero. In our case deviations from harmonicity are not essential. The estimates made for several concrete models show that the rate of particle production \u02d9N /N(whereNis particle density in unit of space) is, as a rule, higher than the universe expansion rate,H= 2/(3t).", "source": "particlephysics.pdf"}, {"text": "for several concrete models show that the rate of particle production \u02d9N /N(whereNis particle density in unit of space) is, as a rule, higher than the universe expansion rate,H= 2/(3t). It is assumed in the standard model that\u03d5is the Higgs field, so its couplings to other particles are proportional to the masses of the latter. Thus predominantly heavist particles are created under condition that their muss is not too much larger then the oscillations frequency\u03c9. Hence the universe would be filled by superheavy boson in strongly out of equilibrium state. The last condition is favorable for generation of the baryon asymmetry of the universe. After superheavy boson decays light particles such as leptons and quarks are created and the primeval", "source": "particlephysics.pdf"}, {"text": "The last condition is favorable for generation of the baryon asymmetry of the universe. After superheavy boson decays light particles such as leptons and quarks are created and the primeval plasma was at last thermalized and acquires some temperature T1 and the expansion law became relativistic (15). The fact that the energy-momentum tensor was for a while dominated by heavy particles and the particle number density was smaller than the equilibrium one leads to some diminishing of the baryon asymmetry after thermalization: \u03b2= 3\u03b2 0 T1 m(\u03d5=\u03c3) ,(34) wherem(\u03d5=\u03c3) is the scalar field mass at the equilibrium point\u03c3and\u03b2 0 is the baryon asymmetry originally created via decays of heavy bosons. Thus the models in which the primeval plasma is cooled", "source": "particlephysics.pdf"}, {"text": "the scalar field mass at the equilibrium point\u03c3and\u03b2 0 is the baryon asymmetry originally created via decays of heavy bosons. Thus the models in which the primeval plasma is cooled down too much to the moment of thermalization are excluded. In addition to the difficulties described above, which are more technical than fundamen- tal, the inflating universe theory faces a number of problems related to quantum tunneling in a gravitational field. The tunneling theory proposed in refs. [44, 45] for flat space was generalized to the case of tunneling in de Sitter space [51]. However, the results of these works cannot be directly applied to the case of interest, since the transition to the imaginary time, which lies at the", "source": "particlephysics.pdf"}, {"text": "de Sitter space [51]. However, the results of these works cannot be directly applied to the case of interest, since the transition to the imaginary time, which lies at the heart of the method used, was performed for the exact de Sitter space, while the real universe is not such, and deviations of the exact solution from the approximate one may be significant. Here it might make sense 4 to use the Hamilton formalism and to try to solve the functional Schr\u00a8 odinger equation in quasi classical approximation (there is no other known way anyhow), that describes the considered quantum field theory in the external curved metric. The neglect of the back reaction of the field on gravity is justified", "source": "particlephysics.pdf"}, {"text": "no other known way anyhow), that describes the considered quantum field theory in the external curved metric. The neglect of the back reaction of the field on gravity is justified by fact that the condition of a long inflationary phase after formation of the bubble of the new phase is equivalent, as one can easily see, to the demand that the vacuum energy changes very little after the quantum jump. 4Analogous arguments have been used by A. Goncharov and A. Linde, private communication. 16 Assuming the the metric has the formds 2 =dt 2 \u2212e 2Htdr2 we obtain for the wave functional \u03a8(\u03d5) the the following equation: \u0014 2i \u2202 \u2202\u03c4 + \u03b42 \u03b4\u03d52 \u2212 2 9H 2\u03c4 2 Z", "source": "particlephysics.pdf"}, {"text": "formds 2 =dt 2 \u2212e 2Htdr2 we obtain for the wave functional \u03a8(\u03d5) the the following equation: \u0014 2i \u2202 \u2202\u03c4 + \u03b42 \u03b4\u03d52 \u2212 2 9H 2\u03c4 2 Z d3x[ 1 2(3H\u03c4) 2/3(\u2207\u03d5)2 +V(\u03d5)] \u0015 \u03a8 = 0,(35) where\u03c4=e \u22123Ht/(3H), andV(\u03d5) is the effective potential of field\u03d5: V(\u03d5) = 1 2 m2\u03d52 + \u03bb 4 \u03d54 ln \u03d52 \u03c32 .(36) Thus there arises a problem of tunneling in time dependent potential. An example of significantly simpler one dimensional (and not infinitely dimensional) quantum mechanical problem as (35) with the potentialU=\u03c4 \u2212nv(x) shows that for\u03c4\u21920 the usual expression for the tunneling probability \u0393\u223cexp \u0014 \u2212 Z dx \u221a 2mU \u0015 (37) is applicable ifn >2 but is not ifn <2.", "source": "particlephysics.pdf"}, {"text": "potentialU=\u03c4 \u2212nv(x) shows that for\u03c4\u21920 the usual expression for the tunneling probability \u0393\u223cexp \u0014 \u2212 Z dx \u221a 2mU \u0015 (37) is applicable ifn >2 but is not ifn <2. In the casen= 2 the result depends upon the parameter of the potentialV(\u03d5) and eq. (37) is valid if the coefficient in front ofx 2 is sufficiently large. If this result is directly applied to eq. (35), then one can see that for the validity of the improved inflationary model the situation is opposite,m 2 \u226aH 2. But in this case quasiclassical approximation is not applicable. Thus finally we do not have an adequate formalism for description of tunneling in gravitational field. For realization of inflationary model the value\u03d5 0,", "source": "particlephysics.pdf"}, {"text": "case quasiclassical approximation is not applicable. Thus finally we do not have an adequate formalism for description of tunneling in gravitational field. For realization of inflationary model the value\u03d5 0, which takes field\u03d5after tunneling, is of primary importance. For potential (36) in flat space-time the value\u03d5 0 =m/ \u221a \u03bbis sufficiently small to lead to slow motion of\u03d5to the limiting value\u03c3, not destroying long exponential expansion. On the opposite if\u03d5 0, is large, the equilibrium state is reached quickly and exponential expansion turns into the power law one. One can show that if the size of the created bubble of the new phase isr, the magnitude of the field in this bubble is ( \u221a \u03bbr)\u22121. Thus for successful inflation", "source": "particlephysics.pdf"}, {"text": "show that if the size of the created bubble of the new phase isr, the magnitude of the field in this bubble is ( \u221a \u03bbr)\u22121. Thus for successful inflation large bubbles are necessary. However in the theory described by the non-stationary equation (35), the bubble size is unknown. In the case considered in ref [51] it is shown that the bubble size does not exceedH \u22121 that is natural, because this is the horizon size in De Sitter space. Still it is unclear if this result is a consequence of the thin wall approximation used in the quoted papers or, which is probably more important, that the universe is not exactly the De Sitter one. The point is that", "source": "particlephysics.pdf"}, {"text": "of the thin wall approximation used in the quoted papers or, which is probably more important, that the universe is not exactly the De Sitter one. The point is that the transition to imaginary time leads to the transformation of the De Sitter space into four dimensional sphere of radiusH \u22121 so the size of the bubble in three dimensional space in the moment of its formation cannot exceed that. If this result survives in the real situation, the inflationary model could be in serious danger, because in this case\u03d5 0 would be big and hence a large expansion of the bubble is impossible. Another point could be serious is that we use effective Lagrangian assuming that the fields are", "source": "particlephysics.pdf"}, {"text": "0 would be big and hence a large expansion of the bubble is impossible. Another point could be serious is that we use effective Lagrangian assuming that the fields are slowly changing but their variation in the expanding world is not so small, generally speaking it is \u02d9\u03d5/\u03d5\u2248H. So we need to take into account loop corrections (but how?) not assuming\u03d5=const. It is not clear how all that may influence on the tunneling and the value of\u03d5 0. 17 The super-small size of the region from which our Universe began to inflate is also often a source of concern. According to equations (29) and (27) even for a rather modest value \u03bb= 0.1 the size of the region which now", "source": "particlephysics.pdf"}, {"text": "to inflate is also often a source of concern. According to equations (29) and (27) even for a rather modest value \u03bb= 0.1 the size of the region which now makes all the visible universe was surely smaller than, say, 10 \u2212100 cm. It is difficult to agree that at so small distances, even in vacuum (which as we now know is quite complicated) a serious modifications of the known to us physical laws have not took place. In my opinion there is no reasons for anxiety, because one can always speak about exponential expansion of a sufficiently large regions where no surprise in the vacuum structure happens. Even if in the course of inflation of very small regions some", "source": "particlephysics.pdf"}, {"text": "always speak about exponential expansion of a sufficiently large regions where no surprise in the vacuum structure happens. Even if in the course of inflation of very small regions some unknown phenomena appear, they should disappear when the size of these regions becomes sufficiently large, Recently an interesting version of inflationary model for which no phase transition is necessary was suggested by Linde [52]. The starting point of this model is the assumption that at some initial moment scalar field\u03d5might take very large values\u03d5\u226bm P l \u224810 19 GeV, with spatial variation of\u03d5being sufficiently low. Such a situation can be realized in the case of chaotic initial conditions if the self-interaction coupling constant is small, so thatV=\u03bb\u03d5 4/4< m 4", "source": "particlephysics.pdf"}, {"text": "spatial variation of\u03d5being sufficiently low. Such a situation can be realized in the case of chaotic initial conditions if the self-interaction coupling constant is small, so thatV=\u03bb\u03d5 4/4< m 4 P l. In this case at the right hand side of evolutionary equation (28) the termH \u02d9\u03d5starts to dominate, whereH\u2242(8\u03c0\u03f1m \u22122 P l /3)1/2 \u2248(2\u03c0\u03bb/3) 1/2\u03d52m\u22121 P l and the solution of this equation takes the form \u03d5=\u03d5 i exp \" \u2212 \u221a \u03bb\u221a 6\u03c0 mP lt # .(38) Hence the expansion rate of the universe \u02d9a/a=Hhappens to be larger than the rate of \u03d5decrease due to the factor\u03d5 i/mP l \u226b1 and the universe region, where the conditions mentioned above were accidentally, has expanded exponentially: a a0 = exp", "source": "particlephysics.pdf"}, {"text": "than the rate of \u03d5decrease due to the factor\u03d5 i/mP l \u226b1 and the universe region, where the conditions mentioned above were accidentally, has expanded exponentially: a a0 = exp \u0012 2\u03c0 \u03d52 i m2 P l \u0013 .(39) This expansion could provide a solution of the problems discussed above if the ratio \u03d52 i /m2 P l is sufficiently large, namely\u03d5 2 i /m2 P l \u227310. So with chaotic initial conditions in infinite universe there always could found a region that strongly exponentially expanded and as a result reach the state suitable for our existence. Other uncomfortable regions of the universe would be outside of possibilities of our observations For realization of this model it is not necessary", "source": "particlephysics.pdf"}, {"text": "reach the state suitable for our existence. Other uncomfortable regions of the universe would be outside of possibilities of our observations For realization of this model it is not necessary to impose many special condition on the field theory, it is enough to use a simple hypothesis on existence of weakly interacting and self-interacting field\u03d5, i.e the assumption of a small\u03bband weak coupling to other fields. Not yet worked out is the problem of of quantum gravity corrections to the classical equations of of motion for a large\u03d5. Of course there is still a question about naturalness of the initial conditions. In contrast to the classical Friedman cosmology, where a very precise fine tuning of the initial state is demanded,", "source": "particlephysics.pdf"}, {"text": "there is still a question about naturalness of the initial conditions. In contrast to the classical Friedman cosmology, where a very precise fine tuning of the initial state is demanded, here we have the stochatsically distributed field\u03d5in chaotic universe near singularity. In my opinion the hypothesis about initial chaos is much more attractive and this variant possibly corresponds to the real case, though the question about the origin of the initial chaotic state remains open. 18 Conclusion So presently there are two principally different approaches to the problem of the origin and evolution of the universe. The first one, to one or other degree, is based on the anthropic principle, according to which the fact that life existence in the", "source": "particlephysics.pdf"}, {"text": "origin and evolution of the universe. The first one, to one or other degree, is based on the anthropic principle, according to which the fact that life existence in the universe makes senseless the question why the universe is such but not other. This approach cannot be denied the right to exist, especially if there are an infinite set of different universes is realized. Then out of this chaotic set only a few universes with very specific conditions could be available for us. However from the point of view of the anthropic principle the colossal redundancy of other galaxies is mysterious. In other approach is assumed that the universe is one and only one but initial conditions there is arbitrary.", "source": "particlephysics.pdf"}, {"text": "the anthropic principle the colossal redundancy of other galaxies is mysterious. In other approach is assumed that the universe is one and only one but initial conditions there is arbitrary. However, and this lies in the basement of all theoretical models, the laws of physics are such that practically from any initial state we arrive to our very non-trivial world. Inflationary model discussed above responds positively to the latter demand. However, the version with chaotic universe is an intermediate one between those two approaches. It goes without saying that this model cannot be considered as the final theory. From one side there are some unsolved problems inside the model, such as e.g. that concerning tunneling in the expanding world. On", "source": "particlephysics.pdf"}, {"text": "model cannot be considered as the final theory. From one side there are some unsolved problems inside the model, such as e.g. that concerning tunneling in the expanding world. On the other hand it is not established on which field theory this model is based. It s difficult to expect to find the solution to the last problem until the elementary particle theory is not worked out that is applicable up to the Planck energies. Sooner it is another way around, if it is assumed that inflationary model based on strongly delayed first order phase transition, does indeed correctly describes reality, one could derive conditions that the elementary particle theory must satisfy. What ground we have to believe that the", "source": "particlephysics.pdf"}, {"text": "strongly delayed first order phase transition, does indeed correctly describes reality, one could derive conditions that the elementary particle theory must satisfy. What ground we have to believe that the model of inflationary universe is really true? First, it is beautiful, since it is based on one very simple condition on existence of De Sitter stage some time in the past. This allows to solve in a uniform way the problems of homogeneity, isotropy, horizon, flatness, and relic magnetic monopoles. These facts are surely in favor of this scenario. Against the the model, though indirectly, is the problem of the cosmological constant and to a smaller (up to the present time) extent and an absence of the detailed theoretical scheme.", "source": "particlephysics.pdf"}, {"text": "the the model, though indirectly, is the problem of the cosmological constant and to a smaller (up to the present time) extent and an absence of the detailed theoretical scheme. The status of inflationary model would be very much stronger if it confirmed that the cosmological parameter \u2126 =\u03f1/\u03f1 c is equal to 1. Unfortunately at the present time it is not seen if this can be established with sufficient accuracy On the opposite it seems it is easier to reject the model obtaining an upper bound on \u2126. Still even if inflationary model obtains convincing evidence in its favor (most probably they will be theoretical but not observational ) still complete happiness will be far away until the approaches", "source": "particlephysics.pdf"}, {"text": "even if inflationary model obtains convincing evidence in its favor (most probably they will be theoretical but not observational ) still complete happiness will be far away until the approaches to solving the two remaining critical problems have been found: the problem of cosmological constant and the problem of the universe creation and singularity. However, we mustn\u2019t forget that \u201dappetite comes with eating\u201d - after all, quite recently, those fundamental problems that we now, thanks to the inflationary model, consider al- ready solved, or we say (those who are more cautious) that there appears a possibility of their solution. A few years ago, they seemed completely impregnable, and the importance of this achievement should not be underestimated. 19 Appendix -", "source": "particlephysics.pdf"}, {"text": "cautious) that there appears a possibility of their solution. A few years ago, they seemed completely impregnable, and the importance of this achievement should not be underestimated. 19 Appendix - translation of the content of Einshteinovskij Sbornik A. Einstein. How the Theory of Relativity Was Created. B.E. Yavelov, V. Ya. Frenkel. On Some Historical and Physical Aspects of the Einstein-de Haas Experiments. V.Ya. Frenkel, B.E. Yavelov. \u201dThis is What Can Happen to a Person Who Thinks a Lot but Reads Little.\u201d B.G. Kuznetsov. The Einstein-Bohr Collision, the Einstein-Bergson Collision, and Science in the Second Half of the Twentieth Century. V.P. Vizgin. Einstein, Hilbert, Weyl: The Genesis of the Program of Unified Geometrized Field Theories. A. Salam. Einstein\u2019s Final Vision: Unifying", "source": "particlephysics.pdf"}, {"text": "Science in the Second Half of the Twentieth Century. V.P. Vizgin. Einstein, Hilbert, Weyl: The Genesis of the Program of Unified Geometrized Field Theories. A. Salam. Einstein\u2019s Final Vision: Unifying Fundamental Interactions and the Properties of Space-Time. A.D. Dolgov. Progress in Particle Physics and Modern Cosmology. B. M. Bolotovsky. On the Apparent Form of Rapidly Moving Bodies. G. A. Lorentz. On Einstein\u2019s Theory of Gravitation. T. Levi-Civita. On an Analytical Expression for the Gravitational Tensor in Einstein\u2019s Theory. E. Schr\u00a8odinger. Components of the Gravitational Field Energy. G. Bauer. On the Components of the Gravitational Field Energy. G. Nordstr\u00a8om. On the Gravitational Field Energy in Einstein\u2019s Theory. F. Klein. On the Integral Form of the Conservation Laws of the Theory of", "source": "particlephysics.pdf"}, {"text": "Components of the Gravitational Field Energy. G. Nordstr\u00a8om. On the Gravitational Field Energy in Einstein\u2019s Theory. F. Klein. On the Integral Form of the Conservation Laws of the Theory of a Spatially Closed World. L. Rosenfeld. On the Gravitational Actions of Light. M.P. Bronstein. Quantum Theory of Weak Gravitational Fields. M.P. Bronstein. On the Possibility of Spontaneous Photon Splitting. G.E. Gorelik, V.Ya. Frenkel. M.P. Bronstein and His Role in the Development of the Quantum Theory of Gravity. I.Yu. Kobzarev. Review of W. Rindler\u2019s book \u201dFoundations of the Theory of Relativity (WHAT, GTR, and Cosmology)\u201d References [1] A.D. Dolgov, p. 111, Ejnshtejnovskij Sbornik, 1980-1981, Collection of papers, Moscow, \u201dNauka\u201d, Editor I.Yu. Kobzarev [2] G. Lemaitre, Annales Soc. Sci. Brux. Ser. A", "source": "particlephysics.pdf"}, {"text": "(WHAT, GTR, and Cosmology)\u201d References [1] A.D. Dolgov, p. 111, Ejnshtejnovskij Sbornik, 1980-1981, Collection of papers, Moscow, \u201dNauka\u201d, Editor I.Yu. Kobzarev [2] G. Lemaitre, Annales Soc. Sci. Brux. Ser. A 53, 51 (1933). [3] E.V. Arbuzova, A.D. Dolgov, Eur. Phys. J, C 85 (2025) 8, 912, e-Print: 2502.05581. [4] Einstein, A. Ann. Phys., 1916, Bd. 49, S. 769-822. Russian translation: Collected Scientific Works, 1965, vol. I, pp. 452\u2013504. [5] Friedman A., Ztschr. Phys., 1922, Bd. 40, pp. 377\u2013386; 19, vol. 21, pp. 326\u2013332. 20 [6] Einstein, A. Sitzungsber. Preuss. Akad. Wiss., 1917, Vol. 4 42. Russian.translation: Collection of Works, 1965, vol. 1, pp. 601-616. [7] Hubble E.R., Proc. Nat. Acad. Sci., 1927, vol. 15, pp. 618-625. [8] Gamow, G., Phys.", "source": "particlephysics.pdf"}, {"text": "Wiss., 1917, Vol. 4 42. Russian.translation: Collection of Works, 1965, vol. 1, pp. 601-616. [7] Hubble E.R., Proc. Nat. Acad. Sci., 1927, vol. 15, pp. 618-625. [8] Gamow, G., Phys. Rev., 1946, vol. 70, pp. 572-573; AIpher R.A., Bethe H., Gamow G. Phys. Rev., 1948, vol. 73, pp. 803-804. [9] Penzias A.A., Wilson R.W., Astrophysics. J., 1965, vol. 342, pp. 49\u2013427. [10] Wagoner R.V., Fowler W.A., Hoyle F., Astrophys. J., 1967, vol. 148, p. 3 -17 . [11] Vaucouleurs, de G., et al., Astrophys. J., 1981, vol. 248, pp. 395\u2013407; 408\u2013422; Mon. Not. Roy. Astron. Soc., 4983, vol. 202, pp. 367\u2013371; Aaronson M., Mould J., Huchra J. et al., Astrophys. J., 1980, vol. 239, pp. 42\u201337; Hanes D.A. - Mon.", "source": "particlephysics.pdf"}, {"text": "408\u2013422; Mon. Not. Roy. Astron. Soc., 4983, vol. 202, pp. 367\u2013371; Aaronson M., Mould J., Huchra J. et al., Astrophys. J., 1980, vol. 239, pp. 42\u201337; Hanes D.A. - Mon. Not. Roy. Astron. Soc., 1982, vol. 201, pp. 145\u2013148; Buta R., Vaucouleurs, de G., Astrophys. J., 1983, vol. 266, pp. 1\u201317. [12] Sandage, A., Tamman G.A., Astrophys. J., 1967, vol. 201, pp. 7\u201324. [13] Gott J.R., Turner E.L., Astrophys. J., 1976, vol. 209, pp. 1\u20135; Davis M., Tonry J. , Huchra J., Lathham D . W. - Astrophys. J., 1980, vol. 238, pp. L143\u2014L116; Aaronson M., Mould J., Huchra J. et al., Astrophys. J., 1980, vol. 239, pp. 12-37; Peebles P.J.E., Astron. J., 1979, vol. 84, pp. 730-734. [14] Einasto", "source": "particlephysics.pdf"}, {"text": "vol. 238, pp. L143\u2014L116; Aaronson M., Mould J., Huchra J. et al., Astrophys. J., 1980, vol. 239, pp. 12-37; Peebles P.J.E., Astron. J., 1979, vol. 84, pp. 730-734. [14] Einasto J.E., Kaasic A., Saar E.M., Nature, 1974, vol. 250, pp. 309-310; Ostriker J.R., Peebles R.J.E., Yahil A. \u2014Astrophysics. J., 1974, vol. 193, pp. L1-L4. [15] Faber S. M., Gallagher J. S. Ann. Rev. Astr. Ap., 1979, vol. 17, 135-136. [16] Zeldovich Ya., Khlopov M. UFN, 1984, vol. 135, pp. 45-77; Sclama D. W. - Proc. Nuffield Workshop on the Very Early Universe. Cambridge: Univ. Press, Eds. G. W. Gibbons, S. Hawking, S. Siklos, 1982, pp. 399\u2013406; Abbott B.F., Axion Cosmology. Talk presented at the IVth Latin Amer. Symp. on Relativity", "source": "particlephysics.pdf"}, {"text": "Early Universe. Cambridge: Univ. Press, Eds. G. W. Gibbons, S. Hawking, S. Siklos, 1982, pp. 399\u2013406; Abbott B.F., Axion Cosmology. Talk presented at the IVth Latin Amer. Symp. on Relativity and Gravitation. [17] Rees M., Proc. Nuffield Workshop on the Very Ealry Universe. Cambridge.: Univ. Press, Eds. C. W. Gibbons, S. Hawking, S. Siklos, 1982, 29-58. [18] A brief review of the observational data can be found e.g. in Berg van den S. Science, 1981, vol. 213, pp. 825-827; there are more later works where the conclusion of large universe age is done: Brown J.C., Berman B.L. Phys. Rev., 1981, vol. C 23, pp. 1434\u20131645. Thieleman F.-K., Metzinger J., Klapder H.V. - Ztschr. Phys., 1983, Bd. A309, S 301\u2013317. [19]", "source": "particlephysics.pdf"}, {"text": "age is done: Brown J.C., Berman B.L. Phys. Rev., 1981, vol. C 23, pp. 1434\u20131645. Thieleman F.-K., Metzinger J., Klapder H.V. - Ztschr. Phys., 1983, Bd. A309, S 301\u2013317. [19] Zeldovich Ya.B., Kobzarev I.Yu., Okun L.B. ZhETF 1973, vol. 67, pages 3-11. 21 [20] Zel\u2019dovich Ya. B., Mon. Not. Roy. Astron. Soc., 1980, vol. 192. pages 663- 665 ; Vilenkin, A. Phys. Rev. Lett, 1981, vol. 46, p. 1169\u20141172, 1496 (E); Phys. Rev., 1981, vol. D24, p. 2082\u20142089. [21] Kirzhnits D.A. ZhETF Letters, 1972, vol. 15, page 745\u2014748; Kirzhnits D.A., Linde A.D. Phys. Lett., 1972, vol. 42B, p. 471-474; see also the review Linde A.D. Rep. Progr. Phys., 1979, 389-457. [22] Markov M.A., Preprint INR PR-0227, ZhETF Letters 1981, v.36,", "source": "particlephysics.pdf"}, {"text": "Linde A.D. Phys. Lett., 1972, vol. 42B, p. 471-474; see also the review Linde A.D. Rep. Progr. Phys., 1979, 389-457. [22] Markov M.A., Preprint INR PR-0227, ZhETF Letters 1981, v.36, pp 214-216 [23] Sakharov A.D. ZhETF Letters 1967, vol.. 5, page. 32-35; Kuznim V.A., ZhETF 1970, vol. 12 , pp 335-337. [24] The complete list of references on the problem of generation of the baryon asymmetry of the universe is almost infinite. It is partly contained in the review A.D. Dolgov, Ya, B. Zeldovich, Rev. Mod. Phys., 1981, vol. 53, p. 1-41. [25] A.D. Dolgov, Ya.B. Zeldovich, Priroda (Nature), 1982, No. 8, 33-45 . [26] Zeldovich Ya.B., Khlopov M.Yu. Phys. Lett, 1978, vol. B79, p. 239\u2014241; Preskill J.R. Phys. Rev.", "source": "particlephysics.pdf"}, {"text": "p. 1-41. [25] A.D. Dolgov, Ya.B. Zeldovich, Priroda (Nature), 1982, No. 8, 33-45 . [26] Zeldovich Ya.B., Khlopov M.Yu. Phys. Lett, 1978, vol. B79, p. 239\u2014241; Preskill J.R. Phys. Rev. Lett., 1979, vol. 43, p. 1365-1368. [27] Guth A. Phys. Rev., 1981, vol. D23, p. 347-356. [28] Linde A.D., Phys. Lett., 1982, vol. 108B, p. 389-393; Albrecht A., Steinhardt P.J., Phys. Rev. Lett. 1982, vol. 48, pp 1220-1223. [29] Dolgov A.D., Linde A.D. Phys. Lett., 1982, vol. 116B, p. 329-334; Abbot L.E., Farhi E., Wise M.B. Phys. Lett., 1982, vol. 117B, p. 29-33; Albrecht A., Steinhardt P.J., Turner M.S., Wilczek F. Phys. Rev. Lett., 1982, vol. 48, p. 1437-1440. [30] Rees M. Proc. Nuffield Workshop on the Very Early Universe.", "source": "particlephysics.pdf"}, {"text": "117B, p. 29-33; Albrecht A., Steinhardt P.J., Turner M.S., Wilczek F. Phys. Rev. Lett., 1982, vol. 48, p. 1437-1440. [30] Rees M. Proc. Nuffield Workshop on the Very Early Universe. Cambridge Univ. Press, Eds. C.W. Gibbons, S. Hawking, S. Siklos, 1982, p. 29-58. [31] Cabrerra B. Phys. Rev. Lett., 1982, vol. 48, p. 1378-1381 [32] Steinhardt P.J. Proc. Nuffield Workshop on the Very Early Universe. Cambridge: Univ. Press, Eds. G.W. Gibbons, S.W . Hawking, S.T.C. Siklos, 1982, P. 251-266; Ellis J., Nanopoulos D.V., Olive K.A., Phys. Lett., 1983, vol. 127B, p. 30\u201434; Ellis J. SLAC-PUB-3006, 1982. [33] Linde A.D. Phys. Lett., 1982, vol. 108B, p. 389-393; Albrecht A., Steinhardt P.J. Phys. Rev. Lett., 1982, vol. 48, p. 1220\u20141223. [34] Beresin", "source": "particlephysics.pdf"}, {"text": "p. 30\u201434; Ellis J. SLAC-PUB-3006, 1982. [33] Linde A.D. Phys. Lett., 1982, vol. 108B, p. 389-393; Albrecht A., Steinhardt P.J. Phys. Rev. Lett., 1982, vol. 48, p. 1220\u20141223. [34] Beresin V.A., Kuzmin V.A., Tkachev I.I. Phys. Lett., 1982, vol. 120B, P. 91-96; preprint IC/83/16 (Triest). 22 [35] Proc. Nuffield Workshop on the Very Early Universe. Cambridge, Univ. Press, Eds. G.W. Gibbons, S.W. Hawking, S.T.C. Siklos, 1982; Starobinsky A.A. Phys. Lett., 1982, vol. 117B, p. 175-178; Guth A.H., Pi S-Y, Phys. Rev. Lett., 1982, vol. 49, 1110\u20141113; Haking S.W. Phys. Lett., 1982, vol. 115B, p. 295-297; Bardeen J., Steinhardt P.J., Turner M., Phys. Rev., 1983, vol. D28, p. 679\u2014693. [36] Steinhardt P.J., Proc. Nuffield Workshop on the Very Early Universe. Cambridge", "source": "particlephysics.pdf"}, {"text": "1982, vol. 115B, p. 295-297; Bardeen J., Steinhardt P.J., Turner M., Phys. Rev., 1983, vol. D28, p. 679\u2014693. [36] Steinhardt P.J., Proc. Nuffield Workshop on the Very Early Universe. Cambridge Univ. Press Eds. G.W. Gibbons; S.W. Hawking, S.T.C. Siklos, 1982, p. 251-266; Ellis J., Nanopoulos D.V., Olive K.A., Tamvakis K. - Phys. Lett., 1983, vol. 118B, p. 335 - 337; vol. 120B, p. 331-334; Nanopoulos D.V., Olive K.A., Srednicki M., Tamvakis K. Phys. Lett., 1983, vol. 123B, p. 41-44; vol. 124B, p. 171-174; Nanopoulos D.V., Olive K.A., Srednicki M. Phys. Lett., 1983, vol. 127B, p. 30-34; Ellis J. SLAC-PUB- 3006. 1982. [37] Starobinsky A.A. Phys. Lett., 1980, vol. 91B, p. 99-102. [38] Lapchinsky V.G., Rubakov V.A., Veryaskin A.V. preprint INR", "source": "particlephysics.pdf"}, {"text": "Lett., 1983, vol. 127B, p. 30-34; Ellis J. SLAC-PUB- 3006. 1982. [37] Starobinsky A.A. Phys. Lett., 1980, vol. 91B, p. 99-102. [38] Lapchinsky V.G., Rubakov V.A., Veryaskin A.V. preprint INR R-0234, 1982. [39] Dolgov A.D. Proc. Nuffield Workshop on the Very Early Universe. Cambridge, Univ. Pres /Eds. G.W. Gibbons, S.W. Hawking, T.C. Siklos, 1982, P. 449 - 458 [40] Kirzhnits D.A. ZhETP Letters 1972, vol. 15, 745-748; Kirzhnits D., Linde A, Phys. Lett., 1972, vol 42B, pp 471-474; See also the review Linde A.D. Rep. Progr. Phys.,vol. 42, p. 389-437. [41] Guth A.H. Proc. Nuffield Workshop on the Very Early Universe. Cambridge, Univ. Press, Eds. G.W. Gibbons, S. W. Hawking, S.T.C. Siklos, 1982, 171-204. [42] Linde A.D., Proc. Nuffield Workshop", "source": "particlephysics.pdf"}, {"text": "[41] Guth A.H. Proc. Nuffield Workshop on the Very Early Universe. Cambridge, Univ. Press, Eds. G.W. Gibbons, S. W. Hawking, S.T.C. Siklos, 1982, 171-204. [42] Linde A.D., Proc. Nuffield Workshop on the Very Early Universe, Cambridge, Univ. Press, Eds. G.W. Gibbons, S.W. Hawking, S.T.C. Siklos, 1982, p. 205\u2014250. [43] Dolan L., Jackiw R., Phys. Rev., 1974, vol. D9, p. 3320-3340; Weinberg S., Phys. Rev., 1974 vol. D9, 3357\u20143378. [44] Voloshin M.B, Kobzarev I.Yu., Okun L.B. Yadernaya Fizika, 1974, vol. 20, pp. 1229- 1234. [45] Coleman S. Phys. Rev., 1977, vol. D15, p. 2929-2936; Callan C.G., Coleman S., Phys. Rev., 1977, vol. D16, p. 1762\u20141768. [46] Gibbons G., Hawking S.W. Phys. Rev., 1977, vol. D15, p. 2738-2751. [47] Detailed discussion of", "source": "particlephysics.pdf"}, {"text": "D15, p. 2929-2936; Callan C.G., Coleman S., Phys. Rev., 1977, vol. D16, p. 1762\u20141768. [46] Gibbons G., Hawking S.W. Phys. Rev., 1977, vol. D15, p. 2738-2751. [47] Detailed discussion of the rise of inhomogeneities in inflationary universe is discussed in Proc. of the Nuffield Workshop on the very early universe, Eds. G.W. Gibbons, S.W . Hawking, S.T.C. Siklos, 1982; Starobinsky A.A. Phys. Lett., 1982: 23 Guth A.H., Pi S.-Y., Phys. Rev. Lett., 1982, vol. 49, 1110\u20141113; Hawking S.W., Phys. Lett., 1982, vol. 115B, p .295=297; Bardeen J., Steinhardt P.J., Turner M. Phys. Rev., 1983, vol. D28, p. 679-693. [48] Coleman S., Weinberg E. Phys. Rev., 1973, vol. D7, p. 1888-1909. [49] Voloshin M.B., Dolgov A.D. Yadernaya Fizika 1982, vol. 35,", "source": "particlephysics.pdf"}, {"text": "M. Phys. Rev., 1983, vol. D28, p. 679-693. [48] Coleman S., Weinberg E. Phys. Rev., 1973, vol. D7, p. 1888-1909. [49] Voloshin M.B., Dolgov A.D. Yadernaya Fizika 1982, vol. 35, pp. 213-215 [50] Vilenkin A., Ford L.H. Phys. Rev., 1984, vol. D26, P. 1401; Linde A.D. Phys. Lett., 1982, vol.116, pp. 335-339; Starobinsky A.A., Phys. Lett.,1982, vol. 117B, p. 175-178. [51] Coleman S., De Luccia F., Phys. Rev., 1980, vol. D21, p. 3305\u20143315; Hawking S.W., Moss I.G., Phys. Lett., 1982, vol. 110B, Phys. Lett., 1983, vol. 121B; Steinhardt P.J.,University of Pennsylvania preprint \u00a8UPR-022T, 1983. [52] Linde A.D., Invited Talk at the Shelter Island Conference II, 1983. 24", "source": "particlephysics.pdf"}, {"text": "Pennsylvania preprint \u00a8UPR-022T, 1983. [52] Linde A.D., Invited Talk at the Shelter Island Conference II, 1983. 24", "source": "particlephysics.pdf"}]